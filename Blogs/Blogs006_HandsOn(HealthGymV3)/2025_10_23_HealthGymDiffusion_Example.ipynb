{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vn-mfRQx3VkL"
      },
      "outputs": [],
      "source": [
        "# ========================\n",
        "# Standard Library\n",
        "# ========================\n",
        "import os\n",
        "import random\n",
        "import itertools\n",
        "import math\n",
        "import copy\n",
        "from copy import deepcopy\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple, Optional, Mapping, Sequence\n",
        "\n",
        "from    scipy.stats         import  boxcox\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "# ========================\n",
        "# Core Scientific Stack\n",
        "# ========================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from scipy import stats\n",
        "from scipy.special import boxcox as sp_boxcox\n",
        "\n",
        "# ========================\n",
        "# Machine Learning Utilities\n",
        "# ========================\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ========================\n",
        "# Deep Learning (PyTorch)\n",
        "# ========================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import autograd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import  torch.utils.data    as  utils\n",
        "from torch.optim import Adam\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.amp import GradScaler  # <-- add this import at the top of file\n",
        "\n",
        "# ========================\n",
        "# Visualization\n",
        "# ========================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_all(seed: int = 42, deterministic: bool = True) -> None:\n",
        "    import os, random, numpy as np, torch\n",
        "\n",
        "    # Python built-ins\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "    # NumPy\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # PyTorch\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # cuDNN settings\n",
        "    torch.backends.cudnn.deterministic = deterministic\n",
        "    torch.backends.cudnn.benchmark = not deterministic\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cuda.matmul.allow_tf32 = False\n",
        "        torch.backends.cudnn.allow_tf32 = False"
      ],
      "metadata": {
        "id": "eHhQPPwM5B_U"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_all()\n",
        "\n",
        "# === Categorical Mapping Utilities (simple & tidy) ===\n",
        "BINCAT_STR2INT = {\n",
        "    \"Gender\": {\"Male\": 0, \"Female\": 1},\n",
        "    \"Ethnic\": {\"Asian\": 0, \"African\": 1, \"Caucasian\": 2, \"Other\": 3},\n",
        "    \"Base Drug Combo\": {\n",
        "        \"FTC + TDF\": 0, \"3TC + ABC\": 1, \"FTC + TAF\": 2,\n",
        "        \"DRV + FTC + TDF\": 3, \"FTC + RTVB + TDF\": 4, \"Other\": 5\n",
        "    },\n",
        "    \"Extra PI\": {\"DRV\": 0, \"RTVB\": 1, \"LPV\": 2, \"RTV\": 3, \"ATV\": 4, \"Not Applied\": 5},\n",
        "    \"Extra pk-En\": {\"False\": 0, \"True\": 1},   # ← fixed typo\n",
        "}\n",
        "\n",
        "BINCAT_INT2STR = {\n",
        "    \"Gender\": {0: \"Male\", 1: \"Female\"},\n",
        "    \"Ethnic\": {0: \"Asian\", 1: \"African\", 2: \"Caucasian\", 3: \"Other\"},\n",
        "    \"Base Drug Combo\": {\n",
        "        0: \"FTC + TDF\", 1: \"3TC + ABC\", 2: \"FTC + TAF\",\n",
        "        3: \"DRV + FTC + TDF\", 4: \"FTC + RTVB + TDF\", 5: \"Other\"\n",
        "    },\n",
        "    \"Extra PI\": {0: \"DRV\", 1: \"RTVB\", 2: \"LPV\", 3: \"RTV\", 4: \"ATV\", 5: \"Not Applied\"},\n",
        "    \"Extra pk-En\": {0: \"False\", 1: \"True\"},   # ← fixed typo\n",
        "}\n",
        "\n",
        "def BinCat2Num(df: pd.DataFrame, allow_already_int: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Map categorical string columns → integer codes (0..K-1).\n",
        "    If allow_already_int=True, columns already coded as ints in the valid range are left as-is.\n",
        "    \"\"\"\n",
        "    out = df.copy()\n",
        "    for col, mapping in BINCAT_STR2INT.items():\n",
        "        if col not in out.columns:\n",
        "            continue\n",
        "        s = out[col]\n",
        "        if allow_already_int and pd.api.types.is_integer_dtype(s):\n",
        "            # Validate codes are within allowed range\n",
        "            allowed = set(mapping.values())\n",
        "            bad = set(pd.Series(s.dropna().unique(), dtype=int)) - allowed\n",
        "            if bad:\n",
        "                raise ValueError(f\"Unexpected integer codes in '{col}': {sorted(bad)}\")\n",
        "            continue\n",
        "        mapped = s.map(mapping)\n",
        "        if mapped.isnull().any():\n",
        "            bad_vals = s[mapped.isnull()].unique()\n",
        "            raise ValueError(f\"Unmapped values in column '{col}': {bad_vals}\")\n",
        "        out[col] = mapped.astype(int)\n",
        "    return out\n",
        "\n",
        "def BinCat2Str(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Map categorical integer codes → string labels.\"\"\"\n",
        "    out = df.copy()\n",
        "    for col, mapping in BINCAT_INT2STR.items():\n",
        "        if col in out.columns:\n",
        "            out[col] = out[col].map(mapping)\n",
        "    return out"
      ],
      "metadata": {
        "id": "yASh7TQF5DU8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_all()\n",
        "\n",
        "# ========================\n",
        "# Fit Box–Cox parameters\n",
        "# ========================\n",
        "def compute_boxcox_params(\n",
        "    df: pd.DataFrame,\n",
        "    columns: List[str] = (\"VL\", \"CD4\"),\n",
        "    eps: float = 1e-3,\n",
        ") -> Dict[str, dict]:\n",
        "    \"\"\"\n",
        "    Fit Box–Cox λ per column on (x + eps), and store min/range of the\n",
        "    transformed values for later [0,1] scaling.\n",
        "    \"\"\"\n",
        "    params: Dict[str, dict] = {}\n",
        "    for col in columns:\n",
        "        x = df[col].dropna().astype(float).to_numpy()\n",
        "        x = x + eps  # ensure positivity\n",
        "\n",
        "        # Handle empty/degenerate columns safely\n",
        "        if x.size == 0 or not np.all(np.isfinite(x)) or np.min(x) <= 0 or np.ptp(x) == 0:\n",
        "            params[col] = {\"lambda\": 1.0, \"min\": 0.0, \"range\": 1.0, \"eps\": eps}\n",
        "            continue\n",
        "\n",
        "        bc, lam = stats.boxcox(x)  # returns transformed values + MLE λ\n",
        "        bc_min = float(np.min(bc))\n",
        "        bc_rng = float(np.max(bc) - bc_min) or 1.0  # avoid zero range\n",
        "\n",
        "        params[col] = {\"lambda\": float(lam), \"min\": bc_min, \"range\": bc_rng, \"eps\": eps}\n",
        "    return params\n",
        "\n",
        "\n",
        "# ========================\n",
        "# Box–Cox + Min–Max → [0,1]\n",
        "# ========================\n",
        "def apply_boxcox_minmax_transform(\n",
        "    df: pd.DataFrame,\n",
        "    params: Dict[str, dict],\n",
        "    columns: List[str] = (\"VL\", \"CD4\"),\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Apply Box–Cox with fitted λ, then scale to [0,1] via stored min/range.\n",
        "    NaNs are preserved.\n",
        "    \"\"\"\n",
        "    out = df.copy()\n",
        "    for col in columns:\n",
        "        if col not in out.columns:\n",
        "            continue\n",
        "        p = params[col]\n",
        "        mask = out[col].notna().to_numpy()\n",
        "        if not mask.any():\n",
        "            continue\n",
        "\n",
        "        x = out.loc[mask, col].astype(float).to_numpy() + p[\"eps\"]\n",
        "        bc = sp_boxcox(x, p[\"lambda\"])  # fixed-λ forward transform\n",
        "        scaled = (bc - p[\"min\"]) / p[\"range\"]\n",
        "        out.loc[mask, col] = scaled\n",
        "    return out\n",
        "\n",
        "\n",
        "# ========================\n",
        "# Inverse Box–Cox (Torch)\n",
        "# ========================\n",
        "def inverse_boxcox_torch(bc: torch.Tensor, lmbda: float, eps: float = 1e-3) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Invert Box–Cox (on Box–Coxed values) to original x (minus eps shift).\n",
        "    \"\"\"\n",
        "    if lmbda == 0.0:\n",
        "        return torch.exp(bc) - eps\n",
        "    base = torch.clamp(lmbda * bc + 1.0, min=1e-12)  # numeric safety\n",
        "    return torch.pow(base, 1.0 / lmbda) - eps\n",
        "\n",
        "\n",
        "# ========================\n",
        "# Back-transform features\n",
        "# ========================\n",
        "def backtransform_art_tensor(\n",
        "    tensor: torch.Tensor,\n",
        "    feature_names: List[str],\n",
        "    transform_params: Dict[str, dict],\n",
        "    real_columns: List[str] = (\"VL\", \"CD4\"),\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Expects `tensor` with real cols in scaled Box–Cox space ([0,1]).\n",
        "    Steps: unscale to Box–Cox → inverse Box–Cox → subtract eps.\n",
        "    \"\"\"\n",
        "    x = tensor.detach().clone()\n",
        "    idx_map = {n: i for i, n in enumerate(feature_names)}\n",
        "\n",
        "    for col in real_columns:\n",
        "        if col not in idx_map:\n",
        "            continue\n",
        "        i = idx_map[col]\n",
        "        p = transform_params[col]\n",
        "        bc = x[:, i] * p[\"range\"] + p[\"min\"]\n",
        "        x[:, i] = inverse_boxcox_torch(bc, p[\"lambda\"], p[\"eps\"])\n",
        "\n",
        "    return pd.DataFrame(x.cpu().numpy(), columns=feature_names)"
      ],
      "metadata": {
        "id": "_vahJWh75E8r"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_all()\n",
        "\n",
        "# === Load and Preprocess Raw Data ===\n",
        "raw_url = \"https://figshare.com/ndownloader/files/40584980\"\n",
        "DROP_COLS = [\"VL (M)\", \"CD4 (M)\", \"Drug (M)\"]\n",
        "\n",
        "# 1) Read only the columns we need\n",
        "_all_cols = pd.read_csv(raw_url, nrows=0).columns.tolist()\n",
        "usecols = [c for c in _all_cols if c not in DROP_COLS]\n",
        "All_Data = pd.read_csv(raw_url, usecols=usecols)\n",
        "\n",
        "# 2) Map numeric codes -> human-readable labels (with typo fixed)\n",
        "NUM2STR = {\n",
        "    \"Gender\":          {1: \"Male\", 2: \"Female\"},\n",
        "    \"Ethnic\":          {1: \"Asian\", 2: \"African\", 3: \"Caucasian\", 4: \"Other\"},\n",
        "    \"Base Drug Combo\": {\n",
        "        0: \"FTC + TDF\", 1: \"3TC + ABC\", 2: \"FTC + TAF\",\n",
        "        3: \"DRV + FTC + TDF\", 4: \"FTC + RTVB + TDF\", 5: \"Other\"\n",
        "    },\n",
        "    \"Comp. INI\":       {0: \"DTG\", 1: \"RAL\", 2: \"EVG\", 3: \"Not Applied\"},\n",
        "    \"Comp. NNRTI\":     {0: \"NVP\", 1: \"EFV\", 2: \"RPV\", 3: \"Not Applied\"},\n",
        "    \"Extra PI\":        {0: \"DRV\", 1: \"RTVB\", 2: \"LPV\", 3: \"RTV\", 4: \"ATV\", 5: \"Not Applied\"},\n",
        "    \"Extra pk-En\":     {0: \"False\", 1: \"True\"},   # <-- fixed typo\n",
        "}\n",
        "\n",
        "for col, mapping in NUM2STR.items():\n",
        "    if col in All_Data.columns:\n",
        "        mapped = All_Data[col].map(mapping)\n",
        "        if mapped.isnull().any():\n",
        "            bad = All_Data.loc[mapped.isnull(), col].unique()\n",
        "            raise ValueError(f\"Unmapped codes in '{col}': {bad}\")\n",
        "        All_Data[col] = mapped.astype(\"category\")\n",
        "\n",
        "for cont in (\"VL\", \"CD4\"):\n",
        "    if cont in All_Data.columns:\n",
        "        All_Data[cont] = pd.to_numeric(All_Data[cont], errors=\"coerce\")"
      ],
      "metadata": {
        "id": "Oridy88n5H5L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_all()\n",
        "\n",
        "# === Create smaller demo dataset for collaborators ===\n",
        "SEL_COLS = [\n",
        "    \"VL\", \"CD4\", \"Gender\", \"Ethnic\", \"Base Drug Combo\",\n",
        "    \"Extra PI\", \"Extra pk-En\", \"PatientID\", \"Timestep\"\n",
        "]\n",
        "\n",
        "# Filter by PatientID, select only wanted columns, reset index\n",
        "Sub_Data = (\n",
        "    All_Data.loc[All_Data[\"PatientID\"] < 300, SEL_COLS]\n",
        "            .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "missing = set(SEL_COLS) - set(All_Data.columns)\n",
        "if missing:\n",
        "    raise KeyError(f\"Missing expected columns in All_Data: {missing}\")\n"
      ],
      "metadata": {
        "id": "_SJ878SY5LcV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_all()\n",
        "# --- Canonical column names for downstream ---\n",
        "REAL_COLS = [\"VL\", \"CD4\"]\n",
        "CAT_SIZES = {\n",
        "    \"Gender\": 2,\n",
        "    \"Ethnic\": 4,\n",
        "    \"Base_Drug_Combo\": 6,\n",
        "    \"Extra_PI\": 6,\n",
        "    \"Extra_pk_En\": 2,\n",
        "}\n",
        "\n",
        "# Embedding sizes policy\n",
        "EMB_SIZE_FOR = {\n",
        "    \"real\": lambda k: 1,     # passthrough\n",
        "    \"bin\":  lambda k: 2,     # small embedding for binary\n",
        "    \"cat\":  lambda k: 4,     # fixed 4-dim for multiclass\n",
        "}\n",
        "\n",
        "def build_dtype(real_cols, cat_sizes, emb_policy=EMB_SIZE_FOR, start_idx=0):\n",
        "    rows = []\n",
        "    idx = start_idx\n",
        "    i = 0\n",
        "\n",
        "    # Reals\n",
        "    for name in real_cols:\n",
        "        k = 1\n",
        "        emb = emb_policy[\"real\"](k)\n",
        "        rows.append([i, name, \"real\", k, emb, idx, idx + k])\n",
        "        idx += k; i += 1\n",
        "\n",
        "    # Categorical (emit num_classes one-hot dims)\n",
        "    for name, k in cat_sizes.items():\n",
        "        ftype = \"bin\" if k == 2 else \"cat\"\n",
        "        emb = emb_policy[ftype](k)\n",
        "        rows.append([i, name, ftype, k, emb, idx, idx + k])\n",
        "        idx += k; i += 1\n",
        "\n",
        "    dtype = pd.DataFrame(rows, columns=[\n",
        "        \"index\", \"name\", \"type\", \"num_classes\",\n",
        "        \"embedding_size\", \"index_start\", \"index_end\"\n",
        "    ])\n",
        "    return dtype\n",
        "\n",
        "def validate_dtype(dtype: pd.DataFrame, expected_total=None):\n",
        "    d = dtype.sort_values(\"index\").reset_index(drop=True)\n",
        "    # contiguous spans\n",
        "    starts = d[\"index_start\"].to_numpy()\n",
        "    ends   = d[\"index_end\"].to_numpy()\n",
        "    assert np.all(starts[1:] == ends[:-1]), \"Spans must be contiguous\"\n",
        "    # monotonic & positive widths\n",
        "    assert np.all(ends > starts), \"Each span must have positive width\"\n",
        "    # total width\n",
        "    total = int(ends[-1])\n",
        "    if expected_total is not None:\n",
        "        assert total == expected_total, f\"Total dims {total} != expected {expected_total}\"\n",
        "    return total\n",
        "\n",
        "def expand_feature_names(dtype: pd.DataFrame):\n",
        "    names = []\n",
        "    for _, r in dtype.iterrows():\n",
        "        if r[\"type\"] == \"real\":\n",
        "            names.append(r[\"name\"])\n",
        "        else:\n",
        "            k = int(r[\"num_classes\"])\n",
        "            names.extend([f\"{r['name']}_{j}\" for j in range(k)])\n",
        "    return names\n",
        "\n",
        "# ---- Building schema ----\n",
        "dtype = build_dtype(REAL_COLS, CAT_SIZES)\n",
        "TOTAL_DIM = validate_dtype(dtype)\n",
        "FEATURE_NAMES = expand_feature_names(dtype)\n",
        "assert len(FEATURE_NAMES) == TOTAL_DIM\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yNLB6S-A5NPO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_all()\n",
        "\n",
        "Sub_Data = BinCat2Num(Sub_Data)\n",
        "\n",
        "# 2) Rename once so columns match  dtype/schema (snake_case)\n",
        "NAME_MAP = {\n",
        "    \"Base Drug Combo\": \"Base_Drug_Combo\",\n",
        "    \"Extra PI\":        \"Extra_PI\",\n",
        "    \"Extra pk-En\":     \"Extra_pk_En\",\n",
        "}\n",
        "Sub_Data = Sub_Data.rename(columns=NAME_MAP)"
      ],
      "metadata": {
        "id": "MbKiMFuQ5Nvj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1u9kZSdz5h4M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qqyLiLrP5h6j"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x9RiWTRQ5h8z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###===###\n",
        "seed_all()\n",
        "\n",
        "def _collect_schema(dtype: pd.DataFrame) -> tuple[list[str], dict]:\n",
        "    \"\"\"Return (real_cols, cat_spec) from dtype manifest.\"\"\"\n",
        "    real_cols: list[str] = []\n",
        "    cat_spec: dict[str, int] = {}\n",
        "    for _, r in dtype.iterrows():\n",
        "        if r[\"type\"] == \"real\":\n",
        "            real_cols.append(r[\"name\"])\n",
        "        else:\n",
        "            cat_spec[r[\"name\"]] = int(r[\"num_classes\"])\n",
        "    return real_cols, cat_spec\n",
        "\n",
        "\n",
        "def _one_hot_numpy(int_codes: np.ndarray, num_classes: int) -> np.ndarray:\n",
        "    \"\"\"Fast one-hot using NumPy indexing; shape: [N, num_classes].\"\"\"\n",
        "    return np.eye(num_classes, dtype=np.float32)[int_codes]\n",
        "\n",
        "\n",
        "def ExecuteB002(\n",
        "    My_df: pd.DataFrame,\n",
        "    dtype: pd.DataFrame,\n",
        "    Hyper005_BatchSize: int,\n",
        "    *,\n",
        "    seq_len: int = 60,\n",
        "    feature_names: Optional[list[str]] = None,\n",
        "    transform_params: Optional[dict] = None,   # from compute_boxcox_params (preferred)\n",
        "    use_legacy_scaling: bool = False,          # fallback to old VL/CD4 scheme if True\n",
        ") -> tuple[np.ndarray, torch.utils.data.DataLoader, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Build a 3D design tensor [N_patients, L, D] from a flat dataframe using `dtype`\n",
        "    as the authoritative schema, then return (numpy_copy, DataLoader, flat_train_tensor).\n",
        "\n",
        "    Assumptions:\n",
        "      - My_df already has integer-coded categoricals (via BinCat2Num) and snake_case names.\n",
        "      - If `transform_params` is given, VL/CD4 are scaled using Box–Cox->[0,1] from params.\n",
        "        Otherwise, if `use_legacy_scaling=True`, fall back to the original constants.\n",
        "\n",
        "    Returns:\n",
        "      - data_np: np.ndarray of shape [N, L, D]\n",
        "      - trn_loader: DataLoader over TensorDataset((X), (lengths))\n",
        "      - All_Trainable_Data: torch.FloatTensor of shape [N*L, D]\n",
        "    \"\"\"\n",
        "    # --- 0) Schema & sizes ---\n",
        "    total_dim: int = int(dtype[\"index_end\"].max())\n",
        "    real_cols, cat_spec = _collect_schema(dtype)\n",
        "\n",
        "    # Optional: expand names once, only for robust checks\n",
        "    if feature_names is None:\n",
        "        feature_names = []\n",
        "        for _, r in dtype.iterrows():\n",
        "            if r[\"type\"] == \"real\":\n",
        "                feature_names.append(r[\"name\"])\n",
        "            else:\n",
        "                k = int(r[\"num_classes\"])\n",
        "                feature_names.extend([f\"{r['name']}_{j}\" for j in range(k)])\n",
        "    assert len(feature_names) == total_dim, \"FEATURE_NAMES length must equal TOTAL_DIM\"\n",
        "\n",
        "    # --- 1) Input hygiene & shape accounting ---\n",
        "    # Ensure required columns exist\n",
        "    needed_cols = set(real_cols) | set(cat_spec.keys())\n",
        "    missing = needed_cols - set(My_df.columns)\n",
        "    if missing:\n",
        "        raise KeyError(f\"Missing columns for design matrix: {sorted(missing)}\")\n",
        "\n",
        "    # How many rows and patients? We assume flat [N*L] rows and reshape to [-1, L, D].\n",
        "    n_rows = len(My_df)\n",
        "    if n_rows % seq_len != 0:\n",
        "        raise ValueError(f\"Row count {n_rows} is not divisible by seq_len={seq_len}. \"\n",
        "                         f\"Please align or pad/truncate.\")\n",
        "    n_patients = n_rows // seq_len\n",
        "\n",
        "    # --- 2) Allocate and fill the design matrix in one pass ---\n",
        "    X = np.zeros((n_rows, total_dim), dtype=np.float32)\n",
        "\n",
        "    # 2a) Real columns (as float32)\n",
        "    for _, r in dtype.iterrows():\n",
        "        if r[\"type\"] != \"real\":\n",
        "            continue\n",
        "        col = r[\"name\"]\n",
        "        i0, i1 = int(r[\"index_start\"]), int(r[\"index_end\"])\n",
        "        x = pd.to_numeric(My_df[col], errors=\"coerce\").astype(np.float32).to_numpy()\n",
        "        # Forward transforms:\n",
        "        if transform_params is not None:\n",
        "            # (Assumes My_df[col] is in raw units; apply Box–Cox -> [0,1])\n",
        "            p = transform_params[col]\n",
        "            x_bc = stats.boxcox(x + p[\"eps\"], p[\"lambda\"])  # fixed-λ forward\n",
        "            x = (x_bc - p[\"min\"]) / (p[\"range\"] if p[\"range\"] != 0 else 1.0)\n",
        "        elif use_legacy_scaling:\n",
        "            # Legacy scheme from the original code\n",
        "            if col == \"VL\":\n",
        "                x = x.copy()\n",
        "                x[x >= 100000] = 100000\n",
        "                x = stats.boxcox(x + 1.0, -0.11862349303078497)\n",
        "                x = x / 6.278682\n",
        "            elif col == \"CD4\":\n",
        "                x = np.log(x + 1.0)\n",
        "                x = x / 13.311331132544721\n",
        "        # else: pass-through (already pre-scaled upstream)\n",
        "        X[:, i0:i1] = x.reshape(-1, 1)\n",
        "\n",
        "    # 2b) Categorical blocks (strict one-hot using known class counts & column order)\n",
        "    for _, r in dtype.iterrows():\n",
        "        if r[\"type\"] == \"real\":\n",
        "            continue\n",
        "        name = r[\"name\"]\n",
        "        k = int(r[\"num_classes\"])\n",
        "        i0, i1 = int(r[\"index_start\"]), int(r[\"index_end\"])\n",
        "        codes = pd.to_numeric(My_df[name], errors=\"raise\").astype(\"int64\").to_numpy()\n",
        "        # Validate codes are within range [0, k-1]\n",
        "        min_c, max_c = codes.min(), codes.max()\n",
        "        if min_c < 0 or max_c >= k:\n",
        "            raise ValueError(f\"Column '{name}' has codes outside [0,{k-1}]: \"\n",
        "                             f\"observed [{min_c},{max_c}]\")\n",
        "        X[:, i0:i1] = _one_hot_numpy(codes, k)\n",
        "\n",
        "    # --- 3) Reshape to [N, L, D] ---\n",
        "    X3 = X.reshape(n_patients, seq_len, total_dim)\n",
        "\n",
        "    # --- 4) Torch dataset/loader (no extra deep copies) ---\n",
        "    X_tensor = torch.from_numpy(X3).float()             # [N, L, D]\n",
        "    lengths  = torch.full((n_patients, 1, 1), seq_len)  # keep original shape contract\n",
        "\n",
        "    ds = TensorDataset(X_tensor, lengths)\n",
        "    trn_loader = DataLoader(ds, batch_size=Hyper005_BatchSize, shuffle=True, drop_last=True)\n",
        "\n",
        "    # --- 5) Flat trainable tensor [N*L, D] without looping over the loader ---\n",
        "    All_Trainable_Data = X_tensor.reshape(-1, total_dim).contiguous()\n",
        "\n",
        "    # Return a NumPy copy in case downstream plotting expects ndarray\n",
        "    data_np = X3.copy()\n",
        "    return data_np, trn_loader, All_Trainable_Data\n"
      ],
      "metadata": {
        "id": "BXqJdpFB5iDk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "St45tc6x7DgM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LVCgsOh17DjB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VVu06ZQC7Dm8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_all()\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Time embedding (sinusoidal)\n",
        "# -----------------------------\n",
        "def build_sinusoidal_table(n_steps: int, dim: int, device=None) -> torch.Tensor:\n",
        "    \"\"\"Non-trainable sinusoidal embeddings [n_steps, dim].\"\"\"\n",
        "    pe = torch.zeros(n_steps, dim, device=device)\n",
        "    position = torch.arange(0, n_steps, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, dim, 2, device=device, dtype=torch.float32) *\n",
        "                         (-math.log(10000.0) / dim))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    return pe\n",
        "\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    \"\"\"t -> embedding -> FiLM (scale, shift).\"\"\"\n",
        "    def __init__(self, n_steps: int, dim: int, out_channels: int):\n",
        "        super().__init__()\n",
        "        self.table = nn.Embedding(n_steps, dim)\n",
        "        with torch.no_grad():\n",
        "            self.table.weight.copy_(build_sinusoidal_table(n_steps, dim))\n",
        "        self.table.weight.requires_grad_(False)  # fixed sinusoidal\n",
        "        # small MLP to map time embedding -> FiLM params\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, out_channels * 2),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(out_channels * 2, out_channels * 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, t: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Returns (gamma, beta) with shape [B, C] for FiLM conditioning.\n",
        "        \"\"\"\n",
        "        h = self.table(t)          # [B, dim]\n",
        "        h = self.mlp(h)            # [B, 2C]\n",
        "        gamma, beta = torch.chunk(h, 2, dim=-1)\n",
        "        return gamma, beta\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Residual 1D conv block\n",
        "# -----------------------------\n",
        "class ResBlock1D(nn.Module):\n",
        "    def __init__(self, channels: int, kernel_size: int = 3, groups: int = 8, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        padding = kernel_size // 2\n",
        "        g = max(1, min(groups, channels))  # clamp groups\n",
        "        self.norm1 = nn.GroupNorm(g, channels)\n",
        "        self.conv1 = nn.Conv1d(channels, channels, kernel_size, padding=padding)\n",
        "        self.norm2 = nn.GroupNorm(g, channels)\n",
        "        self.conv2 = nn.Conv1d(channels, channels, kernel_size, padding=padding)\n",
        "        self.act   = nn.SiLU()\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x: torch.Tensor, gamma: torch.Tensor | None = None, beta: torch.Tensor | None = None):\n",
        "        \"\"\"\n",
        "        x: [B, C, L]; gamma/beta: [B, C] (FiLM).\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        y = self.norm1(x)\n",
        "        if gamma is not None and beta is not None:\n",
        "            # FiLM: per-channel scale/shift\n",
        "            y = y * (1 + gamma.unsqueeze(-1)) + beta.unsqueeze(-1)\n",
        "        y = self.act(y)\n",
        "        y = self.conv1(y)\n",
        "\n",
        "        y = self.norm2(y)\n",
        "        y = self.act(y)\n",
        "        y = self.dropout(y)\n",
        "        y = self.conv2(y)\n",
        "\n",
        "        return y + residual\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3) U-Net (1D over time)\n",
        "# -----------------------------\n",
        "class MyUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Clean 1D U-Net with FiLM time conditioning.\n",
        "\n",
        "    Input : x_t [B, 1, Feat_Dim, Len]\n",
        "    Output: noise_hat with same shape.\n",
        "\n",
        "    Args:\n",
        "        Cur_HD:     base channel width (e.g., 128)\n",
        "        N_Steps:    diffusion steps for time embedding\n",
        "        Time_Emb_Dim: size of sinusoidal time embedding\n",
        "        Feat_Dim:   number of features (default 22)\n",
        "        Len:        sequence length along time (e.g., 100 or 60)\n",
        "        levels:     number of down/up levels\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 Cur_HD: int,\n",
        "                 N_Steps: int = 1000,\n",
        "                 Time_Emb_Dim: int = 128,\n",
        "                 Feat_Dim: int = 22,\n",
        "                 Len: int = 100,\n",
        "                 levels: int = 3):\n",
        "        super().__init__()\n",
        "        self.Feat_Dim = Feat_Dim\n",
        "        self.Len = Len\n",
        "        C = Cur_HD\n",
        "\n",
        "        # Pre/post: feature mixing at each time step (per-time linear)\n",
        "        self.bt1 = nn.Sequential(nn.Linear(Feat_Dim, C), nn.SiLU(), nn.Linear(C, C))\n",
        "        self.bt2 = nn.Sequential(nn.Linear(C, C), nn.SiLU(), nn.Linear(C, Feat_Dim))\n",
        "\n",
        "        # Time embedding -> FiLM for each stage (we reuse the same module per stage)\n",
        "        self.time_emb_down = nn.ModuleList([TimeEmbedding(N_Steps, Time_Emb_Dim, C) for _ in range(levels)])\n",
        "        self.time_emb_up   = nn.ModuleList([TimeEmbedding(N_Steps, Time_Emb_Dim, C) for _ in range(levels)])\n",
        "        self.time_emb_mid  = TimeEmbedding(N_Steps, Time_Emb_Dim, C)\n",
        "\n",
        "        # Down path\n",
        "        self.down_blocks = nn.ModuleList()\n",
        "        self.downsample  = nn.ModuleList()\n",
        "        for _ in range(levels):\n",
        "            self.down_blocks.append(ResBlock1D(C))\n",
        "            self.downsample.append(nn.Conv1d(C, C, kernel_size=4, stride=2, padding=1))  # /2\n",
        "\n",
        "        # Middle\n",
        "        self.mid_block1 = ResBlock1D(C)\n",
        "        self.mid_block2 = ResBlock1D(C)\n",
        "\n",
        "        # Up path\n",
        "        self.upsample   = nn.ModuleList()\n",
        "        self.up_blocks  = nn.ModuleList()\n",
        "        for _ in range(levels):\n",
        "            self.upsample.append(nn.ConvTranspose1d(C, C, kernel_size=4, stride=2, padding=1))  # x2\n",
        "            self.up_blocks.append(ResBlock1D(C))\n",
        "\n",
        "        # Final conv over time back to channel C then bt2 to Feat_Dim\n",
        "        self.out_conv = nn.Conv1d(C, C, kernel_size=3, padding=1)\n",
        "\n",
        "    def _to_1d(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # [B, 1, Feat, Len] -> [B, Len, Feat] -> bt1 -> [B, Len, C] -> [B, C, Len]\n",
        "        x = x.squeeze(1).transpose(1, 2)                 # [B, Len, Feat]\n",
        "        x = self.bt1(x)                                  # [B, Len, C]\n",
        "        x = x.transpose(1, 2).contiguous()               # [B, C, Len]\n",
        "        return x\n",
        "\n",
        "    def _to_2d(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # [B, C, Len] -> [B, Len, C] -> bt2 -> [B, Len, Feat] -> [B, 1, Feat, Len]\n",
        "        x = x.transpose(1, 2).contiguous()               # [B, Len, C]\n",
        "        x = self.bt2(x)                                  # [B, Len, Feat]\n",
        "        x = x.transpose(1, 2).unsqueeze(1).contiguous()  # [B, 1, Feat, Len]\n",
        "        return x\n",
        "\n",
        "    def forward(self, x_t: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x_t: [B, 1, Feat_Dim, Len]\n",
        "        t  : [B] int64 diffusion steps\n",
        "        \"\"\"\n",
        "        B = x_t.size(0)\n",
        "        x = self._to_1d(x_t)   # [B, C, L]\n",
        "\n",
        "        # Down\n",
        "        skip = []\n",
        "        for i, (block, down) in enumerate(zip(self.down_blocks, self.downsample)):\n",
        "            gamma, beta = self.time_emb_down[i](t)      # [B, C]\n",
        "            x = block(x, gamma, beta)                   # [B, C, L]\n",
        "            skip.append(x)\n",
        "            x = down(x)                                 # [B, C, L/2]\n",
        "\n",
        "        # Mid\n",
        "        g_mid, b_mid = self.time_emb_mid(t)\n",
        "        x = self.mid_block1(x, g_mid, b_mid)\n",
        "        x = self.mid_block2(x, g_mid, b_mid)\n",
        "\n",
        "        # Up\n",
        "        for i, (up, block) in enumerate(zip(self.upsample, self.up_blocks)):\n",
        "            x = up(x)                                   # [B, C, L*2]\n",
        "            # Align length (due to odd L) – crop or pad as needed\n",
        "            if x.size(-1) > skip[-1].size(-1):\n",
        "                x = x[..., :skip[-1].size(-1)]\n",
        "            elif x.size(-1) < skip[-1].size(-1):\n",
        "                pad = skip[-1].size(-1) - x.size(-1)\n",
        "                x = F.pad(x, (0, pad))\n",
        "            x = x + skip.pop()                          # simple skip add\n",
        "            gamma, beta = self.time_emb_up[i](t)\n",
        "            x = block(x, gamma, beta)\n",
        "\n",
        "        x = self.out_conv(x)                            # [B, C, L]\n",
        "        out = self._to_2d(x)                            # [B, 1, Feat_Dim, Len]\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "2O7MK5197DuD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_all()\n",
        "\n",
        "# -------------------------\n",
        "# AMP (version-safe imports)\n",
        "# -------------------------\n",
        "try:\n",
        "    # PyTorch >= 2.4\n",
        "    from torch.amp import GradScaler, autocast\n",
        "    _NEW_AMP = True\n",
        "except Exception:\n",
        "    # PyTorch <= 2.3\n",
        "    from torch.cuda.amp import GradScaler, autocast\n",
        "    _NEW_AMP = False\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# utilities\n",
        "# -------------------------\n",
        "def correlation(x: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Column-wise correlation of a 2D tensor.\n",
        "    x: [N, D] (flattened across time if needed)\n",
        "    \"\"\"\n",
        "    x = x - x.mean(dim=0, keepdim=True)\n",
        "    std = x.norm(dim=0, keepdim=True).clamp_min(eps)\n",
        "    x = x / std\n",
        "    return x.t() @ x  # (D,D)\n",
        "\n",
        "\n",
        "def Recentre_Data(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Map [0,1] -> [-1,1].\"\"\"\n",
        "    return x.mul(2.0).sub(1.0)\n",
        "\n",
        "\n",
        "def LoadPreTrain(content = [False, 'U_SD', 0]):\n",
        "    \"\"\"\n",
        "    content = [Continue(bool), ckpt_name(str), epoch_idx(int)]\n",
        "    Loads './Z002_Parameters/Epoch_XXXX/<ckpt_name>' if Continue=True.\n",
        "    \"\"\"\n",
        "    Continue, name, ep = content\n",
        "    if not Continue:\n",
        "        return 0\n",
        "\n",
        "    folder = Path(\"./Z002_Parameters\") / f\"Epoch_{int(ep):04d}\"\n",
        "    ckpt_path = folder / name\n",
        "    if not ckpt_path.exists():\n",
        "        raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
        "\n",
        "    # map_location='cpu' so we can .to(device) later\n",
        "    state = torch.load(str(ckpt_path), map_location=\"cpu\")\n",
        "    return state\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainCfg:\n",
        "    hd: int\n",
        "    n_steps: int\n",
        "    beta_min: float\n",
        "    beta_max: float\n",
        "    batch_size: int\n",
        "    epochs: int\n",
        "    lr: float\n",
        "    target_len: int = 100            # pad/truncate length on time axis\n",
        "    recon_weight: float = 20.0       # λ for reconstruction loss\n",
        "    grad_clip: float = 1.0           # 0 or None to disable\n",
        "    use_amp: bool = True             # AMP for speed on CUDA\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# trainer\n",
        "# -------------------------\n",
        "class ExecuteB003(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        All_Trainable_Data: torch.Tensor,\n",
        "        Hyper001_HD,\n",
        "        Hyper002_NSteps, Hyper003_MinBeta, Hyper004_MaxBeta,\n",
        "        Hyper005_BatchSize, Hyper006_NEpochs,\n",
        "        Hyper007_LR,\n",
        "        data_types,\n",
        "        continue_info = [False, 'U-SD', 0],\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # device\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # config (retain original signature; wrap as a dataclass internally)\n",
        "        self.cfg = TrainCfg(\n",
        "            hd=Hyper001_HD,\n",
        "            n_steps=Hyper002_NSteps,\n",
        "            beta_min=Hyper003_MinBeta,\n",
        "            beta_max=Hyper004_MaxBeta,\n",
        "            batch_size=Hyper005_BatchSize,\n",
        "            epochs=Hyper006_NEpochs,\n",
        "            lr=Hyper007_LR,\n",
        "            target_len=100,\n",
        "        )\n",
        "\n",
        "        # recenter & cache real correlation (if want to add a penalty later)\n",
        "        atd = Recentre_Data(All_Trainable_Data)  # [N*L, D]\n",
        "        with torch.no_grad():\n",
        "            self.correlation_real = correlation(atd).to(self.device)\n",
        "\n",
        "        # model\n",
        "        self.UNet = MyUNet(\n",
        "            Cur_HD=self.cfg.hd,\n",
        "            N_Steps=self.cfg.n_steps,\n",
        "            Time_Emb_Dim=128,\n",
        "            Feat_Dim=22,\n",
        "            Len=self.cfg.target_len,\n",
        "        ).to(self.device)\n",
        "\n",
        "        # (optional) resume\n",
        "        U_SD = LoadPreTrain(continue_info)\n",
        "        if U_SD != 0:\n",
        "            self.UNet.load_state_dict(U_SD, strict=True)\n",
        "            self.PreviousEpoch = int(continue_info[2])\n",
        "        else:\n",
        "            self.PreviousEpoch = 0\n",
        "\n",
        "        # diffusion schedule as buffers (create ON DEVICE so indexing works)\n",
        "        betas = torch.linspace(\n",
        "            self.cfg.beta_min, self.cfg.beta_max, self.cfg.n_steps,\n",
        "            dtype=torch.float32, device=self.device\n",
        "        )\n",
        "        alphas = 1.0 - betas\n",
        "        alphas_bar = torch.cumprod(alphas, dim=0)\n",
        "        self.register_buffer(\"betas\", betas)\n",
        "        self.register_buffer(\"alphas\", alphas)\n",
        "        self.register_buffer(\"alphas_bar\", alphas_bar)\n",
        "\n",
        "        # optim & loss\n",
        "        self.criterion = nn.MSELoss(reduction=\"none\")  # we reduce manually with masks\n",
        "        self.optimiser = Adam(self.UNet.parameters(), lr=self.cfg.lr, betas=(0.9, 0.999))\n",
        "\n",
        "        # AMP scaler (version-safe)\n",
        "        if _NEW_AMP:\n",
        "            # PyTorch >= 2.4\n",
        "            self.scaler = GradScaler(\n",
        "                \"cuda\" if self.device.type == \"cuda\" else \"cpu\",\n",
        "                enabled=self.cfg.use_amp,\n",
        "            )\n",
        "        else:\n",
        "            # PyTorch <= 2.3\n",
        "            self.scaler = GradScaler(enabled=(self.cfg.use_amp and self.device.type == \"cuda\"))\n",
        "\n",
        "    # ------------- core ops -------------\n",
        "    def Manual_Forward(self, x_T0: torch.Tensor, Cur_T: torch.Tensor, Cur_Eta: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x_T0: [B,1,C,T], Cur_T: [B], Cur_Eta: [B,1,C,T]\n",
        "        \"\"\"\n",
        "        a_bar = self.alphas_bar[Cur_T].view(-1, 1, 1, 1)  # [B,1,1,1]\n",
        "        return a_bar.sqrt() * x_T0 + (1.0 - a_bar).sqrt() * Cur_Eta\n",
        "\n",
        "    def Network_Backward(self, Fuzzy_X: torch.Tensor, Cur_T: torch.Tensor) -> torch.Tensor:\n",
        "        return self.UNet(Fuzzy_X, Cur_T)  # expects t=[B]\n",
        "\n",
        "    def Reconstruct(self, Fuzzy_X, Cur_T, Cur_Eta, Predicted_Eta):\n",
        "        a_bar = self.alphas_bar[Cur_T].view(-1, 1, 1, 1)\n",
        "        # blend true+pred noise like original (can set blend=0.0 to disable)\n",
        "        blend = torch.rand_like(Cur_Eta)\n",
        "        Pred_Noise = blend * Cur_Eta + (1.0 - blend) * Predicted_Eta\n",
        "        x0_hat = (Fuzzy_X - (1.0 - a_bar).sqrt() * Pred_Noise) / (a_bar.sqrt() + 1e-8)\n",
        "        return x0_hat\n",
        "\n",
        "    # ------------- helpers -------------\n",
        "    def _pad_time(self, x: torch.Tensor, tgt_len: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        x: [B,1,C,L]  -> returns (x_padded [B,1,C,tgt_len], mask [B,1,1,tgt_len] with 1=valid)\n",
        "        \"\"\"\n",
        "        B, _, C, L = x.shape\n",
        "        if L == tgt_len:\n",
        "            mask = x.new_ones((B, 1, 1, L))\n",
        "            return x, mask\n",
        "        if L > tgt_len:\n",
        "            x = x[..., :tgt_len]\n",
        "            mask = x.new_ones((B, 1, 1, tgt_len))\n",
        "            return x, mask\n",
        "        pad = tgt_len - L\n",
        "        x = F.pad(x, (0, pad), value=0.0)      # pad time on the right\n",
        "        mask = torch.cat([x.new_ones((B,1,1,L)), x.new_zeros((B,1,1,pad))], dim=-1)\n",
        "        return x, mask\n",
        "\n",
        "    def _masked_mse(self, pred: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        pred/target: [B,1,C,T], mask: [B,1,1,T]\n",
        "        \"\"\"\n",
        "        loss = self.criterion(pred, target)  # [B,1,C,T]\n",
        "        loss = loss * mask                  # broadcast over C\n",
        "        denom = mask.sum() * pred.size(2)   # valid elements = sum over T * C\n",
        "        denom = denom.clamp_min(1.0)\n",
        "        return loss.sum() / denom\n",
        "\n",
        "    # ------------- train loop -------------\n",
        "    def train(self, My_Loader):\n",
        "        self.UNet.train()\n",
        "\n",
        "        for e in range(self.cfg.epochs - self.PreviousEpoch):\n",
        "            tot, l_diff, l_rec = 0.0, 0.0, 0.0\n",
        "\n",
        "            for Cur_X, lengths in My_Loader:\n",
        "                # Cur_X: [B, L, D] in [0,1]; lengths shape like (B,1,1) but constant\n",
        "                B, L, D = Cur_X.shape\n",
        "                Cur_X = Cur_X.to(self.device).unsqueeze(1).transpose(2, 3)  # -> [B,1,D,L]\n",
        "                Cur_X = Recentre_Data(Cur_X)\n",
        "\n",
        "                # pad to target length\n",
        "                Cur_X_padded, mask = self._pad_time(Cur_X, self.cfg.target_len)\n",
        "\n",
        "                # sample t and noise\n",
        "                Cur_T   = torch.randint(0, self.cfg.n_steps, (B,), device=self.device)\n",
        "                Cur_Eta = torch.randn_like(Cur_X_padded)\n",
        "\n",
        "                # version-safe autocast\n",
        "                if _NEW_AMP:\n",
        "                    ac_kwargs = dict(device_type=\"cuda\", enabled=(self.cfg.use_amp and self.device.type == \"cuda\"))\n",
        "                else:\n",
        "                    ac_kwargs = dict(enabled=(self.cfg.use_amp and self.device.type == \"cuda\"))\n",
        "\n",
        "                with autocast(**ac_kwargs):\n",
        "                    Fuzzy_X  = self.Manual_Forward(Cur_X_padded, Cur_T, Cur_Eta)\n",
        "                    Pred_Eta = self.Network_Backward(Fuzzy_X, Cur_T)\n",
        "                    X_hat    = self.Reconstruct(Fuzzy_X, Cur_T, Cur_Eta, Pred_Eta)\n",
        "\n",
        "                    # masked losses (only valid time steps)\n",
        "                    loss_diff = self._masked_mse(Pred_Eta, Cur_Eta, mask)\n",
        "                    loss_rec  = self._masked_mse(X_hat, Cur_X_padded, mask) * self.cfg.recon_weight\n",
        "                    loss      = loss_diff + loss_rec\n",
        "\n",
        "                self.optimiser.zero_grad(set_to_none=True)\n",
        "                self.scaler.scale(loss).backward()\n",
        "                if self.cfg.grad_clip and self.cfg.grad_clip > 0:\n",
        "                    self.scaler.unscale_(self.optimiser)\n",
        "                    nn.utils.clip_grad_norm_(self.UNet.parameters(), self.cfg.grad_clip)\n",
        "                self.scaler.step(self.optimiser)\n",
        "                self.scaler.update()\n",
        "\n",
        "                tot   += float(loss.detach())\n",
        "                l_diff+= float(loss_diff.detach())\n",
        "                l_rec += float(loss_rec.detach())\n",
        "\n",
        "            # epoch summary (means)\n",
        "            n_batches = max(1, len(My_Loader))\n",
        "            print(\"---------\")\n",
        "            print(f\"Epoch: {self.PreviousEpoch + e + 1}/{self.cfg.epochs}\")\n",
        "            print(f\"Total loss:         {tot/n_batches:.6f}\")\n",
        "            print(f\"Loss (Diffusion):   {l_diff/n_batches:.6f}\")\n",
        "            print(f\"Loss (Reconstruction x{self.cfg.recon_weight:g}): {l_rec/n_batches:.6f}\")\n"
      ],
      "metadata": {
        "id": "etTdb0ce7wiE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0CvUQa9J7EtV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fOykvVIF9QtL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lRzpoWCk9QwD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# Reproducibility\n",
        "# ======================\n",
        "seed_all()\n",
        "\n",
        "# ======================\n",
        "# Data / batching\n",
        "# ======================\n",
        "BATCH_SIZE   = 128      # per-step batch size                                   ###!!! 256 to 128\n",
        "EPOCHS       = 3000     # total training epochs                                 ###!!! 200 to 3000\n",
        "SEQ_LEN      = 60       # real sequence length in data\n",
        "TARGET_LEN   = 100      # model time length after padding\n",
        "\n",
        "# ======================\n",
        "# Diffusion (DDPM)\n",
        "# ======================\n",
        "T_STEPS      = 500      # number of diffusion steps (T)\n",
        "BETA_MIN     = 1e-4     # beta schedule start\n",
        "BETA_MAX     = 0.01     # beta schedule end\n",
        "\n",
        "# ======================\n",
        "# Model (UNet-1D)\n",
        "# ======================\n",
        "HIDDEN       = 256      # base channel width                                    ###!!! 128 to 256\n",
        "FEAT_DIM     = 22       # number of features (channels)\n",
        "TIME_EMB_DIM = 128      # time-embedding dimension\n",
        "UNET_LEVELS  = 3        # down/up depth\n",
        "\n",
        "# ======================\n",
        "# Optimisation\n",
        "# ======================\n",
        "LR           = 0.001     # Adam learning rate                                   ###!!! 1e-3 to 0.001\n",
        "RECON_WEIGHT = 20.0     # λ for reconstruction term\n",
        "GRAD_CLIP    = 1.0      # max grad-norm (0/None to disable)\n",
        "USE_AMP      = True     # mixed precision on CUDA\n",
        "\n",
        "# ======================\n",
        "# Resume / Pretraining\n",
        "# ======================\n",
        "RESUME       = False\n",
        "CKPT_NAME    = \"U_SD\"   # filename under ./Z002_Parameters/Epoch_XXXX/\n",
        "RESUME_EPOCH = 0\n",
        "\n",
        "# ======================\n",
        "# Back-compat (if other code expects Hyper00X_* names)\n",
        "# ======================\n",
        "Hyper005_BatchSize = BATCH_SIZE\n",
        "Hyper006_NEpochs   = EPOCHS\n",
        "Hyper002_NSteps    = T_STEPS\n",
        "Hyper003_MinBeta   = BETA_MIN\n",
        "Hyper004_MaxBeta   = BETA_MAX\n",
        "Hyper001_HD        = HIDDEN\n",
        "Hyper007_LR        = LR\n",
        "Hyper008_Continue  = RESUME\n",
        "Hyper009_PreParams = CKPT_NAME\n",
        "Hyper009_PreEpoch  = RESUME_EPOCH\n",
        "\n",
        "# Helper for existing trainer signature\n",
        "continue_info = [Hyper008_Continue, Hyper009_PreParams, Hyper009_PreEpoch]\n"
      ],
      "metadata": {
        "id": "iLgJ7iUH9Qy_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wLhcRFJf7EC0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tiHTvVAb7EFM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h2Gqw6vX-bLa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_all()\n",
        "\n",
        "# =========================================\n",
        "# 1) Fit real-value transforms (Box–Cox)\n",
        "# =========================================\n",
        "REAL_COLS = [\"VL\", \"CD4\"]\n",
        "bc_params = compute_boxcox_params(Sub_Data, columns=REAL_COLS)\n",
        "\n",
        "# =========================================\n",
        "# 2) Data → tensors/loaders (schema-driven)\n",
        "# =========================================\n",
        "data_np, train_loader, flat_train = ExecuteB002(\n",
        "    My_df=Sub_Data,                  # expects BinCat2Num + snake_case already applied\n",
        "    dtype=dtype,                     # feature schema (contiguous spans)\n",
        "    Hyper005_BatchSize=BATCH_SIZE,   # from globals\n",
        "    seq_len=SEQ_LEN,                 # from globals\n",
        "    feature_names=FEATURE_NAMES,     # flat column order matching dtype\n",
        "    transform_params=bc_params,      # unified Box–Cox -> [0,1]\n",
        "    use_legacy_scaling=False,\n",
        ")\n",
        "\n",
        "# Quick sanity checks\n",
        "assert isinstance(flat_train, torch.Tensor) and flat_train.ndim == 2, \\\n",
        "    f\"flat_train must be [N*L, D] torch.Tensor, got {type(flat_train)} with shape {getattr(flat_train,'shape',None)}\"\n",
        "assert flat_train.size(1) == len(FEATURE_NAMES), \\\n",
        "    f\"Feature dim mismatch: {flat_train.size(1)} vs {len(FEATURE_NAMES)} (schema)\"\n",
        "assert len(train_loader) > 0, \"Empty DataLoader — check SEQ_LEN, filtering, or BATCH_SIZE.\"\n",
        "\n",
        "# Basic hyper checks (using already-defined globals)\n",
        "assert BETA_MIN < BETA_MAX, \"BETA_MIN must be < BETA_MAX.\"\n",
        "assert isinstance(T_STEPS, int) and T_STEPS > 0, \"T_STEPS must be a positive int.\"\n",
        "assert BATCH_SIZE > 0 and EPOCHS > 0, \"Batch size and epochs must be positive.\"\n",
        "\n",
        "# =========================================\n",
        "# 3) Build trainer with the flattened tensor\n",
        "# =========================================\n",
        "My_Model = ExecuteB003(\n",
        "    All_Trainable_Data = flat_train,          # << from ExecuteB002\n",
        "    Hyper001_HD        = Hyper001_HD,         # already mapped from HIDDEN\n",
        "    Hyper002_NSteps    = Hyper002_NSteps,     # already mapped from T_STEPS\n",
        "    Hyper003_MinBeta   = Hyper003_MinBeta,    # already mapped from BETA_MIN\n",
        "    Hyper004_MaxBeta   = Hyper004_MaxBeta,    # already mapped from BETA_MAX\n",
        "    Hyper005_BatchSize = Hyper005_BatchSize,  # already mapped from BATCH_SIZE\n",
        "    Hyper006_NEpochs   = Hyper006_NEpochs,    # already mapped from EPOCHS\n",
        "    Hyper007_LR        = Hyper007_LR,         # already mapped from LR\n",
        "    data_types         = dtype,\n",
        "    continue_info      = continue_info,       # prebuilt: [RESUME, CKPT_NAME, RESUME_EPOCH]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "I-7q9-IK-bSa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PRJzqzW1_zk8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n35vxFAQ_zoV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9-UVqKhX_zpe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "My_Model.train(train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA_5yyzK_zt6",
        "outputId": "470d98f4-e908-447a-bb86-724ef25f166d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "---------\n",
            "Epoch: 2001/3000\n",
            "Total loss:         0.111667\n",
            "Loss (Diffusion):   0.020215\n",
            "Loss (Reconstruction x20): 0.091453\n",
            "---------\n",
            "Epoch: 2002/3000\n",
            "Total loss:         0.111236\n",
            "Loss (Diffusion):   0.022050\n",
            "Loss (Reconstruction x20): 0.089185\n",
            "---------\n",
            "Epoch: 2003/3000\n",
            "Total loss:         0.123111\n",
            "Loss (Diffusion):   0.019814\n",
            "Loss (Reconstruction x20): 0.103297\n",
            "---------\n",
            "Epoch: 2004/3000\n",
            "Total loss:         0.102026\n",
            "Loss (Diffusion):   0.019539\n",
            "Loss (Reconstruction x20): 0.082487\n",
            "---------\n",
            "Epoch: 2005/3000\n",
            "Total loss:         0.106724\n",
            "Loss (Diffusion):   0.026858\n",
            "Loss (Reconstruction x20): 0.079866\n",
            "---------\n",
            "Epoch: 2006/3000\n",
            "Total loss:         0.127946\n",
            "Loss (Diffusion):   0.021802\n",
            "Loss (Reconstruction x20): 0.106144\n",
            "---------\n",
            "Epoch: 2007/3000\n",
            "Total loss:         0.093890\n",
            "Loss (Diffusion):   0.015973\n",
            "Loss (Reconstruction x20): 0.077917\n",
            "---------\n",
            "Epoch: 2008/3000\n",
            "Total loss:         0.127033\n",
            "Loss (Diffusion):   0.018830\n",
            "Loss (Reconstruction x20): 0.108203\n",
            "---------\n",
            "Epoch: 2009/3000\n",
            "Total loss:         0.102337\n",
            "Loss (Diffusion):   0.019155\n",
            "Loss (Reconstruction x20): 0.083182\n",
            "---------\n",
            "Epoch: 2010/3000\n",
            "Total loss:         0.119773\n",
            "Loss (Diffusion):   0.023400\n",
            "Loss (Reconstruction x20): 0.096374\n",
            "---------\n",
            "Epoch: 2011/3000\n",
            "Total loss:         0.125495\n",
            "Loss (Diffusion):   0.023325\n",
            "Loss (Reconstruction x20): 0.102171\n",
            "---------\n",
            "Epoch: 2012/3000\n",
            "Total loss:         0.115263\n",
            "Loss (Diffusion):   0.027030\n",
            "Loss (Reconstruction x20): 0.088233\n",
            "---------\n",
            "Epoch: 2013/3000\n",
            "Total loss:         0.119960\n",
            "Loss (Diffusion):   0.023149\n",
            "Loss (Reconstruction x20): 0.096811\n",
            "---------\n",
            "Epoch: 2014/3000\n",
            "Total loss:         0.107186\n",
            "Loss (Diffusion):   0.017873\n",
            "Loss (Reconstruction x20): 0.089313\n",
            "---------\n",
            "Epoch: 2015/3000\n",
            "Total loss:         0.114379\n",
            "Loss (Diffusion):   0.019256\n",
            "Loss (Reconstruction x20): 0.095123\n",
            "---------\n",
            "Epoch: 2016/3000\n",
            "Total loss:         0.119962\n",
            "Loss (Diffusion):   0.027101\n",
            "Loss (Reconstruction x20): 0.092861\n",
            "---------\n",
            "Epoch: 2017/3000\n",
            "Total loss:         0.119741\n",
            "Loss (Diffusion):   0.025083\n",
            "Loss (Reconstruction x20): 0.094658\n",
            "---------\n",
            "Epoch: 2018/3000\n",
            "Total loss:         0.100968\n",
            "Loss (Diffusion):   0.023112\n",
            "Loss (Reconstruction x20): 0.077856\n",
            "---------\n",
            "Epoch: 2019/3000\n",
            "Total loss:         0.101275\n",
            "Loss (Diffusion):   0.024674\n",
            "Loss (Reconstruction x20): 0.076601\n",
            "---------\n",
            "Epoch: 2020/3000\n",
            "Total loss:         0.115619\n",
            "Loss (Diffusion):   0.020294\n",
            "Loss (Reconstruction x20): 0.095325\n",
            "---------\n",
            "Epoch: 2021/3000\n",
            "Total loss:         0.124045\n",
            "Loss (Diffusion):   0.023372\n",
            "Loss (Reconstruction x20): 0.100673\n",
            "---------\n",
            "Epoch: 2022/3000\n",
            "Total loss:         0.106348\n",
            "Loss (Diffusion):   0.022330\n",
            "Loss (Reconstruction x20): 0.084018\n",
            "---------\n",
            "Epoch: 2023/3000\n",
            "Total loss:         0.094356\n",
            "Loss (Diffusion):   0.019319\n",
            "Loss (Reconstruction x20): 0.075037\n",
            "---------\n",
            "Epoch: 2024/3000\n",
            "Total loss:         0.094268\n",
            "Loss (Diffusion):   0.018530\n",
            "Loss (Reconstruction x20): 0.075738\n",
            "---------\n",
            "Epoch: 2025/3000\n",
            "Total loss:         0.113434\n",
            "Loss (Diffusion):   0.019100\n",
            "Loss (Reconstruction x20): 0.094333\n",
            "---------\n",
            "Epoch: 2026/3000\n",
            "Total loss:         0.100573\n",
            "Loss (Diffusion):   0.017214\n",
            "Loss (Reconstruction x20): 0.083358\n",
            "---------\n",
            "Epoch: 2027/3000\n",
            "Total loss:         0.104725\n",
            "Loss (Diffusion):   0.016506\n",
            "Loss (Reconstruction x20): 0.088219\n",
            "---------\n",
            "Epoch: 2028/3000\n",
            "Total loss:         0.104042\n",
            "Loss (Diffusion):   0.021458\n",
            "Loss (Reconstruction x20): 0.082583\n",
            "---------\n",
            "Epoch: 2029/3000\n",
            "Total loss:         0.094139\n",
            "Loss (Diffusion):   0.019015\n",
            "Loss (Reconstruction x20): 0.075124\n",
            "---------\n",
            "Epoch: 2030/3000\n",
            "Total loss:         0.109615\n",
            "Loss (Diffusion):   0.018016\n",
            "Loss (Reconstruction x20): 0.091599\n",
            "---------\n",
            "Epoch: 2031/3000\n",
            "Total loss:         0.096882\n",
            "Loss (Diffusion):   0.025014\n",
            "Loss (Reconstruction x20): 0.071868\n",
            "---------\n",
            "Epoch: 2032/3000\n",
            "Total loss:         0.103769\n",
            "Loss (Diffusion):   0.021252\n",
            "Loss (Reconstruction x20): 0.082517\n",
            "---------\n",
            "Epoch: 2033/3000\n",
            "Total loss:         0.094065\n",
            "Loss (Diffusion):   0.019451\n",
            "Loss (Reconstruction x20): 0.074614\n",
            "---------\n",
            "Epoch: 2034/3000\n",
            "Total loss:         0.103202\n",
            "Loss (Diffusion):   0.017977\n",
            "Loss (Reconstruction x20): 0.085225\n",
            "---------\n",
            "Epoch: 2035/3000\n",
            "Total loss:         0.096776\n",
            "Loss (Diffusion):   0.023436\n",
            "Loss (Reconstruction x20): 0.073340\n",
            "---------\n",
            "Epoch: 2036/3000\n",
            "Total loss:         0.095375\n",
            "Loss (Diffusion):   0.019780\n",
            "Loss (Reconstruction x20): 0.075595\n",
            "---------\n",
            "Epoch: 2037/3000\n",
            "Total loss:         0.108177\n",
            "Loss (Diffusion):   0.018100\n",
            "Loss (Reconstruction x20): 0.090076\n",
            "---------\n",
            "Epoch: 2038/3000\n",
            "Total loss:         0.093390\n",
            "Loss (Diffusion):   0.020853\n",
            "Loss (Reconstruction x20): 0.072537\n",
            "---------\n",
            "Epoch: 2039/3000\n",
            "Total loss:         0.100255\n",
            "Loss (Diffusion):   0.022858\n",
            "Loss (Reconstruction x20): 0.077397\n",
            "---------\n",
            "Epoch: 2040/3000\n",
            "Total loss:         0.091218\n",
            "Loss (Diffusion):   0.018575\n",
            "Loss (Reconstruction x20): 0.072643\n",
            "---------\n",
            "Epoch: 2041/3000\n",
            "Total loss:         0.101176\n",
            "Loss (Diffusion):   0.019961\n",
            "Loss (Reconstruction x20): 0.081215\n",
            "---------\n",
            "Epoch: 2042/3000\n",
            "Total loss:         0.083081\n",
            "Loss (Diffusion):   0.019389\n",
            "Loss (Reconstruction x20): 0.063692\n",
            "---------\n",
            "Epoch: 2043/3000\n",
            "Total loss:         0.098294\n",
            "Loss (Diffusion):   0.020370\n",
            "Loss (Reconstruction x20): 0.077923\n",
            "---------\n",
            "Epoch: 2044/3000\n",
            "Total loss:         0.086009\n",
            "Loss (Diffusion):   0.020453\n",
            "Loss (Reconstruction x20): 0.065557\n",
            "---------\n",
            "Epoch: 2045/3000\n",
            "Total loss:         0.091321\n",
            "Loss (Diffusion):   0.020171\n",
            "Loss (Reconstruction x20): 0.071150\n",
            "---------\n",
            "Epoch: 2046/3000\n",
            "Total loss:         0.091457\n",
            "Loss (Diffusion):   0.014387\n",
            "Loss (Reconstruction x20): 0.077070\n",
            "---------\n",
            "Epoch: 2047/3000\n",
            "Total loss:         0.087736\n",
            "Loss (Diffusion):   0.022713\n",
            "Loss (Reconstruction x20): 0.065024\n",
            "---------\n",
            "Epoch: 2048/3000\n",
            "Total loss:         0.094980\n",
            "Loss (Diffusion):   0.017898\n",
            "Loss (Reconstruction x20): 0.077082\n",
            "---------\n",
            "Epoch: 2049/3000\n",
            "Total loss:         0.086355\n",
            "Loss (Diffusion):   0.017302\n",
            "Loss (Reconstruction x20): 0.069053\n",
            "---------\n",
            "Epoch: 2050/3000\n",
            "Total loss:         0.097097\n",
            "Loss (Diffusion):   0.018902\n",
            "Loss (Reconstruction x20): 0.078195\n",
            "---------\n",
            "Epoch: 2051/3000\n",
            "Total loss:         0.098169\n",
            "Loss (Diffusion):   0.017986\n",
            "Loss (Reconstruction x20): 0.080183\n",
            "---------\n",
            "Epoch: 2052/3000\n",
            "Total loss:         0.089195\n",
            "Loss (Diffusion):   0.017921\n",
            "Loss (Reconstruction x20): 0.071274\n",
            "---------\n",
            "Epoch: 2053/3000\n",
            "Total loss:         0.094960\n",
            "Loss (Diffusion):   0.020845\n",
            "Loss (Reconstruction x20): 0.074115\n",
            "---------\n",
            "Epoch: 2054/3000\n",
            "Total loss:         0.104201\n",
            "Loss (Diffusion):   0.024221\n",
            "Loss (Reconstruction x20): 0.079979\n",
            "---------\n",
            "Epoch: 2055/3000\n",
            "Total loss:         0.104137\n",
            "Loss (Diffusion):   0.021344\n",
            "Loss (Reconstruction x20): 0.082793\n",
            "---------\n",
            "Epoch: 2056/3000\n",
            "Total loss:         0.103694\n",
            "Loss (Diffusion):   0.019185\n",
            "Loss (Reconstruction x20): 0.084509\n",
            "---------\n",
            "Epoch: 2057/3000\n",
            "Total loss:         0.090377\n",
            "Loss (Diffusion):   0.020619\n",
            "Loss (Reconstruction x20): 0.069758\n",
            "---------\n",
            "Epoch: 2058/3000\n",
            "Total loss:         0.098458\n",
            "Loss (Diffusion):   0.016256\n",
            "Loss (Reconstruction x20): 0.082201\n",
            "---------\n",
            "Epoch: 2059/3000\n",
            "Total loss:         0.108789\n",
            "Loss (Diffusion):   0.020293\n",
            "Loss (Reconstruction x20): 0.088497\n",
            "---------\n",
            "Epoch: 2060/3000\n",
            "Total loss:         0.096336\n",
            "Loss (Diffusion):   0.020827\n",
            "Loss (Reconstruction x20): 0.075509\n",
            "---------\n",
            "Epoch: 2061/3000\n",
            "Total loss:         0.156021\n",
            "Loss (Diffusion):   0.022284\n",
            "Loss (Reconstruction x20): 0.133737\n",
            "---------\n",
            "Epoch: 2062/3000\n",
            "Total loss:         0.109897\n",
            "Loss (Diffusion):   0.024560\n",
            "Loss (Reconstruction x20): 0.085337\n",
            "---------\n",
            "Epoch: 2063/3000\n",
            "Total loss:         0.118044\n",
            "Loss (Diffusion):   0.029586\n",
            "Loss (Reconstruction x20): 0.088459\n",
            "---------\n",
            "Epoch: 2064/3000\n",
            "Total loss:         0.107006\n",
            "Loss (Diffusion):   0.019568\n",
            "Loss (Reconstruction x20): 0.087438\n",
            "---------\n",
            "Epoch: 2065/3000\n",
            "Total loss:         0.111847\n",
            "Loss (Diffusion):   0.018011\n",
            "Loss (Reconstruction x20): 0.093836\n",
            "---------\n",
            "Epoch: 2066/3000\n",
            "Total loss:         0.111026\n",
            "Loss (Diffusion):   0.031884\n",
            "Loss (Reconstruction x20): 0.079142\n",
            "---------\n",
            "Epoch: 2067/3000\n",
            "Total loss:         0.101520\n",
            "Loss (Diffusion):   0.025590\n",
            "Loss (Reconstruction x20): 0.075930\n",
            "---------\n",
            "Epoch: 2068/3000\n",
            "Total loss:         0.097330\n",
            "Loss (Diffusion):   0.020975\n",
            "Loss (Reconstruction x20): 0.076355\n",
            "---------\n",
            "Epoch: 2069/3000\n",
            "Total loss:         0.091639\n",
            "Loss (Diffusion):   0.019782\n",
            "Loss (Reconstruction x20): 0.071857\n",
            "---------\n",
            "Epoch: 2070/3000\n",
            "Total loss:         0.099663\n",
            "Loss (Diffusion):   0.019202\n",
            "Loss (Reconstruction x20): 0.080461\n",
            "---------\n",
            "Epoch: 2071/3000\n",
            "Total loss:         0.109592\n",
            "Loss (Diffusion):   0.026363\n",
            "Loss (Reconstruction x20): 0.083229\n",
            "---------\n",
            "Epoch: 2072/3000\n",
            "Total loss:         0.100966\n",
            "Loss (Diffusion):   0.020539\n",
            "Loss (Reconstruction x20): 0.080427\n",
            "---------\n",
            "Epoch: 2073/3000\n",
            "Total loss:         0.105794\n",
            "Loss (Diffusion):   0.020410\n",
            "Loss (Reconstruction x20): 0.085385\n",
            "---------\n",
            "Epoch: 2074/3000\n",
            "Total loss:         0.101433\n",
            "Loss (Diffusion):   0.015930\n",
            "Loss (Reconstruction x20): 0.085503\n",
            "---------\n",
            "Epoch: 2075/3000\n",
            "Total loss:         0.109731\n",
            "Loss (Diffusion):   0.016825\n",
            "Loss (Reconstruction x20): 0.092906\n",
            "---------\n",
            "Epoch: 2076/3000\n",
            "Total loss:         0.086173\n",
            "Loss (Diffusion):   0.016217\n",
            "Loss (Reconstruction x20): 0.069956\n",
            "---------\n",
            "Epoch: 2077/3000\n",
            "Total loss:         0.105006\n",
            "Loss (Diffusion):   0.018838\n",
            "Loss (Reconstruction x20): 0.086168\n",
            "---------\n",
            "Epoch: 2078/3000\n",
            "Total loss:         0.104508\n",
            "Loss (Diffusion):   0.016951\n",
            "Loss (Reconstruction x20): 0.087557\n",
            "---------\n",
            "Epoch: 2079/3000\n",
            "Total loss:         0.088186\n",
            "Loss (Diffusion):   0.016984\n",
            "Loss (Reconstruction x20): 0.071202\n",
            "---------\n",
            "Epoch: 2080/3000\n",
            "Total loss:         0.102779\n",
            "Loss (Diffusion):   0.015182\n",
            "Loss (Reconstruction x20): 0.087596\n",
            "---------\n",
            "Epoch: 2081/3000\n",
            "Total loss:         0.108993\n",
            "Loss (Diffusion):   0.018911\n",
            "Loss (Reconstruction x20): 0.090083\n",
            "---------\n",
            "Epoch: 2082/3000\n",
            "Total loss:         0.112090\n",
            "Loss (Diffusion):   0.016367\n",
            "Loss (Reconstruction x20): 0.095724\n",
            "---------\n",
            "Epoch: 2083/3000\n",
            "Total loss:         0.112859\n",
            "Loss (Diffusion):   0.020990\n",
            "Loss (Reconstruction x20): 0.091869\n",
            "---------\n",
            "Epoch: 2084/3000\n",
            "Total loss:         0.095558\n",
            "Loss (Diffusion):   0.018051\n",
            "Loss (Reconstruction x20): 0.077507\n",
            "---------\n",
            "Epoch: 2085/3000\n",
            "Total loss:         0.107040\n",
            "Loss (Diffusion):   0.022479\n",
            "Loss (Reconstruction x20): 0.084561\n",
            "---------\n",
            "Epoch: 2086/3000\n",
            "Total loss:         0.093712\n",
            "Loss (Diffusion):   0.019283\n",
            "Loss (Reconstruction x20): 0.074429\n",
            "---------\n",
            "Epoch: 2087/3000\n",
            "Total loss:         0.113968\n",
            "Loss (Diffusion):   0.018772\n",
            "Loss (Reconstruction x20): 0.095196\n",
            "---------\n",
            "Epoch: 2088/3000\n",
            "Total loss:         0.084140\n",
            "Loss (Diffusion):   0.019800\n",
            "Loss (Reconstruction x20): 0.064341\n",
            "---------\n",
            "Epoch: 2089/3000\n",
            "Total loss:         0.100761\n",
            "Loss (Diffusion):   0.019347\n",
            "Loss (Reconstruction x20): 0.081413\n",
            "---------\n",
            "Epoch: 2090/3000\n",
            "Total loss:         0.104502\n",
            "Loss (Diffusion):   0.018283\n",
            "Loss (Reconstruction x20): 0.086219\n",
            "---------\n",
            "Epoch: 2091/3000\n",
            "Total loss:         0.075741\n",
            "Loss (Diffusion):   0.024005\n",
            "Loss (Reconstruction x20): 0.051736\n",
            "---------\n",
            "Epoch: 2092/3000\n",
            "Total loss:         0.085282\n",
            "Loss (Diffusion):   0.021998\n",
            "Loss (Reconstruction x20): 0.063283\n",
            "---------\n",
            "Epoch: 2093/3000\n",
            "Total loss:         0.095134\n",
            "Loss (Diffusion):   0.018185\n",
            "Loss (Reconstruction x20): 0.076948\n",
            "---------\n",
            "Epoch: 2094/3000\n",
            "Total loss:         0.086892\n",
            "Loss (Diffusion):   0.018959\n",
            "Loss (Reconstruction x20): 0.067933\n",
            "---------\n",
            "Epoch: 2095/3000\n",
            "Total loss:         0.096019\n",
            "Loss (Diffusion):   0.022719\n",
            "Loss (Reconstruction x20): 0.073300\n",
            "---------\n",
            "Epoch: 2096/3000\n",
            "Total loss:         0.089393\n",
            "Loss (Diffusion):   0.020437\n",
            "Loss (Reconstruction x20): 0.068956\n",
            "---------\n",
            "Epoch: 2097/3000\n",
            "Total loss:         0.084138\n",
            "Loss (Diffusion):   0.018100\n",
            "Loss (Reconstruction x20): 0.066038\n",
            "---------\n",
            "Epoch: 2098/3000\n",
            "Total loss:         0.111856\n",
            "Loss (Diffusion):   0.019674\n",
            "Loss (Reconstruction x20): 0.092182\n",
            "---------\n",
            "Epoch: 2099/3000\n",
            "Total loss:         0.102863\n",
            "Loss (Diffusion):   0.017601\n",
            "Loss (Reconstruction x20): 0.085262\n",
            "---------\n",
            "Epoch: 2100/3000\n",
            "Total loss:         0.097215\n",
            "Loss (Diffusion):   0.019134\n",
            "Loss (Reconstruction x20): 0.078082\n",
            "---------\n",
            "Epoch: 2101/3000\n",
            "Total loss:         0.091815\n",
            "Loss (Diffusion):   0.020197\n",
            "Loss (Reconstruction x20): 0.071618\n",
            "---------\n",
            "Epoch: 2102/3000\n",
            "Total loss:         0.094025\n",
            "Loss (Diffusion):   0.023047\n",
            "Loss (Reconstruction x20): 0.070979\n",
            "---------\n",
            "Epoch: 2103/3000\n",
            "Total loss:         0.085156\n",
            "Loss (Diffusion):   0.024002\n",
            "Loss (Reconstruction x20): 0.061154\n",
            "---------\n",
            "Epoch: 2104/3000\n",
            "Total loss:         0.093731\n",
            "Loss (Diffusion):   0.024376\n",
            "Loss (Reconstruction x20): 0.069355\n",
            "---------\n",
            "Epoch: 2105/3000\n",
            "Total loss:         0.089388\n",
            "Loss (Diffusion):   0.022882\n",
            "Loss (Reconstruction x20): 0.066506\n",
            "---------\n",
            "Epoch: 2106/3000\n",
            "Total loss:         0.087640\n",
            "Loss (Diffusion):   0.018653\n",
            "Loss (Reconstruction x20): 0.068987\n",
            "---------\n",
            "Epoch: 2107/3000\n",
            "Total loss:         0.105186\n",
            "Loss (Diffusion):   0.018601\n",
            "Loss (Reconstruction x20): 0.086585\n",
            "---------\n",
            "Epoch: 2108/3000\n",
            "Total loss:         0.091874\n",
            "Loss (Diffusion):   0.022236\n",
            "Loss (Reconstruction x20): 0.069637\n",
            "---------\n",
            "Epoch: 2109/3000\n",
            "Total loss:         0.098428\n",
            "Loss (Diffusion):   0.022264\n",
            "Loss (Reconstruction x20): 0.076164\n",
            "---------\n",
            "Epoch: 2110/3000\n",
            "Total loss:         0.098454\n",
            "Loss (Diffusion):   0.019287\n",
            "Loss (Reconstruction x20): 0.079166\n",
            "---------\n",
            "Epoch: 2111/3000\n",
            "Total loss:         0.100874\n",
            "Loss (Diffusion):   0.023394\n",
            "Loss (Reconstruction x20): 0.077480\n",
            "---------\n",
            "Epoch: 2112/3000\n",
            "Total loss:         0.099129\n",
            "Loss (Diffusion):   0.021757\n",
            "Loss (Reconstruction x20): 0.077372\n",
            "---------\n",
            "Epoch: 2113/3000\n",
            "Total loss:         0.098903\n",
            "Loss (Diffusion):   0.019786\n",
            "Loss (Reconstruction x20): 0.079117\n",
            "---------\n",
            "Epoch: 2114/3000\n",
            "Total loss:         0.096117\n",
            "Loss (Diffusion):   0.023862\n",
            "Loss (Reconstruction x20): 0.072255\n",
            "---------\n",
            "Epoch: 2115/3000\n",
            "Total loss:         0.106842\n",
            "Loss (Diffusion):   0.021039\n",
            "Loss (Reconstruction x20): 0.085803\n",
            "---------\n",
            "Epoch: 2116/3000\n",
            "Total loss:         0.084914\n",
            "Loss (Diffusion):   0.019188\n",
            "Loss (Reconstruction x20): 0.065726\n",
            "---------\n",
            "Epoch: 2117/3000\n",
            "Total loss:         0.099790\n",
            "Loss (Diffusion):   0.022513\n",
            "Loss (Reconstruction x20): 0.077277\n",
            "---------\n",
            "Epoch: 2118/3000\n",
            "Total loss:         0.097427\n",
            "Loss (Diffusion):   0.016369\n",
            "Loss (Reconstruction x20): 0.081058\n",
            "---------\n",
            "Epoch: 2119/3000\n",
            "Total loss:         0.097244\n",
            "Loss (Diffusion):   0.020198\n",
            "Loss (Reconstruction x20): 0.077047\n",
            "---------\n",
            "Epoch: 2120/3000\n",
            "Total loss:         0.095382\n",
            "Loss (Diffusion):   0.014456\n",
            "Loss (Reconstruction x20): 0.080926\n",
            "---------\n",
            "Epoch: 2121/3000\n",
            "Total loss:         0.089394\n",
            "Loss (Diffusion):   0.021842\n",
            "Loss (Reconstruction x20): 0.067552\n",
            "---------\n",
            "Epoch: 2122/3000\n",
            "Total loss:         0.099678\n",
            "Loss (Diffusion):   0.015736\n",
            "Loss (Reconstruction x20): 0.083942\n",
            "---------\n",
            "Epoch: 2123/3000\n",
            "Total loss:         0.079633\n",
            "Loss (Diffusion):   0.016252\n",
            "Loss (Reconstruction x20): 0.063380\n",
            "---------\n",
            "Epoch: 2124/3000\n",
            "Total loss:         0.085788\n",
            "Loss (Diffusion):   0.021843\n",
            "Loss (Reconstruction x20): 0.063946\n",
            "---------\n",
            "Epoch: 2125/3000\n",
            "Total loss:         0.084836\n",
            "Loss (Diffusion):   0.020018\n",
            "Loss (Reconstruction x20): 0.064819\n",
            "---------\n",
            "Epoch: 2126/3000\n",
            "Total loss:         0.089336\n",
            "Loss (Diffusion):   0.019949\n",
            "Loss (Reconstruction x20): 0.069387\n",
            "---------\n",
            "Epoch: 2127/3000\n",
            "Total loss:         0.075632\n",
            "Loss (Diffusion):   0.015311\n",
            "Loss (Reconstruction x20): 0.060321\n",
            "---------\n",
            "Epoch: 2128/3000\n",
            "Total loss:         0.089398\n",
            "Loss (Diffusion):   0.017498\n",
            "Loss (Reconstruction x20): 0.071900\n",
            "---------\n",
            "Epoch: 2129/3000\n",
            "Total loss:         0.069394\n",
            "Loss (Diffusion):   0.017888\n",
            "Loss (Reconstruction x20): 0.051506\n",
            "---------\n",
            "Epoch: 2130/3000\n",
            "Total loss:         0.085657\n",
            "Loss (Diffusion):   0.016500\n",
            "Loss (Reconstruction x20): 0.069157\n",
            "---------\n",
            "Epoch: 2131/3000\n",
            "Total loss:         0.085573\n",
            "Loss (Diffusion):   0.015254\n",
            "Loss (Reconstruction x20): 0.070318\n",
            "---------\n",
            "Epoch: 2132/3000\n",
            "Total loss:         0.100837\n",
            "Loss (Diffusion):   0.020069\n",
            "Loss (Reconstruction x20): 0.080768\n",
            "---------\n",
            "Epoch: 2133/3000\n",
            "Total loss:         0.104585\n",
            "Loss (Diffusion):   0.020344\n",
            "Loss (Reconstruction x20): 0.084241\n",
            "---------\n",
            "Epoch: 2134/3000\n",
            "Total loss:         0.114389\n",
            "Loss (Diffusion):   0.018046\n",
            "Loss (Reconstruction x20): 0.096343\n",
            "---------\n",
            "Epoch: 2135/3000\n",
            "Total loss:         0.101988\n",
            "Loss (Diffusion):   0.016587\n",
            "Loss (Reconstruction x20): 0.085401\n",
            "---------\n",
            "Epoch: 2136/3000\n",
            "Total loss:         0.090332\n",
            "Loss (Diffusion):   0.019155\n",
            "Loss (Reconstruction x20): 0.071178\n",
            "---------\n",
            "Epoch: 2137/3000\n",
            "Total loss:         0.092448\n",
            "Loss (Diffusion):   0.018121\n",
            "Loss (Reconstruction x20): 0.074327\n",
            "---------\n",
            "Epoch: 2138/3000\n",
            "Total loss:         0.090889\n",
            "Loss (Diffusion):   0.019899\n",
            "Loss (Reconstruction x20): 0.070989\n",
            "---------\n",
            "Epoch: 2139/3000\n",
            "Total loss:         0.093695\n",
            "Loss (Diffusion):   0.017081\n",
            "Loss (Reconstruction x20): 0.076614\n",
            "---------\n",
            "Epoch: 2140/3000\n",
            "Total loss:         0.081155\n",
            "Loss (Diffusion):   0.018106\n",
            "Loss (Reconstruction x20): 0.063049\n",
            "---------\n",
            "Epoch: 2141/3000\n",
            "Total loss:         0.088368\n",
            "Loss (Diffusion):   0.019822\n",
            "Loss (Reconstruction x20): 0.068547\n",
            "---------\n",
            "Epoch: 2142/3000\n",
            "Total loss:         0.088634\n",
            "Loss (Diffusion):   0.016064\n",
            "Loss (Reconstruction x20): 0.072570\n",
            "---------\n",
            "Epoch: 2143/3000\n",
            "Total loss:         0.096203\n",
            "Loss (Diffusion):   0.022516\n",
            "Loss (Reconstruction x20): 0.073688\n",
            "---------\n",
            "Epoch: 2144/3000\n",
            "Total loss:         0.084858\n",
            "Loss (Diffusion):   0.019216\n",
            "Loss (Reconstruction x20): 0.065642\n",
            "---------\n",
            "Epoch: 2145/3000\n",
            "Total loss:         0.094772\n",
            "Loss (Diffusion):   0.024716\n",
            "Loss (Reconstruction x20): 0.070055\n",
            "---------\n",
            "Epoch: 2146/3000\n",
            "Total loss:         0.083476\n",
            "Loss (Diffusion):   0.019908\n",
            "Loss (Reconstruction x20): 0.063568\n",
            "---------\n",
            "Epoch: 2147/3000\n",
            "Total loss:         0.087421\n",
            "Loss (Diffusion):   0.019547\n",
            "Loss (Reconstruction x20): 0.067874\n",
            "---------\n",
            "Epoch: 2148/3000\n",
            "Total loss:         0.089339\n",
            "Loss (Diffusion):   0.024052\n",
            "Loss (Reconstruction x20): 0.065288\n",
            "---------\n",
            "Epoch: 2149/3000\n",
            "Total loss:         0.079428\n",
            "Loss (Diffusion):   0.016586\n",
            "Loss (Reconstruction x20): 0.062843\n",
            "---------\n",
            "Epoch: 2150/3000\n",
            "Total loss:         0.074492\n",
            "Loss (Diffusion):   0.015588\n",
            "Loss (Reconstruction x20): 0.058904\n",
            "---------\n",
            "Epoch: 2151/3000\n",
            "Total loss:         0.087830\n",
            "Loss (Diffusion):   0.016856\n",
            "Loss (Reconstruction x20): 0.070973\n",
            "---------\n",
            "Epoch: 2152/3000\n",
            "Total loss:         0.072322\n",
            "Loss (Diffusion):   0.022187\n",
            "Loss (Reconstruction x20): 0.050136\n",
            "---------\n",
            "Epoch: 2153/3000\n",
            "Total loss:         0.083744\n",
            "Loss (Diffusion):   0.018350\n",
            "Loss (Reconstruction x20): 0.065394\n",
            "---------\n",
            "Epoch: 2154/3000\n",
            "Total loss:         0.080722\n",
            "Loss (Diffusion):   0.015239\n",
            "Loss (Reconstruction x20): 0.065483\n",
            "---------\n",
            "Epoch: 2155/3000\n",
            "Total loss:         0.093660\n",
            "Loss (Diffusion):   0.021224\n",
            "Loss (Reconstruction x20): 0.072436\n",
            "---------\n",
            "Epoch: 2156/3000\n",
            "Total loss:         0.072434\n",
            "Loss (Diffusion):   0.017553\n",
            "Loss (Reconstruction x20): 0.054881\n",
            "---------\n",
            "Epoch: 2157/3000\n",
            "Total loss:         0.102076\n",
            "Loss (Diffusion):   0.015875\n",
            "Loss (Reconstruction x20): 0.086201\n",
            "---------\n",
            "Epoch: 2158/3000\n",
            "Total loss:         0.076400\n",
            "Loss (Diffusion):   0.014052\n",
            "Loss (Reconstruction x20): 0.062348\n",
            "---------\n",
            "Epoch: 2159/3000\n",
            "Total loss:         0.089156\n",
            "Loss (Diffusion):   0.022103\n",
            "Loss (Reconstruction x20): 0.067053\n",
            "---------\n",
            "Epoch: 2160/3000\n",
            "Total loss:         0.083810\n",
            "Loss (Diffusion):   0.015932\n",
            "Loss (Reconstruction x20): 0.067879\n",
            "---------\n",
            "Epoch: 2161/3000\n",
            "Total loss:         0.094388\n",
            "Loss (Diffusion):   0.015108\n",
            "Loss (Reconstruction x20): 0.079280\n",
            "---------\n",
            "Epoch: 2162/3000\n",
            "Total loss:         0.082331\n",
            "Loss (Diffusion):   0.015804\n",
            "Loss (Reconstruction x20): 0.066528\n",
            "---------\n",
            "Epoch: 2163/3000\n",
            "Total loss:         0.082746\n",
            "Loss (Diffusion):   0.015216\n",
            "Loss (Reconstruction x20): 0.067530\n",
            "---------\n",
            "Epoch: 2164/3000\n",
            "Total loss:         0.092133\n",
            "Loss (Diffusion):   0.018273\n",
            "Loss (Reconstruction x20): 0.073859\n",
            "---------\n",
            "Epoch: 2165/3000\n",
            "Total loss:         0.097710\n",
            "Loss (Diffusion):   0.021230\n",
            "Loss (Reconstruction x20): 0.076480\n",
            "---------\n",
            "Epoch: 2166/3000\n",
            "Total loss:         0.090513\n",
            "Loss (Diffusion):   0.015449\n",
            "Loss (Reconstruction x20): 0.075064\n",
            "---------\n",
            "Epoch: 2167/3000\n",
            "Total loss:         0.086866\n",
            "Loss (Diffusion):   0.019119\n",
            "Loss (Reconstruction x20): 0.067747\n",
            "---------\n",
            "Epoch: 2168/3000\n",
            "Total loss:         0.094521\n",
            "Loss (Diffusion):   0.016437\n",
            "Loss (Reconstruction x20): 0.078084\n",
            "---------\n",
            "Epoch: 2169/3000\n",
            "Total loss:         0.101977\n",
            "Loss (Diffusion):   0.028025\n",
            "Loss (Reconstruction x20): 0.073953\n",
            "---------\n",
            "Epoch: 2170/3000\n",
            "Total loss:         0.086404\n",
            "Loss (Diffusion):   0.021491\n",
            "Loss (Reconstruction x20): 0.064914\n",
            "---------\n",
            "Epoch: 2171/3000\n",
            "Total loss:         0.098593\n",
            "Loss (Diffusion):   0.017504\n",
            "Loss (Reconstruction x20): 0.081089\n",
            "---------\n",
            "Epoch: 2172/3000\n",
            "Total loss:         0.108599\n",
            "Loss (Diffusion):   0.019844\n",
            "Loss (Reconstruction x20): 0.088755\n",
            "---------\n",
            "Epoch: 2173/3000\n",
            "Total loss:         0.082191\n",
            "Loss (Diffusion):   0.016905\n",
            "Loss (Reconstruction x20): 0.065286\n",
            "---------\n",
            "Epoch: 2174/3000\n",
            "Total loss:         0.095581\n",
            "Loss (Diffusion):   0.020131\n",
            "Loss (Reconstruction x20): 0.075450\n",
            "---------\n",
            "Epoch: 2175/3000\n",
            "Total loss:         0.105819\n",
            "Loss (Diffusion):   0.021802\n",
            "Loss (Reconstruction x20): 0.084017\n",
            "---------\n",
            "Epoch: 2176/3000\n",
            "Total loss:         0.100647\n",
            "Loss (Diffusion):   0.020615\n",
            "Loss (Reconstruction x20): 0.080031\n",
            "---------\n",
            "Epoch: 2177/3000\n",
            "Total loss:         0.107572\n",
            "Loss (Diffusion):   0.020935\n",
            "Loss (Reconstruction x20): 0.086638\n",
            "---------\n",
            "Epoch: 2178/3000\n",
            "Total loss:         0.093197\n",
            "Loss (Diffusion):   0.015973\n",
            "Loss (Reconstruction x20): 0.077224\n",
            "---------\n",
            "Epoch: 2179/3000\n",
            "Total loss:         0.099063\n",
            "Loss (Diffusion):   0.017450\n",
            "Loss (Reconstruction x20): 0.081613\n",
            "---------\n",
            "Epoch: 2180/3000\n",
            "Total loss:         0.091199\n",
            "Loss (Diffusion):   0.023147\n",
            "Loss (Reconstruction x20): 0.068052\n",
            "---------\n",
            "Epoch: 2181/3000\n",
            "Total loss:         0.106406\n",
            "Loss (Diffusion):   0.023452\n",
            "Loss (Reconstruction x20): 0.082955\n",
            "---------\n",
            "Epoch: 2182/3000\n",
            "Total loss:         0.096632\n",
            "Loss (Diffusion):   0.020450\n",
            "Loss (Reconstruction x20): 0.076182\n",
            "---------\n",
            "Epoch: 2183/3000\n",
            "Total loss:         0.100347\n",
            "Loss (Diffusion):   0.019510\n",
            "Loss (Reconstruction x20): 0.080837\n",
            "---------\n",
            "Epoch: 2184/3000\n",
            "Total loss:         0.106318\n",
            "Loss (Diffusion):   0.024239\n",
            "Loss (Reconstruction x20): 0.082078\n",
            "---------\n",
            "Epoch: 2185/3000\n",
            "Total loss:         0.116589\n",
            "Loss (Diffusion):   0.027338\n",
            "Loss (Reconstruction x20): 0.089251\n",
            "---------\n",
            "Epoch: 2186/3000\n",
            "Total loss:         0.095612\n",
            "Loss (Diffusion):   0.022422\n",
            "Loss (Reconstruction x20): 0.073190\n",
            "---------\n",
            "Epoch: 2187/3000\n",
            "Total loss:         0.110162\n",
            "Loss (Diffusion):   0.026420\n",
            "Loss (Reconstruction x20): 0.083741\n",
            "---------\n",
            "Epoch: 2188/3000\n",
            "Total loss:         0.112300\n",
            "Loss (Diffusion):   0.021998\n",
            "Loss (Reconstruction x20): 0.090302\n",
            "---------\n",
            "Epoch: 2189/3000\n",
            "Total loss:         0.098801\n",
            "Loss (Diffusion):   0.020415\n",
            "Loss (Reconstruction x20): 0.078386\n",
            "---------\n",
            "Epoch: 2190/3000\n",
            "Total loss:         0.096016\n",
            "Loss (Diffusion):   0.014484\n",
            "Loss (Reconstruction x20): 0.081532\n",
            "---------\n",
            "Epoch: 2191/3000\n",
            "Total loss:         0.110666\n",
            "Loss (Diffusion):   0.019428\n",
            "Loss (Reconstruction x20): 0.091238\n",
            "---------\n",
            "Epoch: 2192/3000\n",
            "Total loss:         0.108453\n",
            "Loss (Diffusion):   0.020548\n",
            "Loss (Reconstruction x20): 0.087905\n",
            "---------\n",
            "Epoch: 2193/3000\n",
            "Total loss:         0.106679\n",
            "Loss (Diffusion):   0.019272\n",
            "Loss (Reconstruction x20): 0.087407\n",
            "---------\n",
            "Epoch: 2194/3000\n",
            "Total loss:         0.104977\n",
            "Loss (Diffusion):   0.023648\n",
            "Loss (Reconstruction x20): 0.081330\n",
            "---------\n",
            "Epoch: 2195/3000\n",
            "Total loss:         0.094774\n",
            "Loss (Diffusion):   0.020492\n",
            "Loss (Reconstruction x20): 0.074282\n",
            "---------\n",
            "Epoch: 2196/3000\n",
            "Total loss:         0.097987\n",
            "Loss (Diffusion):   0.021694\n",
            "Loss (Reconstruction x20): 0.076293\n",
            "---------\n",
            "Epoch: 2197/3000\n",
            "Total loss:         0.099711\n",
            "Loss (Diffusion):   0.023049\n",
            "Loss (Reconstruction x20): 0.076662\n",
            "---------\n",
            "Epoch: 2198/3000\n",
            "Total loss:         0.113893\n",
            "Loss (Diffusion):   0.019030\n",
            "Loss (Reconstruction x20): 0.094863\n",
            "---------\n",
            "Epoch: 2199/3000\n",
            "Total loss:         0.100616\n",
            "Loss (Diffusion):   0.017078\n",
            "Loss (Reconstruction x20): 0.083538\n",
            "---------\n",
            "Epoch: 2200/3000\n",
            "Total loss:         0.100073\n",
            "Loss (Diffusion):   0.021340\n",
            "Loss (Reconstruction x20): 0.078732\n",
            "---------\n",
            "Epoch: 2201/3000\n",
            "Total loss:         0.115059\n",
            "Loss (Diffusion):   0.021948\n",
            "Loss (Reconstruction x20): 0.093111\n",
            "---------\n",
            "Epoch: 2202/3000\n",
            "Total loss:         0.117707\n",
            "Loss (Diffusion):   0.020248\n",
            "Loss (Reconstruction x20): 0.097459\n",
            "---------\n",
            "Epoch: 2203/3000\n",
            "Total loss:         0.112313\n",
            "Loss (Diffusion):   0.024805\n",
            "Loss (Reconstruction x20): 0.087509\n",
            "---------\n",
            "Epoch: 2204/3000\n",
            "Total loss:         0.117546\n",
            "Loss (Diffusion):   0.022256\n",
            "Loss (Reconstruction x20): 0.095290\n",
            "---------\n",
            "Epoch: 2205/3000\n",
            "Total loss:         0.110027\n",
            "Loss (Diffusion):   0.018204\n",
            "Loss (Reconstruction x20): 0.091823\n",
            "---------\n",
            "Epoch: 2206/3000\n",
            "Total loss:         0.126900\n",
            "Loss (Diffusion):   0.019781\n",
            "Loss (Reconstruction x20): 0.107119\n",
            "---------\n",
            "Epoch: 2207/3000\n",
            "Total loss:         0.108528\n",
            "Loss (Diffusion):   0.022248\n",
            "Loss (Reconstruction x20): 0.086280\n",
            "---------\n",
            "Epoch: 2208/3000\n",
            "Total loss:         0.107030\n",
            "Loss (Diffusion):   0.022741\n",
            "Loss (Reconstruction x20): 0.084289\n",
            "---------\n",
            "Epoch: 2209/3000\n",
            "Total loss:         0.096599\n",
            "Loss (Diffusion):   0.017593\n",
            "Loss (Reconstruction x20): 0.079006\n",
            "---------\n",
            "Epoch: 2210/3000\n",
            "Total loss:         0.114839\n",
            "Loss (Diffusion):   0.018542\n",
            "Loss (Reconstruction x20): 0.096297\n",
            "---------\n",
            "Epoch: 2211/3000\n",
            "Total loss:         0.099359\n",
            "Loss (Diffusion):   0.019634\n",
            "Loss (Reconstruction x20): 0.079724\n",
            "---------\n",
            "Epoch: 2212/3000\n",
            "Total loss:         0.089887\n",
            "Loss (Diffusion):   0.021878\n",
            "Loss (Reconstruction x20): 0.068009\n",
            "---------\n",
            "Epoch: 2213/3000\n",
            "Total loss:         0.097553\n",
            "Loss (Diffusion):   0.022349\n",
            "Loss (Reconstruction x20): 0.075205\n",
            "---------\n",
            "Epoch: 2214/3000\n",
            "Total loss:         0.092660\n",
            "Loss (Diffusion):   0.019325\n",
            "Loss (Reconstruction x20): 0.073335\n",
            "---------\n",
            "Epoch: 2215/3000\n",
            "Total loss:         0.096132\n",
            "Loss (Diffusion):   0.019988\n",
            "Loss (Reconstruction x20): 0.076143\n",
            "---------\n",
            "Epoch: 2216/3000\n",
            "Total loss:         0.088875\n",
            "Loss (Diffusion):   0.015068\n",
            "Loss (Reconstruction x20): 0.073807\n",
            "---------\n",
            "Epoch: 2217/3000\n",
            "Total loss:         0.099898\n",
            "Loss (Diffusion):   0.018099\n",
            "Loss (Reconstruction x20): 0.081799\n",
            "---------\n",
            "Epoch: 2218/3000\n",
            "Total loss:         0.107371\n",
            "Loss (Diffusion):   0.021167\n",
            "Loss (Reconstruction x20): 0.086204\n",
            "---------\n",
            "Epoch: 2219/3000\n",
            "Total loss:         0.091281\n",
            "Loss (Diffusion):   0.020543\n",
            "Loss (Reconstruction x20): 0.070738\n",
            "---------\n",
            "Epoch: 2220/3000\n",
            "Total loss:         0.102375\n",
            "Loss (Diffusion):   0.018621\n",
            "Loss (Reconstruction x20): 0.083755\n",
            "---------\n",
            "Epoch: 2221/3000\n",
            "Total loss:         0.103281\n",
            "Loss (Diffusion):   0.023020\n",
            "Loss (Reconstruction x20): 0.080261\n",
            "---------\n",
            "Epoch: 2222/3000\n",
            "Total loss:         0.085415\n",
            "Loss (Diffusion):   0.017800\n",
            "Loss (Reconstruction x20): 0.067614\n",
            "---------\n",
            "Epoch: 2223/3000\n",
            "Total loss:         0.094446\n",
            "Loss (Diffusion):   0.018310\n",
            "Loss (Reconstruction x20): 0.076136\n",
            "---------\n",
            "Epoch: 2224/3000\n",
            "Total loss:         0.102207\n",
            "Loss (Diffusion):   0.022787\n",
            "Loss (Reconstruction x20): 0.079420\n",
            "---------\n",
            "Epoch: 2225/3000\n",
            "Total loss:         0.099566\n",
            "Loss (Diffusion):   0.016838\n",
            "Loss (Reconstruction x20): 0.082728\n",
            "---------\n",
            "Epoch: 2226/3000\n",
            "Total loss:         0.101050\n",
            "Loss (Diffusion):   0.022871\n",
            "Loss (Reconstruction x20): 0.078179\n",
            "---------\n",
            "Epoch: 2227/3000\n",
            "Total loss:         0.112467\n",
            "Loss (Diffusion):   0.027410\n",
            "Loss (Reconstruction x20): 0.085058\n",
            "---------\n",
            "Epoch: 2228/3000\n",
            "Total loss:         0.105815\n",
            "Loss (Diffusion):   0.023276\n",
            "Loss (Reconstruction x20): 0.082538\n",
            "---------\n",
            "Epoch: 2229/3000\n",
            "Total loss:         0.104869\n",
            "Loss (Diffusion):   0.022214\n",
            "Loss (Reconstruction x20): 0.082654\n",
            "---------\n",
            "Epoch: 2230/3000\n",
            "Total loss:         0.108809\n",
            "Loss (Diffusion):   0.016041\n",
            "Loss (Reconstruction x20): 0.092768\n",
            "---------\n",
            "Epoch: 2231/3000\n",
            "Total loss:         0.100014\n",
            "Loss (Diffusion):   0.019556\n",
            "Loss (Reconstruction x20): 0.080458\n",
            "---------\n",
            "Epoch: 2232/3000\n",
            "Total loss:         0.114462\n",
            "Loss (Diffusion):   0.020091\n",
            "Loss (Reconstruction x20): 0.094371\n",
            "---------\n",
            "Epoch: 2233/3000\n",
            "Total loss:         0.110761\n",
            "Loss (Diffusion):   0.028173\n",
            "Loss (Reconstruction x20): 0.082588\n",
            "---------\n",
            "Epoch: 2234/3000\n",
            "Total loss:         0.086148\n",
            "Loss (Diffusion):   0.025244\n",
            "Loss (Reconstruction x20): 0.060904\n",
            "---------\n",
            "Epoch: 2235/3000\n",
            "Total loss:         0.096902\n",
            "Loss (Diffusion):   0.020466\n",
            "Loss (Reconstruction x20): 0.076436\n",
            "---------\n",
            "Epoch: 2236/3000\n",
            "Total loss:         0.087957\n",
            "Loss (Diffusion):   0.022186\n",
            "Loss (Reconstruction x20): 0.065771\n",
            "---------\n",
            "Epoch: 2237/3000\n",
            "Total loss:         0.100687\n",
            "Loss (Diffusion):   0.021497\n",
            "Loss (Reconstruction x20): 0.079190\n",
            "---------\n",
            "Epoch: 2238/3000\n",
            "Total loss:         0.092084\n",
            "Loss (Diffusion):   0.015156\n",
            "Loss (Reconstruction x20): 0.076928\n",
            "---------\n",
            "Epoch: 2239/3000\n",
            "Total loss:         0.096490\n",
            "Loss (Diffusion):   0.019292\n",
            "Loss (Reconstruction x20): 0.077198\n",
            "---------\n",
            "Epoch: 2240/3000\n",
            "Total loss:         0.089801\n",
            "Loss (Diffusion):   0.019163\n",
            "Loss (Reconstruction x20): 0.070638\n",
            "---------\n",
            "Epoch: 2241/3000\n",
            "Total loss:         0.081669\n",
            "Loss (Diffusion):   0.015072\n",
            "Loss (Reconstruction x20): 0.066597\n",
            "---------\n",
            "Epoch: 2242/3000\n",
            "Total loss:         0.103558\n",
            "Loss (Diffusion):   0.018924\n",
            "Loss (Reconstruction x20): 0.084634\n",
            "---------\n",
            "Epoch: 2243/3000\n",
            "Total loss:         0.085734\n",
            "Loss (Diffusion):   0.019984\n",
            "Loss (Reconstruction x20): 0.065751\n",
            "---------\n",
            "Epoch: 2244/3000\n",
            "Total loss:         0.088049\n",
            "Loss (Diffusion):   0.020861\n",
            "Loss (Reconstruction x20): 0.067189\n",
            "---------\n",
            "Epoch: 2245/3000\n",
            "Total loss:         0.097979\n",
            "Loss (Diffusion):   0.016290\n",
            "Loss (Reconstruction x20): 0.081689\n",
            "---------\n",
            "Epoch: 2246/3000\n",
            "Total loss:         0.106909\n",
            "Loss (Diffusion):   0.022760\n",
            "Loss (Reconstruction x20): 0.084150\n",
            "---------\n",
            "Epoch: 2247/3000\n",
            "Total loss:         0.103120\n",
            "Loss (Diffusion):   0.020032\n",
            "Loss (Reconstruction x20): 0.083088\n",
            "---------\n",
            "Epoch: 2248/3000\n",
            "Total loss:         0.096455\n",
            "Loss (Diffusion):   0.022111\n",
            "Loss (Reconstruction x20): 0.074343\n",
            "---------\n",
            "Epoch: 2249/3000\n",
            "Total loss:         0.113133\n",
            "Loss (Diffusion):   0.018216\n",
            "Loss (Reconstruction x20): 0.094917\n",
            "---------\n",
            "Epoch: 2250/3000\n",
            "Total loss:         0.099641\n",
            "Loss (Diffusion):   0.019653\n",
            "Loss (Reconstruction x20): 0.079988\n",
            "---------\n",
            "Epoch: 2251/3000\n",
            "Total loss:         0.111361\n",
            "Loss (Diffusion):   0.017991\n",
            "Loss (Reconstruction x20): 0.093370\n",
            "---------\n",
            "Epoch: 2252/3000\n",
            "Total loss:         0.091408\n",
            "Loss (Diffusion):   0.021034\n",
            "Loss (Reconstruction x20): 0.070374\n",
            "---------\n",
            "Epoch: 2253/3000\n",
            "Total loss:         0.114549\n",
            "Loss (Diffusion):   0.020842\n",
            "Loss (Reconstruction x20): 0.093708\n",
            "---------\n",
            "Epoch: 2254/3000\n",
            "Total loss:         0.104784\n",
            "Loss (Diffusion):   0.019918\n",
            "Loss (Reconstruction x20): 0.084866\n",
            "---------\n",
            "Epoch: 2255/3000\n",
            "Total loss:         0.084004\n",
            "Loss (Diffusion):   0.020814\n",
            "Loss (Reconstruction x20): 0.063190\n",
            "---------\n",
            "Epoch: 2256/3000\n",
            "Total loss:         0.095568\n",
            "Loss (Diffusion):   0.025749\n",
            "Loss (Reconstruction x20): 0.069819\n",
            "---------\n",
            "Epoch: 2257/3000\n",
            "Total loss:         0.086867\n",
            "Loss (Diffusion):   0.020652\n",
            "Loss (Reconstruction x20): 0.066215\n",
            "---------\n",
            "Epoch: 2258/3000\n",
            "Total loss:         0.091531\n",
            "Loss (Diffusion):   0.019559\n",
            "Loss (Reconstruction x20): 0.071972\n",
            "---------\n",
            "Epoch: 2259/3000\n",
            "Total loss:         0.096716\n",
            "Loss (Diffusion):   0.019250\n",
            "Loss (Reconstruction x20): 0.077466\n",
            "---------\n",
            "Epoch: 2260/3000\n",
            "Total loss:         0.075547\n",
            "Loss (Diffusion):   0.017277\n",
            "Loss (Reconstruction x20): 0.058270\n",
            "---------\n",
            "Epoch: 2261/3000\n",
            "Total loss:         0.102010\n",
            "Loss (Diffusion):   0.019773\n",
            "Loss (Reconstruction x20): 0.082236\n",
            "---------\n",
            "Epoch: 2262/3000\n",
            "Total loss:         0.090162\n",
            "Loss (Diffusion):   0.022804\n",
            "Loss (Reconstruction x20): 0.067358\n",
            "---------\n",
            "Epoch: 2263/3000\n",
            "Total loss:         0.093757\n",
            "Loss (Diffusion):   0.020613\n",
            "Loss (Reconstruction x20): 0.073143\n",
            "---------\n",
            "Epoch: 2264/3000\n",
            "Total loss:         0.091899\n",
            "Loss (Diffusion):   0.017160\n",
            "Loss (Reconstruction x20): 0.074738\n",
            "---------\n",
            "Epoch: 2265/3000\n",
            "Total loss:         0.100187\n",
            "Loss (Diffusion):   0.019590\n",
            "Loss (Reconstruction x20): 0.080597\n",
            "---------\n",
            "Epoch: 2266/3000\n",
            "Total loss:         0.084781\n",
            "Loss (Diffusion):   0.019322\n",
            "Loss (Reconstruction x20): 0.065459\n",
            "---------\n",
            "Epoch: 2267/3000\n",
            "Total loss:         0.094188\n",
            "Loss (Diffusion):   0.017910\n",
            "Loss (Reconstruction x20): 0.076278\n",
            "---------\n",
            "Epoch: 2268/3000\n",
            "Total loss:         0.094710\n",
            "Loss (Diffusion):   0.020288\n",
            "Loss (Reconstruction x20): 0.074422\n",
            "---------\n",
            "Epoch: 2269/3000\n",
            "Total loss:         0.075510\n",
            "Loss (Diffusion):   0.018721\n",
            "Loss (Reconstruction x20): 0.056789\n",
            "---------\n",
            "Epoch: 2270/3000\n",
            "Total loss:         0.087672\n",
            "Loss (Diffusion):   0.018463\n",
            "Loss (Reconstruction x20): 0.069208\n",
            "---------\n",
            "Epoch: 2271/3000\n",
            "Total loss:         0.090115\n",
            "Loss (Diffusion):   0.020435\n",
            "Loss (Reconstruction x20): 0.069680\n",
            "---------\n",
            "Epoch: 2272/3000\n",
            "Total loss:         0.095606\n",
            "Loss (Diffusion):   0.021618\n",
            "Loss (Reconstruction x20): 0.073987\n",
            "---------\n",
            "Epoch: 2273/3000\n",
            "Total loss:         0.076586\n",
            "Loss (Diffusion):   0.020658\n",
            "Loss (Reconstruction x20): 0.055929\n",
            "---------\n",
            "Epoch: 2274/3000\n",
            "Total loss:         0.096766\n",
            "Loss (Diffusion):   0.015624\n",
            "Loss (Reconstruction x20): 0.081142\n",
            "---------\n",
            "Epoch: 2275/3000\n",
            "Total loss:         0.107305\n",
            "Loss (Diffusion):   0.018688\n",
            "Loss (Reconstruction x20): 0.088617\n",
            "---------\n",
            "Epoch: 2276/3000\n",
            "Total loss:         0.092875\n",
            "Loss (Diffusion):   0.021651\n",
            "Loss (Reconstruction x20): 0.071224\n",
            "---------\n",
            "Epoch: 2277/3000\n",
            "Total loss:         0.092835\n",
            "Loss (Diffusion):   0.018587\n",
            "Loss (Reconstruction x20): 0.074248\n",
            "---------\n",
            "Epoch: 2278/3000\n",
            "Total loss:         0.109198\n",
            "Loss (Diffusion):   0.023188\n",
            "Loss (Reconstruction x20): 0.086010\n",
            "---------\n",
            "Epoch: 2279/3000\n",
            "Total loss:         0.092886\n",
            "Loss (Diffusion):   0.020276\n",
            "Loss (Reconstruction x20): 0.072610\n",
            "---------\n",
            "Epoch: 2280/3000\n",
            "Total loss:         0.083812\n",
            "Loss (Diffusion):   0.016053\n",
            "Loss (Reconstruction x20): 0.067759\n",
            "---------\n",
            "Epoch: 2281/3000\n",
            "Total loss:         0.115101\n",
            "Loss (Diffusion):   0.017741\n",
            "Loss (Reconstruction x20): 0.097360\n",
            "---------\n",
            "Epoch: 2282/3000\n",
            "Total loss:         0.090128\n",
            "Loss (Diffusion):   0.022373\n",
            "Loss (Reconstruction x20): 0.067754\n",
            "---------\n",
            "Epoch: 2283/3000\n",
            "Total loss:         0.094427\n",
            "Loss (Diffusion):   0.018612\n",
            "Loss (Reconstruction x20): 0.075815\n",
            "---------\n",
            "Epoch: 2284/3000\n",
            "Total loss:         0.088442\n",
            "Loss (Diffusion):   0.021561\n",
            "Loss (Reconstruction x20): 0.066881\n",
            "---------\n",
            "Epoch: 2285/3000\n",
            "Total loss:         0.078473\n",
            "Loss (Diffusion):   0.017719\n",
            "Loss (Reconstruction x20): 0.060754\n",
            "---------\n",
            "Epoch: 2286/3000\n",
            "Total loss:         0.082397\n",
            "Loss (Diffusion):   0.017783\n",
            "Loss (Reconstruction x20): 0.064615\n",
            "---------\n",
            "Epoch: 2287/3000\n",
            "Total loss:         0.084786\n",
            "Loss (Diffusion):   0.016268\n",
            "Loss (Reconstruction x20): 0.068518\n",
            "---------\n",
            "Epoch: 2288/3000\n",
            "Total loss:         0.114044\n",
            "Loss (Diffusion):   0.020457\n",
            "Loss (Reconstruction x20): 0.093586\n",
            "---------\n",
            "Epoch: 2289/3000\n",
            "Total loss:         0.089055\n",
            "Loss (Diffusion):   0.024055\n",
            "Loss (Reconstruction x20): 0.065000\n",
            "---------\n",
            "Epoch: 2290/3000\n",
            "Total loss:         0.068165\n",
            "Loss (Diffusion):   0.019903\n",
            "Loss (Reconstruction x20): 0.048261\n",
            "---------\n",
            "Epoch: 2291/3000\n",
            "Total loss:         0.094407\n",
            "Loss (Diffusion):   0.018867\n",
            "Loss (Reconstruction x20): 0.075541\n",
            "---------\n",
            "Epoch: 2292/3000\n",
            "Total loss:         0.085968\n",
            "Loss (Diffusion):   0.015653\n",
            "Loss (Reconstruction x20): 0.070315\n",
            "---------\n",
            "Epoch: 2293/3000\n",
            "Total loss:         0.081058\n",
            "Loss (Diffusion):   0.014615\n",
            "Loss (Reconstruction x20): 0.066442\n",
            "---------\n",
            "Epoch: 2294/3000\n",
            "Total loss:         0.076120\n",
            "Loss (Diffusion):   0.017184\n",
            "Loss (Reconstruction x20): 0.058936\n",
            "---------\n",
            "Epoch: 2295/3000\n",
            "Total loss:         0.099026\n",
            "Loss (Diffusion):   0.015688\n",
            "Loss (Reconstruction x20): 0.083338\n",
            "---------\n",
            "Epoch: 2296/3000\n",
            "Total loss:         0.089849\n",
            "Loss (Diffusion):   0.018019\n",
            "Loss (Reconstruction x20): 0.071829\n",
            "---------\n",
            "Epoch: 2297/3000\n",
            "Total loss:         0.082768\n",
            "Loss (Diffusion):   0.016937\n",
            "Loss (Reconstruction x20): 0.065831\n",
            "---------\n",
            "Epoch: 2298/3000\n",
            "Total loss:         0.086555\n",
            "Loss (Diffusion):   0.015115\n",
            "Loss (Reconstruction x20): 0.071440\n",
            "---------\n",
            "Epoch: 2299/3000\n",
            "Total loss:         0.084166\n",
            "Loss (Diffusion):   0.014416\n",
            "Loss (Reconstruction x20): 0.069751\n",
            "---------\n",
            "Epoch: 2300/3000\n",
            "Total loss:         0.082294\n",
            "Loss (Diffusion):   0.019206\n",
            "Loss (Reconstruction x20): 0.063089\n",
            "---------\n",
            "Epoch: 2301/3000\n",
            "Total loss:         0.085011\n",
            "Loss (Diffusion):   0.015304\n",
            "Loss (Reconstruction x20): 0.069707\n",
            "---------\n",
            "Epoch: 2302/3000\n",
            "Total loss:         0.088297\n",
            "Loss (Diffusion):   0.014761\n",
            "Loss (Reconstruction x20): 0.073535\n",
            "---------\n",
            "Epoch: 2303/3000\n",
            "Total loss:         0.091984\n",
            "Loss (Diffusion):   0.015136\n",
            "Loss (Reconstruction x20): 0.076849\n",
            "---------\n",
            "Epoch: 2304/3000\n",
            "Total loss:         0.084497\n",
            "Loss (Diffusion):   0.023957\n",
            "Loss (Reconstruction x20): 0.060540\n",
            "---------\n",
            "Epoch: 2305/3000\n",
            "Total loss:         0.094210\n",
            "Loss (Diffusion):   0.018954\n",
            "Loss (Reconstruction x20): 0.075256\n",
            "---------\n",
            "Epoch: 2306/3000\n",
            "Total loss:         0.099969\n",
            "Loss (Diffusion):   0.021561\n",
            "Loss (Reconstruction x20): 0.078408\n",
            "---------\n",
            "Epoch: 2307/3000\n",
            "Total loss:         0.091428\n",
            "Loss (Diffusion):   0.020482\n",
            "Loss (Reconstruction x20): 0.070946\n",
            "---------\n",
            "Epoch: 2308/3000\n",
            "Total loss:         0.093405\n",
            "Loss (Diffusion):   0.017588\n",
            "Loss (Reconstruction x20): 0.075816\n",
            "---------\n",
            "Epoch: 2309/3000\n",
            "Total loss:         0.083593\n",
            "Loss (Diffusion):   0.018993\n",
            "Loss (Reconstruction x20): 0.064599\n",
            "---------\n",
            "Epoch: 2310/3000\n",
            "Total loss:         0.100700\n",
            "Loss (Diffusion):   0.018988\n",
            "Loss (Reconstruction x20): 0.081712\n",
            "---------\n",
            "Epoch: 2311/3000\n",
            "Total loss:         0.099470\n",
            "Loss (Diffusion):   0.018537\n",
            "Loss (Reconstruction x20): 0.080933\n",
            "---------\n",
            "Epoch: 2312/3000\n",
            "Total loss:         0.094901\n",
            "Loss (Diffusion):   0.020980\n",
            "Loss (Reconstruction x20): 0.073922\n",
            "---------\n",
            "Epoch: 2313/3000\n",
            "Total loss:         0.085329\n",
            "Loss (Diffusion):   0.015234\n",
            "Loss (Reconstruction x20): 0.070094\n",
            "---------\n",
            "Epoch: 2314/3000\n",
            "Total loss:         0.081205\n",
            "Loss (Diffusion):   0.018921\n",
            "Loss (Reconstruction x20): 0.062284\n",
            "---------\n",
            "Epoch: 2315/3000\n",
            "Total loss:         0.084817\n",
            "Loss (Diffusion):   0.015307\n",
            "Loss (Reconstruction x20): 0.069510\n",
            "---------\n",
            "Epoch: 2316/3000\n",
            "Total loss:         0.113321\n",
            "Loss (Diffusion):   0.015720\n",
            "Loss (Reconstruction x20): 0.097601\n",
            "---------\n",
            "Epoch: 2317/3000\n",
            "Total loss:         0.088887\n",
            "Loss (Diffusion):   0.020358\n",
            "Loss (Reconstruction x20): 0.068529\n",
            "---------\n",
            "Epoch: 2318/3000\n",
            "Total loss:         0.102780\n",
            "Loss (Diffusion):   0.017825\n",
            "Loss (Reconstruction x20): 0.084954\n",
            "---------\n",
            "Epoch: 2319/3000\n",
            "Total loss:         0.090445\n",
            "Loss (Diffusion):   0.014884\n",
            "Loss (Reconstruction x20): 0.075561\n",
            "---------\n",
            "Epoch: 2320/3000\n",
            "Total loss:         0.105027\n",
            "Loss (Diffusion):   0.016531\n",
            "Loss (Reconstruction x20): 0.088496\n",
            "---------\n",
            "Epoch: 2321/3000\n",
            "Total loss:         0.093455\n",
            "Loss (Diffusion):   0.018153\n",
            "Loss (Reconstruction x20): 0.075302\n",
            "---------\n",
            "Epoch: 2322/3000\n",
            "Total loss:         0.099536\n",
            "Loss (Diffusion):   0.019034\n",
            "Loss (Reconstruction x20): 0.080503\n",
            "---------\n",
            "Epoch: 2323/3000\n",
            "Total loss:         0.086280\n",
            "Loss (Diffusion):   0.021880\n",
            "Loss (Reconstruction x20): 0.064400\n",
            "---------\n",
            "Epoch: 2324/3000\n",
            "Total loss:         0.091314\n",
            "Loss (Diffusion):   0.016393\n",
            "Loss (Reconstruction x20): 0.074921\n",
            "---------\n",
            "Epoch: 2325/3000\n",
            "Total loss:         0.084926\n",
            "Loss (Diffusion):   0.020048\n",
            "Loss (Reconstruction x20): 0.064878\n",
            "---------\n",
            "Epoch: 2326/3000\n",
            "Total loss:         0.092633\n",
            "Loss (Diffusion):   0.019271\n",
            "Loss (Reconstruction x20): 0.073362\n",
            "---------\n",
            "Epoch: 2327/3000\n",
            "Total loss:         0.081866\n",
            "Loss (Diffusion):   0.021604\n",
            "Loss (Reconstruction x20): 0.060262\n",
            "---------\n",
            "Epoch: 2328/3000\n",
            "Total loss:         0.100090\n",
            "Loss (Diffusion):   0.020937\n",
            "Loss (Reconstruction x20): 0.079154\n",
            "---------\n",
            "Epoch: 2329/3000\n",
            "Total loss:         0.095965\n",
            "Loss (Diffusion):   0.015842\n",
            "Loss (Reconstruction x20): 0.080123\n",
            "---------\n",
            "Epoch: 2330/3000\n",
            "Total loss:         0.096023\n",
            "Loss (Diffusion):   0.017464\n",
            "Loss (Reconstruction x20): 0.078558\n",
            "---------\n",
            "Epoch: 2331/3000\n",
            "Total loss:         0.091668\n",
            "Loss (Diffusion):   0.023107\n",
            "Loss (Reconstruction x20): 0.068561\n",
            "---------\n",
            "Epoch: 2332/3000\n",
            "Total loss:         0.074259\n",
            "Loss (Diffusion):   0.019333\n",
            "Loss (Reconstruction x20): 0.054925\n",
            "---------\n",
            "Epoch: 2333/3000\n",
            "Total loss:         0.099094\n",
            "Loss (Diffusion):   0.021051\n",
            "Loss (Reconstruction x20): 0.078043\n",
            "---------\n",
            "Epoch: 2334/3000\n",
            "Total loss:         0.099431\n",
            "Loss (Diffusion):   0.019650\n",
            "Loss (Reconstruction x20): 0.079782\n",
            "---------\n",
            "Epoch: 2335/3000\n",
            "Total loss:         0.082033\n",
            "Loss (Diffusion):   0.016562\n",
            "Loss (Reconstruction x20): 0.065470\n",
            "---------\n",
            "Epoch: 2336/3000\n",
            "Total loss:         0.092812\n",
            "Loss (Diffusion):   0.019302\n",
            "Loss (Reconstruction x20): 0.073510\n",
            "---------\n",
            "Epoch: 2337/3000\n",
            "Total loss:         0.085478\n",
            "Loss (Diffusion):   0.018189\n",
            "Loss (Reconstruction x20): 0.067289\n",
            "---------\n",
            "Epoch: 2338/3000\n",
            "Total loss:         0.091336\n",
            "Loss (Diffusion):   0.015757\n",
            "Loss (Reconstruction x20): 0.075580\n",
            "---------\n",
            "Epoch: 2339/3000\n",
            "Total loss:         0.076931\n",
            "Loss (Diffusion):   0.018038\n",
            "Loss (Reconstruction x20): 0.058894\n",
            "---------\n",
            "Epoch: 2340/3000\n",
            "Total loss:         0.092357\n",
            "Loss (Diffusion):   0.022107\n",
            "Loss (Reconstruction x20): 0.070250\n",
            "---------\n",
            "Epoch: 2341/3000\n",
            "Total loss:         0.082504\n",
            "Loss (Diffusion):   0.019248\n",
            "Loss (Reconstruction x20): 0.063256\n",
            "---------\n",
            "Epoch: 2342/3000\n",
            "Total loss:         0.083087\n",
            "Loss (Diffusion):   0.015860\n",
            "Loss (Reconstruction x20): 0.067227\n",
            "---------\n",
            "Epoch: 2343/3000\n",
            "Total loss:         0.083978\n",
            "Loss (Diffusion):   0.016154\n",
            "Loss (Reconstruction x20): 0.067824\n",
            "---------\n",
            "Epoch: 2344/3000\n",
            "Total loss:         0.077599\n",
            "Loss (Diffusion):   0.015460\n",
            "Loss (Reconstruction x20): 0.062140\n",
            "---------\n",
            "Epoch: 2345/3000\n",
            "Total loss:         0.083273\n",
            "Loss (Diffusion):   0.018213\n",
            "Loss (Reconstruction x20): 0.065060\n",
            "---------\n",
            "Epoch: 2346/3000\n",
            "Total loss:         0.098533\n",
            "Loss (Diffusion):   0.020759\n",
            "Loss (Reconstruction x20): 0.077775\n",
            "---------\n",
            "Epoch: 2347/3000\n",
            "Total loss:         0.092961\n",
            "Loss (Diffusion):   0.024259\n",
            "Loss (Reconstruction x20): 0.068702\n",
            "---------\n",
            "Epoch: 2348/3000\n",
            "Total loss:         0.090583\n",
            "Loss (Diffusion):   0.016580\n",
            "Loss (Reconstruction x20): 0.074003\n",
            "---------\n",
            "Epoch: 2349/3000\n",
            "Total loss:         0.093796\n",
            "Loss (Diffusion):   0.017989\n",
            "Loss (Reconstruction x20): 0.075808\n",
            "---------\n",
            "Epoch: 2350/3000\n",
            "Total loss:         0.079716\n",
            "Loss (Diffusion):   0.016586\n",
            "Loss (Reconstruction x20): 0.063129\n",
            "---------\n",
            "Epoch: 2351/3000\n",
            "Total loss:         0.091960\n",
            "Loss (Diffusion):   0.025237\n",
            "Loss (Reconstruction x20): 0.066723\n",
            "---------\n",
            "Epoch: 2352/3000\n",
            "Total loss:         0.097412\n",
            "Loss (Diffusion):   0.021884\n",
            "Loss (Reconstruction x20): 0.075528\n",
            "---------\n",
            "Epoch: 2353/3000\n",
            "Total loss:         0.092149\n",
            "Loss (Diffusion):   0.021851\n",
            "Loss (Reconstruction x20): 0.070299\n",
            "---------\n",
            "Epoch: 2354/3000\n",
            "Total loss:         0.093678\n",
            "Loss (Diffusion):   0.019476\n",
            "Loss (Reconstruction x20): 0.074201\n",
            "---------\n",
            "Epoch: 2355/3000\n",
            "Total loss:         0.086587\n",
            "Loss (Diffusion):   0.019154\n",
            "Loss (Reconstruction x20): 0.067433\n",
            "---------\n",
            "Epoch: 2356/3000\n",
            "Total loss:         0.099327\n",
            "Loss (Diffusion):   0.019414\n",
            "Loss (Reconstruction x20): 0.079913\n",
            "---------\n",
            "Epoch: 2357/3000\n",
            "Total loss:         0.096109\n",
            "Loss (Diffusion):   0.022506\n",
            "Loss (Reconstruction x20): 0.073602\n",
            "---------\n",
            "Epoch: 2358/3000\n",
            "Total loss:         0.093284\n",
            "Loss (Diffusion):   0.020107\n",
            "Loss (Reconstruction x20): 0.073177\n",
            "---------\n",
            "Epoch: 2359/3000\n",
            "Total loss:         0.081120\n",
            "Loss (Diffusion):   0.020518\n",
            "Loss (Reconstruction x20): 0.060602\n",
            "---------\n",
            "Epoch: 2360/3000\n",
            "Total loss:         0.100390\n",
            "Loss (Diffusion):   0.022401\n",
            "Loss (Reconstruction x20): 0.077989\n",
            "---------\n",
            "Epoch: 2361/3000\n",
            "Total loss:         0.081410\n",
            "Loss (Diffusion):   0.020401\n",
            "Loss (Reconstruction x20): 0.061009\n",
            "---------\n",
            "Epoch: 2362/3000\n",
            "Total loss:         0.085670\n",
            "Loss (Diffusion):   0.019006\n",
            "Loss (Reconstruction x20): 0.066663\n",
            "---------\n",
            "Epoch: 2363/3000\n",
            "Total loss:         0.081068\n",
            "Loss (Diffusion):   0.018877\n",
            "Loss (Reconstruction x20): 0.062191\n",
            "---------\n",
            "Epoch: 2364/3000\n",
            "Total loss:         0.093014\n",
            "Loss (Diffusion):   0.021626\n",
            "Loss (Reconstruction x20): 0.071388\n",
            "---------\n",
            "Epoch: 2365/3000\n",
            "Total loss:         0.092885\n",
            "Loss (Diffusion):   0.018852\n",
            "Loss (Reconstruction x20): 0.074032\n",
            "---------\n",
            "Epoch: 2366/3000\n",
            "Total loss:         0.091088\n",
            "Loss (Diffusion):   0.020211\n",
            "Loss (Reconstruction x20): 0.070877\n",
            "---------\n",
            "Epoch: 2367/3000\n",
            "Total loss:         0.080313\n",
            "Loss (Diffusion):   0.016829\n",
            "Loss (Reconstruction x20): 0.063483\n",
            "---------\n",
            "Epoch: 2368/3000\n",
            "Total loss:         0.098636\n",
            "Loss (Diffusion):   0.017493\n",
            "Loss (Reconstruction x20): 0.081144\n",
            "---------\n",
            "Epoch: 2369/3000\n",
            "Total loss:         0.081489\n",
            "Loss (Diffusion):   0.021078\n",
            "Loss (Reconstruction x20): 0.060410\n",
            "---------\n",
            "Epoch: 2370/3000\n",
            "Total loss:         0.078910\n",
            "Loss (Diffusion):   0.016128\n",
            "Loss (Reconstruction x20): 0.062782\n",
            "---------\n",
            "Epoch: 2371/3000\n",
            "Total loss:         0.087739\n",
            "Loss (Diffusion):   0.020207\n",
            "Loss (Reconstruction x20): 0.067532\n",
            "---------\n",
            "Epoch: 2372/3000\n",
            "Total loss:         0.110252\n",
            "Loss (Diffusion):   0.022848\n",
            "Loss (Reconstruction x20): 0.087404\n",
            "---------\n",
            "Epoch: 2373/3000\n",
            "Total loss:         0.094256\n",
            "Loss (Diffusion):   0.016012\n",
            "Loss (Reconstruction x20): 0.078244\n",
            "---------\n",
            "Epoch: 2374/3000\n",
            "Total loss:         0.093209\n",
            "Loss (Diffusion):   0.020638\n",
            "Loss (Reconstruction x20): 0.072571\n",
            "---------\n",
            "Epoch: 2375/3000\n",
            "Total loss:         0.094445\n",
            "Loss (Diffusion):   0.019156\n",
            "Loss (Reconstruction x20): 0.075288\n",
            "---------\n",
            "Epoch: 2376/3000\n",
            "Total loss:         0.092809\n",
            "Loss (Diffusion):   0.017337\n",
            "Loss (Reconstruction x20): 0.075472\n",
            "---------\n",
            "Epoch: 2377/3000\n",
            "Total loss:         0.107365\n",
            "Loss (Diffusion):   0.020063\n",
            "Loss (Reconstruction x20): 0.087303\n",
            "---------\n",
            "Epoch: 2378/3000\n",
            "Total loss:         0.095080\n",
            "Loss (Diffusion):   0.015047\n",
            "Loss (Reconstruction x20): 0.080033\n",
            "---------\n",
            "Epoch: 2379/3000\n",
            "Total loss:         0.087525\n",
            "Loss (Diffusion):   0.017137\n",
            "Loss (Reconstruction x20): 0.070388\n",
            "---------\n",
            "Epoch: 2380/3000\n",
            "Total loss:         0.085124\n",
            "Loss (Diffusion):   0.025549\n",
            "Loss (Reconstruction x20): 0.059574\n",
            "---------\n",
            "Epoch: 2381/3000\n",
            "Total loss:         0.084210\n",
            "Loss (Diffusion):   0.018114\n",
            "Loss (Reconstruction x20): 0.066096\n",
            "---------\n",
            "Epoch: 2382/3000\n",
            "Total loss:         0.091306\n",
            "Loss (Diffusion):   0.016628\n",
            "Loss (Reconstruction x20): 0.074678\n",
            "---------\n",
            "Epoch: 2383/3000\n",
            "Total loss:         0.084854\n",
            "Loss (Diffusion):   0.018427\n",
            "Loss (Reconstruction x20): 0.066428\n",
            "---------\n",
            "Epoch: 2384/3000\n",
            "Total loss:         0.083881\n",
            "Loss (Diffusion):   0.018344\n",
            "Loss (Reconstruction x20): 0.065537\n",
            "---------\n",
            "Epoch: 2385/3000\n",
            "Total loss:         0.100413\n",
            "Loss (Diffusion):   0.016520\n",
            "Loss (Reconstruction x20): 0.083893\n",
            "---------\n",
            "Epoch: 2386/3000\n",
            "Total loss:         0.090606\n",
            "Loss (Diffusion):   0.016579\n",
            "Loss (Reconstruction x20): 0.074027\n",
            "---------\n",
            "Epoch: 2387/3000\n",
            "Total loss:         0.082693\n",
            "Loss (Diffusion):   0.020141\n",
            "Loss (Reconstruction x20): 0.062553\n",
            "---------\n",
            "Epoch: 2388/3000\n",
            "Total loss:         0.096560\n",
            "Loss (Diffusion):   0.015785\n",
            "Loss (Reconstruction x20): 0.080775\n",
            "---------\n",
            "Epoch: 2389/3000\n",
            "Total loss:         0.076841\n",
            "Loss (Diffusion):   0.014513\n",
            "Loss (Reconstruction x20): 0.062328\n",
            "---------\n",
            "Epoch: 2390/3000\n",
            "Total loss:         0.082351\n",
            "Loss (Diffusion):   0.020934\n",
            "Loss (Reconstruction x20): 0.061417\n",
            "---------\n",
            "Epoch: 2391/3000\n",
            "Total loss:         0.074399\n",
            "Loss (Diffusion):   0.016625\n",
            "Loss (Reconstruction x20): 0.057773\n",
            "---------\n",
            "Epoch: 2392/3000\n",
            "Total loss:         0.076674\n",
            "Loss (Diffusion):   0.017627\n",
            "Loss (Reconstruction x20): 0.059047\n",
            "---------\n",
            "Epoch: 2393/3000\n",
            "Total loss:         0.094415\n",
            "Loss (Diffusion):   0.019827\n",
            "Loss (Reconstruction x20): 0.074588\n",
            "---------\n",
            "Epoch: 2394/3000\n",
            "Total loss:         0.085678\n",
            "Loss (Diffusion):   0.020732\n",
            "Loss (Reconstruction x20): 0.064946\n",
            "---------\n",
            "Epoch: 2395/3000\n",
            "Total loss:         0.078723\n",
            "Loss (Diffusion):   0.020754\n",
            "Loss (Reconstruction x20): 0.057969\n",
            "---------\n",
            "Epoch: 2396/3000\n",
            "Total loss:         0.102018\n",
            "Loss (Diffusion):   0.013772\n",
            "Loss (Reconstruction x20): 0.088246\n",
            "---------\n",
            "Epoch: 2397/3000\n",
            "Total loss:         0.076938\n",
            "Loss (Diffusion):   0.014624\n",
            "Loss (Reconstruction x20): 0.062313\n",
            "---------\n",
            "Epoch: 2398/3000\n",
            "Total loss:         0.083637\n",
            "Loss (Diffusion):   0.016757\n",
            "Loss (Reconstruction x20): 0.066881\n",
            "---------\n",
            "Epoch: 2399/3000\n",
            "Total loss:         0.083782\n",
            "Loss (Diffusion):   0.024180\n",
            "Loss (Reconstruction x20): 0.059602\n",
            "---------\n",
            "Epoch: 2400/3000\n",
            "Total loss:         0.090249\n",
            "Loss (Diffusion):   0.016615\n",
            "Loss (Reconstruction x20): 0.073635\n",
            "---------\n",
            "Epoch: 2401/3000\n",
            "Total loss:         0.094797\n",
            "Loss (Diffusion):   0.017992\n",
            "Loss (Reconstruction x20): 0.076805\n",
            "---------\n",
            "Epoch: 2402/3000\n",
            "Total loss:         0.103505\n",
            "Loss (Diffusion):   0.021645\n",
            "Loss (Reconstruction x20): 0.081860\n",
            "---------\n",
            "Epoch: 2403/3000\n",
            "Total loss:         0.107315\n",
            "Loss (Diffusion):   0.020525\n",
            "Loss (Reconstruction x20): 0.086790\n",
            "---------\n",
            "Epoch: 2404/3000\n",
            "Total loss:         0.101685\n",
            "Loss (Diffusion):   0.020305\n",
            "Loss (Reconstruction x20): 0.081380\n",
            "---------\n",
            "Epoch: 2405/3000\n",
            "Total loss:         0.116804\n",
            "Loss (Diffusion):   0.022060\n",
            "Loss (Reconstruction x20): 0.094744\n",
            "---------\n",
            "Epoch: 2406/3000\n",
            "Total loss:         0.118051\n",
            "Loss (Diffusion):   0.020543\n",
            "Loss (Reconstruction x20): 0.097507\n",
            "---------\n",
            "Epoch: 2407/3000\n",
            "Total loss:         0.094583\n",
            "Loss (Diffusion):   0.017795\n",
            "Loss (Reconstruction x20): 0.076788\n",
            "---------\n",
            "Epoch: 2408/3000\n",
            "Total loss:         0.105765\n",
            "Loss (Diffusion):   0.021510\n",
            "Loss (Reconstruction x20): 0.084255\n",
            "---------\n",
            "Epoch: 2409/3000\n",
            "Total loss:         0.084739\n",
            "Loss (Diffusion):   0.020347\n",
            "Loss (Reconstruction x20): 0.064392\n",
            "---------\n",
            "Epoch: 2410/3000\n",
            "Total loss:         0.093480\n",
            "Loss (Diffusion):   0.019096\n",
            "Loss (Reconstruction x20): 0.074384\n",
            "---------\n",
            "Epoch: 2411/3000\n",
            "Total loss:         0.098229\n",
            "Loss (Diffusion):   0.021960\n",
            "Loss (Reconstruction x20): 0.076269\n",
            "---------\n",
            "Epoch: 2412/3000\n",
            "Total loss:         0.100106\n",
            "Loss (Diffusion):   0.019534\n",
            "Loss (Reconstruction x20): 0.080572\n",
            "---------\n",
            "Epoch: 2413/3000\n",
            "Total loss:         0.090229\n",
            "Loss (Diffusion):   0.019218\n",
            "Loss (Reconstruction x20): 0.071010\n",
            "---------\n",
            "Epoch: 2414/3000\n",
            "Total loss:         0.093223\n",
            "Loss (Diffusion):   0.020419\n",
            "Loss (Reconstruction x20): 0.072804\n",
            "---------\n",
            "Epoch: 2415/3000\n",
            "Total loss:         0.098360\n",
            "Loss (Diffusion):   0.017241\n",
            "Loss (Reconstruction x20): 0.081119\n",
            "---------\n",
            "Epoch: 2416/3000\n",
            "Total loss:         0.093170\n",
            "Loss (Diffusion):   0.019179\n",
            "Loss (Reconstruction x20): 0.073991\n",
            "---------\n",
            "Epoch: 2417/3000\n",
            "Total loss:         0.092898\n",
            "Loss (Diffusion):   0.016601\n",
            "Loss (Reconstruction x20): 0.076296\n",
            "---------\n",
            "Epoch: 2418/3000\n",
            "Total loss:         0.089860\n",
            "Loss (Diffusion):   0.017457\n",
            "Loss (Reconstruction x20): 0.072403\n",
            "---------\n",
            "Epoch: 2419/3000\n",
            "Total loss:         0.077228\n",
            "Loss (Diffusion):   0.019090\n",
            "Loss (Reconstruction x20): 0.058138\n",
            "---------\n",
            "Epoch: 2420/3000\n",
            "Total loss:         0.086206\n",
            "Loss (Diffusion):   0.016414\n",
            "Loss (Reconstruction x20): 0.069792\n",
            "---------\n",
            "Epoch: 2421/3000\n",
            "Total loss:         0.081772\n",
            "Loss (Diffusion):   0.021116\n",
            "Loss (Reconstruction x20): 0.060656\n",
            "---------\n",
            "Epoch: 2422/3000\n",
            "Total loss:         0.091296\n",
            "Loss (Diffusion):   0.020838\n",
            "Loss (Reconstruction x20): 0.070459\n",
            "---------\n",
            "Epoch: 2423/3000\n",
            "Total loss:         0.093639\n",
            "Loss (Diffusion):   0.017732\n",
            "Loss (Reconstruction x20): 0.075907\n",
            "---------\n",
            "Epoch: 2424/3000\n",
            "Total loss:         0.093730\n",
            "Loss (Diffusion):   0.020054\n",
            "Loss (Reconstruction x20): 0.073677\n",
            "---------\n",
            "Epoch: 2425/3000\n",
            "Total loss:         0.082944\n",
            "Loss (Diffusion):   0.016753\n",
            "Loss (Reconstruction x20): 0.066190\n",
            "---------\n",
            "Epoch: 2426/3000\n",
            "Total loss:         0.091010\n",
            "Loss (Diffusion):   0.014720\n",
            "Loss (Reconstruction x20): 0.076290\n",
            "---------\n",
            "Epoch: 2427/3000\n",
            "Total loss:         0.077754\n",
            "Loss (Diffusion):   0.018138\n",
            "Loss (Reconstruction x20): 0.059615\n",
            "---------\n",
            "Epoch: 2428/3000\n",
            "Total loss:         0.082228\n",
            "Loss (Diffusion):   0.017483\n",
            "Loss (Reconstruction x20): 0.064745\n",
            "---------\n",
            "Epoch: 2429/3000\n",
            "Total loss:         0.085457\n",
            "Loss (Diffusion):   0.020703\n",
            "Loss (Reconstruction x20): 0.064754\n",
            "---------\n",
            "Epoch: 2430/3000\n",
            "Total loss:         0.088182\n",
            "Loss (Diffusion):   0.020493\n",
            "Loss (Reconstruction x20): 0.067689\n",
            "---------\n",
            "Epoch: 2431/3000\n",
            "Total loss:         0.081558\n",
            "Loss (Diffusion):   0.019681\n",
            "Loss (Reconstruction x20): 0.061876\n",
            "---------\n",
            "Epoch: 2432/3000\n",
            "Total loss:         0.089676\n",
            "Loss (Diffusion):   0.020947\n",
            "Loss (Reconstruction x20): 0.068728\n",
            "---------\n",
            "Epoch: 2433/3000\n",
            "Total loss:         0.105828\n",
            "Loss (Diffusion):   0.023203\n",
            "Loss (Reconstruction x20): 0.082625\n",
            "---------\n",
            "Epoch: 2434/3000\n",
            "Total loss:         0.101170\n",
            "Loss (Diffusion):   0.020107\n",
            "Loss (Reconstruction x20): 0.081062\n",
            "---------\n",
            "Epoch: 2435/3000\n",
            "Total loss:         0.099328\n",
            "Loss (Diffusion):   0.019197\n",
            "Loss (Reconstruction x20): 0.080132\n",
            "---------\n",
            "Epoch: 2436/3000\n",
            "Total loss:         0.095423\n",
            "Loss (Diffusion):   0.020439\n",
            "Loss (Reconstruction x20): 0.074984\n",
            "---------\n",
            "Epoch: 2437/3000\n",
            "Total loss:         0.111386\n",
            "Loss (Diffusion):   0.022852\n",
            "Loss (Reconstruction x20): 0.088534\n",
            "---------\n",
            "Epoch: 2438/3000\n",
            "Total loss:         0.098096\n",
            "Loss (Diffusion):   0.021407\n",
            "Loss (Reconstruction x20): 0.076688\n",
            "---------\n",
            "Epoch: 2439/3000\n",
            "Total loss:         0.107292\n",
            "Loss (Diffusion):   0.017048\n",
            "Loss (Reconstruction x20): 0.090243\n",
            "---------\n",
            "Epoch: 2440/3000\n",
            "Total loss:         0.094293\n",
            "Loss (Diffusion):   0.016439\n",
            "Loss (Reconstruction x20): 0.077854\n",
            "---------\n",
            "Epoch: 2441/3000\n",
            "Total loss:         0.091960\n",
            "Loss (Diffusion):   0.021288\n",
            "Loss (Reconstruction x20): 0.070672\n",
            "---------\n",
            "Epoch: 2442/3000\n",
            "Total loss:         0.093291\n",
            "Loss (Diffusion):   0.020559\n",
            "Loss (Reconstruction x20): 0.072732\n",
            "---------\n",
            "Epoch: 2443/3000\n",
            "Total loss:         0.097311\n",
            "Loss (Diffusion):   0.021187\n",
            "Loss (Reconstruction x20): 0.076123\n",
            "---------\n",
            "Epoch: 2444/3000\n",
            "Total loss:         0.098008\n",
            "Loss (Diffusion):   0.018695\n",
            "Loss (Reconstruction x20): 0.079312\n",
            "---------\n",
            "Epoch: 2445/3000\n",
            "Total loss:         0.102446\n",
            "Loss (Diffusion):   0.016488\n",
            "Loss (Reconstruction x20): 0.085958\n",
            "---------\n",
            "Epoch: 2446/3000\n",
            "Total loss:         0.093192\n",
            "Loss (Diffusion):   0.018038\n",
            "Loss (Reconstruction x20): 0.075154\n",
            "---------\n",
            "Epoch: 2447/3000\n",
            "Total loss:         0.092088\n",
            "Loss (Diffusion):   0.018440\n",
            "Loss (Reconstruction x20): 0.073649\n",
            "---------\n",
            "Epoch: 2448/3000\n",
            "Total loss:         0.092668\n",
            "Loss (Diffusion):   0.019152\n",
            "Loss (Reconstruction x20): 0.073517\n",
            "---------\n",
            "Epoch: 2449/3000\n",
            "Total loss:         0.093972\n",
            "Loss (Diffusion):   0.019424\n",
            "Loss (Reconstruction x20): 0.074548\n",
            "---------\n",
            "Epoch: 2450/3000\n",
            "Total loss:         0.095876\n",
            "Loss (Diffusion):   0.018834\n",
            "Loss (Reconstruction x20): 0.077041\n",
            "---------\n",
            "Epoch: 2451/3000\n",
            "Total loss:         0.104519\n",
            "Loss (Diffusion):   0.018521\n",
            "Loss (Reconstruction x20): 0.085998\n",
            "---------\n",
            "Epoch: 2452/3000\n",
            "Total loss:         0.098549\n",
            "Loss (Diffusion):   0.019922\n",
            "Loss (Reconstruction x20): 0.078627\n",
            "---------\n",
            "Epoch: 2453/3000\n",
            "Total loss:         0.095071\n",
            "Loss (Diffusion):   0.020651\n",
            "Loss (Reconstruction x20): 0.074421\n",
            "---------\n",
            "Epoch: 2454/3000\n",
            "Total loss:         0.097798\n",
            "Loss (Diffusion):   0.014708\n",
            "Loss (Reconstruction x20): 0.083091\n",
            "---------\n",
            "Epoch: 2455/3000\n",
            "Total loss:         0.093449\n",
            "Loss (Diffusion):   0.021337\n",
            "Loss (Reconstruction x20): 0.072113\n",
            "---------\n",
            "Epoch: 2456/3000\n",
            "Total loss:         0.098061\n",
            "Loss (Diffusion):   0.019075\n",
            "Loss (Reconstruction x20): 0.078986\n",
            "---------\n",
            "Epoch: 2457/3000\n",
            "Total loss:         0.093631\n",
            "Loss (Diffusion):   0.019542\n",
            "Loss (Reconstruction x20): 0.074088\n",
            "---------\n",
            "Epoch: 2458/3000\n",
            "Total loss:         0.127454\n",
            "Loss (Diffusion):   0.022162\n",
            "Loss (Reconstruction x20): 0.105292\n",
            "---------\n",
            "Epoch: 2459/3000\n",
            "Total loss:         0.094431\n",
            "Loss (Diffusion):   0.023568\n",
            "Loss (Reconstruction x20): 0.070863\n",
            "---------\n",
            "Epoch: 2460/3000\n",
            "Total loss:         0.113919\n",
            "Loss (Diffusion):   0.018973\n",
            "Loss (Reconstruction x20): 0.094945\n",
            "---------\n",
            "Epoch: 2461/3000\n",
            "Total loss:         0.092257\n",
            "Loss (Diffusion):   0.016999\n",
            "Loss (Reconstruction x20): 0.075258\n",
            "---------\n",
            "Epoch: 2462/3000\n",
            "Total loss:         0.104209\n",
            "Loss (Diffusion):   0.020545\n",
            "Loss (Reconstruction x20): 0.083665\n",
            "---------\n",
            "Epoch: 2463/3000\n",
            "Total loss:         0.101079\n",
            "Loss (Diffusion):   0.022488\n",
            "Loss (Reconstruction x20): 0.078591\n",
            "---------\n",
            "Epoch: 2464/3000\n",
            "Total loss:         0.106422\n",
            "Loss (Diffusion):   0.023633\n",
            "Loss (Reconstruction x20): 0.082789\n",
            "---------\n",
            "Epoch: 2465/3000\n",
            "Total loss:         0.107408\n",
            "Loss (Diffusion):   0.018143\n",
            "Loss (Reconstruction x20): 0.089265\n",
            "---------\n",
            "Epoch: 2466/3000\n",
            "Total loss:         0.092845\n",
            "Loss (Diffusion):   0.024245\n",
            "Loss (Reconstruction x20): 0.068600\n",
            "---------\n",
            "Epoch: 2467/3000\n",
            "Total loss:         0.099867\n",
            "Loss (Diffusion):   0.017519\n",
            "Loss (Reconstruction x20): 0.082348\n",
            "---------\n",
            "Epoch: 2468/3000\n",
            "Total loss:         0.091752\n",
            "Loss (Diffusion):   0.019680\n",
            "Loss (Reconstruction x20): 0.072073\n",
            "---------\n",
            "Epoch: 2469/3000\n",
            "Total loss:         0.086787\n",
            "Loss (Diffusion):   0.016366\n",
            "Loss (Reconstruction x20): 0.070421\n",
            "---------\n",
            "Epoch: 2470/3000\n",
            "Total loss:         0.084162\n",
            "Loss (Diffusion):   0.017269\n",
            "Loss (Reconstruction x20): 0.066893\n",
            "---------\n",
            "Epoch: 2471/3000\n",
            "Total loss:         0.089511\n",
            "Loss (Diffusion):   0.019399\n",
            "Loss (Reconstruction x20): 0.070112\n",
            "---------\n",
            "Epoch: 2472/3000\n",
            "Total loss:         0.098487\n",
            "Loss (Diffusion):   0.016822\n",
            "Loss (Reconstruction x20): 0.081665\n",
            "---------\n",
            "Epoch: 2473/3000\n",
            "Total loss:         0.104601\n",
            "Loss (Diffusion):   0.023688\n",
            "Loss (Reconstruction x20): 0.080914\n",
            "---------\n",
            "Epoch: 2474/3000\n",
            "Total loss:         0.086641\n",
            "Loss (Diffusion):   0.016624\n",
            "Loss (Reconstruction x20): 0.070017\n",
            "---------\n",
            "Epoch: 2475/3000\n",
            "Total loss:         0.093050\n",
            "Loss (Diffusion):   0.019652\n",
            "Loss (Reconstruction x20): 0.073398\n",
            "---------\n",
            "Epoch: 2476/3000\n",
            "Total loss:         0.093580\n",
            "Loss (Diffusion):   0.018456\n",
            "Loss (Reconstruction x20): 0.075124\n",
            "---------\n",
            "Epoch: 2477/3000\n",
            "Total loss:         0.096060\n",
            "Loss (Diffusion):   0.019673\n",
            "Loss (Reconstruction x20): 0.076387\n",
            "---------\n",
            "Epoch: 2478/3000\n",
            "Total loss:         0.099691\n",
            "Loss (Diffusion):   0.016504\n",
            "Loss (Reconstruction x20): 0.083187\n",
            "---------\n",
            "Epoch: 2479/3000\n",
            "Total loss:         0.094899\n",
            "Loss (Diffusion):   0.022358\n",
            "Loss (Reconstruction x20): 0.072541\n",
            "---------\n",
            "Epoch: 2480/3000\n",
            "Total loss:         0.088827\n",
            "Loss (Diffusion):   0.018729\n",
            "Loss (Reconstruction x20): 0.070097\n",
            "---------\n",
            "Epoch: 2481/3000\n",
            "Total loss:         0.088434\n",
            "Loss (Diffusion):   0.018923\n",
            "Loss (Reconstruction x20): 0.069511\n",
            "---------\n",
            "Epoch: 2482/3000\n",
            "Total loss:         0.084122\n",
            "Loss (Diffusion):   0.019968\n",
            "Loss (Reconstruction x20): 0.064155\n",
            "---------\n",
            "Epoch: 2483/3000\n",
            "Total loss:         0.105976\n",
            "Loss (Diffusion):   0.017057\n",
            "Loss (Reconstruction x20): 0.088920\n",
            "---------\n",
            "Epoch: 2484/3000\n",
            "Total loss:         0.094836\n",
            "Loss (Diffusion):   0.018232\n",
            "Loss (Reconstruction x20): 0.076604\n",
            "---------\n",
            "Epoch: 2485/3000\n",
            "Total loss:         0.111791\n",
            "Loss (Diffusion):   0.016478\n",
            "Loss (Reconstruction x20): 0.095313\n",
            "---------\n",
            "Epoch: 2486/3000\n",
            "Total loss:         0.096287\n",
            "Loss (Diffusion):   0.016732\n",
            "Loss (Reconstruction x20): 0.079556\n",
            "---------\n",
            "Epoch: 2487/3000\n",
            "Total loss:         0.087676\n",
            "Loss (Diffusion):   0.018429\n",
            "Loss (Reconstruction x20): 0.069247\n",
            "---------\n",
            "Epoch: 2488/3000\n",
            "Total loss:         0.088160\n",
            "Loss (Diffusion):   0.020677\n",
            "Loss (Reconstruction x20): 0.067483\n",
            "---------\n",
            "Epoch: 2489/3000\n",
            "Total loss:         0.092043\n",
            "Loss (Diffusion):   0.018748\n",
            "Loss (Reconstruction x20): 0.073295\n",
            "---------\n",
            "Epoch: 2490/3000\n",
            "Total loss:         0.103065\n",
            "Loss (Diffusion):   0.017504\n",
            "Loss (Reconstruction x20): 0.085560\n",
            "---------\n",
            "Epoch: 2491/3000\n",
            "Total loss:         0.083492\n",
            "Loss (Diffusion):   0.017195\n",
            "Loss (Reconstruction x20): 0.066297\n",
            "---------\n",
            "Epoch: 2492/3000\n",
            "Total loss:         0.087213\n",
            "Loss (Diffusion):   0.017443\n",
            "Loss (Reconstruction x20): 0.069770\n",
            "---------\n",
            "Epoch: 2493/3000\n",
            "Total loss:         0.086814\n",
            "Loss (Diffusion):   0.017701\n",
            "Loss (Reconstruction x20): 0.069112\n",
            "---------\n",
            "Epoch: 2494/3000\n",
            "Total loss:         0.091933\n",
            "Loss (Diffusion):   0.017702\n",
            "Loss (Reconstruction x20): 0.074231\n",
            "---------\n",
            "Epoch: 2495/3000\n",
            "Total loss:         0.092781\n",
            "Loss (Diffusion):   0.016446\n",
            "Loss (Reconstruction x20): 0.076335\n",
            "---------\n",
            "Epoch: 2496/3000\n",
            "Total loss:         0.086670\n",
            "Loss (Diffusion):   0.015536\n",
            "Loss (Reconstruction x20): 0.071134\n",
            "---------\n",
            "Epoch: 2497/3000\n",
            "Total loss:         0.076303\n",
            "Loss (Diffusion):   0.017814\n",
            "Loss (Reconstruction x20): 0.058489\n",
            "---------\n",
            "Epoch: 2498/3000\n",
            "Total loss:         0.089818\n",
            "Loss (Diffusion):   0.016137\n",
            "Loss (Reconstruction x20): 0.073681\n",
            "---------\n",
            "Epoch: 2499/3000\n",
            "Total loss:         0.091791\n",
            "Loss (Diffusion):   0.021049\n",
            "Loss (Reconstruction x20): 0.070742\n",
            "---------\n",
            "Epoch: 2500/3000\n",
            "Total loss:         0.073638\n",
            "Loss (Diffusion):   0.014778\n",
            "Loss (Reconstruction x20): 0.058860\n",
            "---------\n",
            "Epoch: 2501/3000\n",
            "Total loss:         0.081203\n",
            "Loss (Diffusion):   0.016458\n",
            "Loss (Reconstruction x20): 0.064745\n",
            "---------\n",
            "Epoch: 2502/3000\n",
            "Total loss:         0.089636\n",
            "Loss (Diffusion):   0.018993\n",
            "Loss (Reconstruction x20): 0.070643\n",
            "---------\n",
            "Epoch: 2503/3000\n",
            "Total loss:         0.088220\n",
            "Loss (Diffusion):   0.017233\n",
            "Loss (Reconstruction x20): 0.070987\n",
            "---------\n",
            "Epoch: 2504/3000\n",
            "Total loss:         0.089835\n",
            "Loss (Diffusion):   0.019082\n",
            "Loss (Reconstruction x20): 0.070753\n",
            "---------\n",
            "Epoch: 2505/3000\n",
            "Total loss:         0.079842\n",
            "Loss (Diffusion):   0.017157\n",
            "Loss (Reconstruction x20): 0.062684\n",
            "---------\n",
            "Epoch: 2506/3000\n",
            "Total loss:         0.091879\n",
            "Loss (Diffusion):   0.022050\n",
            "Loss (Reconstruction x20): 0.069829\n",
            "---------\n",
            "Epoch: 2507/3000\n",
            "Total loss:         0.076131\n",
            "Loss (Diffusion):   0.014132\n",
            "Loss (Reconstruction x20): 0.062000\n",
            "---------\n",
            "Epoch: 2508/3000\n",
            "Total loss:         0.080258\n",
            "Loss (Diffusion):   0.018472\n",
            "Loss (Reconstruction x20): 0.061785\n",
            "---------\n",
            "Epoch: 2509/3000\n",
            "Total loss:         0.110894\n",
            "Loss (Diffusion):   0.016514\n",
            "Loss (Reconstruction x20): 0.094380\n",
            "---------\n",
            "Epoch: 2510/3000\n",
            "Total loss:         0.089198\n",
            "Loss (Diffusion):   0.017025\n",
            "Loss (Reconstruction x20): 0.072173\n",
            "---------\n",
            "Epoch: 2511/3000\n",
            "Total loss:         0.098837\n",
            "Loss (Diffusion):   0.017524\n",
            "Loss (Reconstruction x20): 0.081313\n",
            "---------\n",
            "Epoch: 2512/3000\n",
            "Total loss:         0.091835\n",
            "Loss (Diffusion):   0.018093\n",
            "Loss (Reconstruction x20): 0.073743\n",
            "---------\n",
            "Epoch: 2513/3000\n",
            "Total loss:         0.083173\n",
            "Loss (Diffusion):   0.014099\n",
            "Loss (Reconstruction x20): 0.069075\n",
            "---------\n",
            "Epoch: 2514/3000\n",
            "Total loss:         0.070795\n",
            "Loss (Diffusion):   0.020986\n",
            "Loss (Reconstruction x20): 0.049810\n",
            "---------\n",
            "Epoch: 2515/3000\n",
            "Total loss:         0.081332\n",
            "Loss (Diffusion):   0.014902\n",
            "Loss (Reconstruction x20): 0.066430\n",
            "---------\n",
            "Epoch: 2516/3000\n",
            "Total loss:         0.098481\n",
            "Loss (Diffusion):   0.018582\n",
            "Loss (Reconstruction x20): 0.079899\n",
            "---------\n",
            "Epoch: 2517/3000\n",
            "Total loss:         0.082441\n",
            "Loss (Diffusion):   0.016643\n",
            "Loss (Reconstruction x20): 0.065798\n",
            "---------\n",
            "Epoch: 2518/3000\n",
            "Total loss:         0.083152\n",
            "Loss (Diffusion):   0.018385\n",
            "Loss (Reconstruction x20): 0.064767\n",
            "---------\n",
            "Epoch: 2519/3000\n",
            "Total loss:         0.086817\n",
            "Loss (Diffusion):   0.017775\n",
            "Loss (Reconstruction x20): 0.069042\n",
            "---------\n",
            "Epoch: 2520/3000\n",
            "Total loss:         0.085850\n",
            "Loss (Diffusion):   0.018192\n",
            "Loss (Reconstruction x20): 0.067658\n",
            "---------\n",
            "Epoch: 2521/3000\n",
            "Total loss:         0.082384\n",
            "Loss (Diffusion):   0.017337\n",
            "Loss (Reconstruction x20): 0.065047\n",
            "---------\n",
            "Epoch: 2522/3000\n",
            "Total loss:         0.077148\n",
            "Loss (Diffusion):   0.021821\n",
            "Loss (Reconstruction x20): 0.055327\n",
            "---------\n",
            "Epoch: 2523/3000\n",
            "Total loss:         0.073284\n",
            "Loss (Diffusion):   0.014652\n",
            "Loss (Reconstruction x20): 0.058631\n",
            "---------\n",
            "Epoch: 2524/3000\n",
            "Total loss:         0.086980\n",
            "Loss (Diffusion):   0.018964\n",
            "Loss (Reconstruction x20): 0.068017\n",
            "---------\n",
            "Epoch: 2525/3000\n",
            "Total loss:         0.080422\n",
            "Loss (Diffusion):   0.016752\n",
            "Loss (Reconstruction x20): 0.063670\n",
            "---------\n",
            "Epoch: 2526/3000\n",
            "Total loss:         0.080422\n",
            "Loss (Diffusion):   0.017619\n",
            "Loss (Reconstruction x20): 0.062802\n",
            "---------\n",
            "Epoch: 2527/3000\n",
            "Total loss:         0.088269\n",
            "Loss (Diffusion):   0.019173\n",
            "Loss (Reconstruction x20): 0.069096\n",
            "---------\n",
            "Epoch: 2528/3000\n",
            "Total loss:         0.073526\n",
            "Loss (Diffusion):   0.018265\n",
            "Loss (Reconstruction x20): 0.055261\n",
            "---------\n",
            "Epoch: 2529/3000\n",
            "Total loss:         0.077681\n",
            "Loss (Diffusion):   0.014706\n",
            "Loss (Reconstruction x20): 0.062976\n",
            "---------\n",
            "Epoch: 2530/3000\n",
            "Total loss:         0.094450\n",
            "Loss (Diffusion):   0.017098\n",
            "Loss (Reconstruction x20): 0.077352\n",
            "---------\n",
            "Epoch: 2531/3000\n",
            "Total loss:         0.083657\n",
            "Loss (Diffusion):   0.015114\n",
            "Loss (Reconstruction x20): 0.068543\n",
            "---------\n",
            "Epoch: 2532/3000\n",
            "Total loss:         0.078190\n",
            "Loss (Diffusion):   0.017569\n",
            "Loss (Reconstruction x20): 0.060621\n",
            "---------\n",
            "Epoch: 2533/3000\n",
            "Total loss:         0.102601\n",
            "Loss (Diffusion):   0.017498\n",
            "Loss (Reconstruction x20): 0.085103\n",
            "---------\n",
            "Epoch: 2534/3000\n",
            "Total loss:         0.079013\n",
            "Loss (Diffusion):   0.017640\n",
            "Loss (Reconstruction x20): 0.061373\n",
            "---------\n",
            "Epoch: 2535/3000\n",
            "Total loss:         0.093728\n",
            "Loss (Diffusion):   0.018652\n",
            "Loss (Reconstruction x20): 0.075077\n",
            "---------\n",
            "Epoch: 2536/3000\n",
            "Total loss:         0.096682\n",
            "Loss (Diffusion):   0.021229\n",
            "Loss (Reconstruction x20): 0.075453\n",
            "---------\n",
            "Epoch: 2537/3000\n",
            "Total loss:         0.079503\n",
            "Loss (Diffusion):   0.014025\n",
            "Loss (Reconstruction x20): 0.065478\n",
            "---------\n",
            "Epoch: 2538/3000\n",
            "Total loss:         0.087543\n",
            "Loss (Diffusion):   0.021418\n",
            "Loss (Reconstruction x20): 0.066125\n",
            "---------\n",
            "Epoch: 2539/3000\n",
            "Total loss:         0.097538\n",
            "Loss (Diffusion):   0.018892\n",
            "Loss (Reconstruction x20): 0.078646\n",
            "---------\n",
            "Epoch: 2540/3000\n",
            "Total loss:         0.081954\n",
            "Loss (Diffusion):   0.016452\n",
            "Loss (Reconstruction x20): 0.065502\n",
            "---------\n",
            "Epoch: 2541/3000\n",
            "Total loss:         0.092523\n",
            "Loss (Diffusion):   0.017081\n",
            "Loss (Reconstruction x20): 0.075442\n",
            "---------\n",
            "Epoch: 2542/3000\n",
            "Total loss:         0.092308\n",
            "Loss (Diffusion):   0.018850\n",
            "Loss (Reconstruction x20): 0.073457\n",
            "---------\n",
            "Epoch: 2543/3000\n",
            "Total loss:         0.086266\n",
            "Loss (Diffusion):   0.014992\n",
            "Loss (Reconstruction x20): 0.071275\n",
            "---------\n",
            "Epoch: 2544/3000\n",
            "Total loss:         0.086881\n",
            "Loss (Diffusion):   0.019234\n",
            "Loss (Reconstruction x20): 0.067647\n",
            "---------\n",
            "Epoch: 2545/3000\n",
            "Total loss:         0.097549\n",
            "Loss (Diffusion):   0.019149\n",
            "Loss (Reconstruction x20): 0.078400\n",
            "---------\n",
            "Epoch: 2546/3000\n",
            "Total loss:         0.079445\n",
            "Loss (Diffusion):   0.018789\n",
            "Loss (Reconstruction x20): 0.060656\n",
            "---------\n",
            "Epoch: 2547/3000\n",
            "Total loss:         0.091148\n",
            "Loss (Diffusion):   0.014923\n",
            "Loss (Reconstruction x20): 0.076225\n",
            "---------\n",
            "Epoch: 2548/3000\n",
            "Total loss:         0.073035\n",
            "Loss (Diffusion):   0.017928\n",
            "Loss (Reconstruction x20): 0.055106\n",
            "---------\n",
            "Epoch: 2549/3000\n",
            "Total loss:         0.094211\n",
            "Loss (Diffusion):   0.020818\n",
            "Loss (Reconstruction x20): 0.073393\n",
            "---------\n",
            "Epoch: 2550/3000\n",
            "Total loss:         0.086471\n",
            "Loss (Diffusion):   0.016415\n",
            "Loss (Reconstruction x20): 0.070056\n",
            "---------\n",
            "Epoch: 2551/3000\n",
            "Total loss:         0.108462\n",
            "Loss (Diffusion):   0.019313\n",
            "Loss (Reconstruction x20): 0.089149\n",
            "---------\n",
            "Epoch: 2552/3000\n",
            "Total loss:         0.094017\n",
            "Loss (Diffusion):   0.020705\n",
            "Loss (Reconstruction x20): 0.073312\n",
            "---------\n",
            "Epoch: 2553/3000\n",
            "Total loss:         0.086783\n",
            "Loss (Diffusion):   0.017688\n",
            "Loss (Reconstruction x20): 0.069095\n",
            "---------\n",
            "Epoch: 2554/3000\n",
            "Total loss:         0.101474\n",
            "Loss (Diffusion):   0.021475\n",
            "Loss (Reconstruction x20): 0.079999\n",
            "---------\n",
            "Epoch: 2555/3000\n",
            "Total loss:         0.101320\n",
            "Loss (Diffusion):   0.019253\n",
            "Loss (Reconstruction x20): 0.082067\n",
            "---------\n",
            "Epoch: 2556/3000\n",
            "Total loss:         0.114620\n",
            "Loss (Diffusion):   0.019983\n",
            "Loss (Reconstruction x20): 0.094637\n",
            "---------\n",
            "Epoch: 2557/3000\n",
            "Total loss:         0.093226\n",
            "Loss (Diffusion):   0.014765\n",
            "Loss (Reconstruction x20): 0.078462\n",
            "---------\n",
            "Epoch: 2558/3000\n",
            "Total loss:         0.120853\n",
            "Loss (Diffusion):   0.021291\n",
            "Loss (Reconstruction x20): 0.099561\n",
            "---------\n",
            "Epoch: 2559/3000\n",
            "Total loss:         0.097377\n",
            "Loss (Diffusion):   0.016057\n",
            "Loss (Reconstruction x20): 0.081320\n",
            "---------\n",
            "Epoch: 2560/3000\n",
            "Total loss:         0.119475\n",
            "Loss (Diffusion):   0.016415\n",
            "Loss (Reconstruction x20): 0.103060\n",
            "---------\n",
            "Epoch: 2561/3000\n",
            "Total loss:         0.104460\n",
            "Loss (Diffusion):   0.017311\n",
            "Loss (Reconstruction x20): 0.087149\n",
            "---------\n",
            "Epoch: 2562/3000\n",
            "Total loss:         0.112178\n",
            "Loss (Diffusion):   0.018535\n",
            "Loss (Reconstruction x20): 0.093643\n",
            "---------\n",
            "Epoch: 2563/3000\n",
            "Total loss:         0.095970\n",
            "Loss (Diffusion):   0.019447\n",
            "Loss (Reconstruction x20): 0.076522\n",
            "---------\n",
            "Epoch: 2564/3000\n",
            "Total loss:         0.092103\n",
            "Loss (Diffusion):   0.017572\n",
            "Loss (Reconstruction x20): 0.074532\n",
            "---------\n",
            "Epoch: 2565/3000\n",
            "Total loss:         0.094800\n",
            "Loss (Diffusion):   0.015446\n",
            "Loss (Reconstruction x20): 0.079354\n",
            "---------\n",
            "Epoch: 2566/3000\n",
            "Total loss:         0.101489\n",
            "Loss (Diffusion):   0.017117\n",
            "Loss (Reconstruction x20): 0.084372\n",
            "---------\n",
            "Epoch: 2567/3000\n",
            "Total loss:         0.090375\n",
            "Loss (Diffusion):   0.019070\n",
            "Loss (Reconstruction x20): 0.071305\n",
            "---------\n",
            "Epoch: 2568/3000\n",
            "Total loss:         0.093230\n",
            "Loss (Diffusion):   0.018842\n",
            "Loss (Reconstruction x20): 0.074387\n",
            "---------\n",
            "Epoch: 2569/3000\n",
            "Total loss:         0.098209\n",
            "Loss (Diffusion):   0.017909\n",
            "Loss (Reconstruction x20): 0.080300\n",
            "---------\n",
            "Epoch: 2570/3000\n",
            "Total loss:         0.085620\n",
            "Loss (Diffusion):   0.019244\n",
            "Loss (Reconstruction x20): 0.066376\n",
            "---------\n",
            "Epoch: 2571/3000\n",
            "Total loss:         0.099649\n",
            "Loss (Diffusion):   0.019207\n",
            "Loss (Reconstruction x20): 0.080442\n",
            "---------\n",
            "Epoch: 2572/3000\n",
            "Total loss:         0.099596\n",
            "Loss (Diffusion):   0.016032\n",
            "Loss (Reconstruction x20): 0.083564\n",
            "---------\n",
            "Epoch: 2573/3000\n",
            "Total loss:         0.088232\n",
            "Loss (Diffusion):   0.018809\n",
            "Loss (Reconstruction x20): 0.069423\n",
            "---------\n",
            "Epoch: 2574/3000\n",
            "Total loss:         0.089999\n",
            "Loss (Diffusion):   0.019974\n",
            "Loss (Reconstruction x20): 0.070026\n",
            "---------\n",
            "Epoch: 2575/3000\n",
            "Total loss:         0.101020\n",
            "Loss (Diffusion):   0.019465\n",
            "Loss (Reconstruction x20): 0.081555\n",
            "---------\n",
            "Epoch: 2576/3000\n",
            "Total loss:         0.088423\n",
            "Loss (Diffusion):   0.017254\n",
            "Loss (Reconstruction x20): 0.071169\n",
            "---------\n",
            "Epoch: 2577/3000\n",
            "Total loss:         0.088130\n",
            "Loss (Diffusion):   0.017929\n",
            "Loss (Reconstruction x20): 0.070201\n",
            "---------\n",
            "Epoch: 2578/3000\n",
            "Total loss:         0.085146\n",
            "Loss (Diffusion):   0.018221\n",
            "Loss (Reconstruction x20): 0.066924\n",
            "---------\n",
            "Epoch: 2579/3000\n",
            "Total loss:         0.108341\n",
            "Loss (Diffusion):   0.021402\n",
            "Loss (Reconstruction x20): 0.086939\n",
            "---------\n",
            "Epoch: 2580/3000\n",
            "Total loss:         0.103105\n",
            "Loss (Diffusion):   0.019290\n",
            "Loss (Reconstruction x20): 0.083815\n",
            "---------\n",
            "Epoch: 2581/3000\n",
            "Total loss:         0.104324\n",
            "Loss (Diffusion):   0.018846\n",
            "Loss (Reconstruction x20): 0.085478\n",
            "---------\n",
            "Epoch: 2582/3000\n",
            "Total loss:         0.095921\n",
            "Loss (Diffusion):   0.019127\n",
            "Loss (Reconstruction x20): 0.076794\n",
            "---------\n",
            "Epoch: 2583/3000\n",
            "Total loss:         0.114110\n",
            "Loss (Diffusion):   0.021529\n",
            "Loss (Reconstruction x20): 0.092581\n",
            "---------\n",
            "Epoch: 2584/3000\n",
            "Total loss:         0.084413\n",
            "Loss (Diffusion):   0.020147\n",
            "Loss (Reconstruction x20): 0.064266\n",
            "---------\n",
            "Epoch: 2585/3000\n",
            "Total loss:         0.089067\n",
            "Loss (Diffusion):   0.023869\n",
            "Loss (Reconstruction x20): 0.065198\n",
            "---------\n",
            "Epoch: 2586/3000\n",
            "Total loss:         0.090337\n",
            "Loss (Diffusion):   0.021512\n",
            "Loss (Reconstruction x20): 0.068825\n",
            "---------\n",
            "Epoch: 2587/3000\n",
            "Total loss:         0.091648\n",
            "Loss (Diffusion):   0.017694\n",
            "Loss (Reconstruction x20): 0.073953\n",
            "---------\n",
            "Epoch: 2588/3000\n",
            "Total loss:         0.100170\n",
            "Loss (Diffusion):   0.019751\n",
            "Loss (Reconstruction x20): 0.080419\n",
            "---------\n",
            "Epoch: 2589/3000\n",
            "Total loss:         0.088986\n",
            "Loss (Diffusion):   0.018067\n",
            "Loss (Reconstruction x20): 0.070919\n",
            "---------\n",
            "Epoch: 2590/3000\n",
            "Total loss:         0.082278\n",
            "Loss (Diffusion):   0.015740\n",
            "Loss (Reconstruction x20): 0.066538\n",
            "---------\n",
            "Epoch: 2591/3000\n",
            "Total loss:         0.081861\n",
            "Loss (Diffusion):   0.018028\n",
            "Loss (Reconstruction x20): 0.063833\n",
            "---------\n",
            "Epoch: 2592/3000\n",
            "Total loss:         0.078005\n",
            "Loss (Diffusion):   0.017584\n",
            "Loss (Reconstruction x20): 0.060422\n",
            "---------\n",
            "Epoch: 2593/3000\n",
            "Total loss:         0.088001\n",
            "Loss (Diffusion):   0.020057\n",
            "Loss (Reconstruction x20): 0.067944\n",
            "---------\n",
            "Epoch: 2594/3000\n",
            "Total loss:         0.100064\n",
            "Loss (Diffusion):   0.016237\n",
            "Loss (Reconstruction x20): 0.083827\n",
            "---------\n",
            "Epoch: 2595/3000\n",
            "Total loss:         0.092339\n",
            "Loss (Diffusion):   0.019121\n",
            "Loss (Reconstruction x20): 0.073219\n",
            "---------\n",
            "Epoch: 2596/3000\n",
            "Total loss:         0.093003\n",
            "Loss (Diffusion):   0.019779\n",
            "Loss (Reconstruction x20): 0.073223\n",
            "---------\n",
            "Epoch: 2597/3000\n",
            "Total loss:         0.100555\n",
            "Loss (Diffusion):   0.015750\n",
            "Loss (Reconstruction x20): 0.084805\n",
            "---------\n",
            "Epoch: 2598/3000\n",
            "Total loss:         0.079018\n",
            "Loss (Diffusion):   0.016505\n",
            "Loss (Reconstruction x20): 0.062513\n",
            "---------\n",
            "Epoch: 2599/3000\n",
            "Total loss:         0.096287\n",
            "Loss (Diffusion):   0.018919\n",
            "Loss (Reconstruction x20): 0.077368\n",
            "---------\n",
            "Epoch: 2600/3000\n",
            "Total loss:         0.089292\n",
            "Loss (Diffusion):   0.019984\n",
            "Loss (Reconstruction x20): 0.069308\n",
            "---------\n",
            "Epoch: 2601/3000\n",
            "Total loss:         0.083102\n",
            "Loss (Diffusion):   0.015783\n",
            "Loss (Reconstruction x20): 0.067319\n",
            "---------\n",
            "Epoch: 2602/3000\n",
            "Total loss:         0.080777\n",
            "Loss (Diffusion):   0.017685\n",
            "Loss (Reconstruction x20): 0.063093\n",
            "---------\n",
            "Epoch: 2603/3000\n",
            "Total loss:         0.094891\n",
            "Loss (Diffusion):   0.019177\n",
            "Loss (Reconstruction x20): 0.075714\n",
            "---------\n",
            "Epoch: 2604/3000\n",
            "Total loss:         0.101600\n",
            "Loss (Diffusion):   0.015977\n",
            "Loss (Reconstruction x20): 0.085623\n",
            "---------\n",
            "Epoch: 2605/3000\n",
            "Total loss:         0.078682\n",
            "Loss (Diffusion):   0.016171\n",
            "Loss (Reconstruction x20): 0.062512\n",
            "---------\n",
            "Epoch: 2606/3000\n",
            "Total loss:         0.083271\n",
            "Loss (Diffusion):   0.022981\n",
            "Loss (Reconstruction x20): 0.060290\n",
            "---------\n",
            "Epoch: 2607/3000\n",
            "Total loss:         0.087620\n",
            "Loss (Diffusion):   0.019566\n",
            "Loss (Reconstruction x20): 0.068053\n",
            "---------\n",
            "Epoch: 2608/3000\n",
            "Total loss:         0.085044\n",
            "Loss (Diffusion):   0.015711\n",
            "Loss (Reconstruction x20): 0.069333\n",
            "---------\n",
            "Epoch: 2609/3000\n",
            "Total loss:         0.096586\n",
            "Loss (Diffusion):   0.018257\n",
            "Loss (Reconstruction x20): 0.078329\n",
            "---------\n",
            "Epoch: 2610/3000\n",
            "Total loss:         0.079496\n",
            "Loss (Diffusion):   0.018531\n",
            "Loss (Reconstruction x20): 0.060965\n",
            "---------\n",
            "Epoch: 2611/3000\n",
            "Total loss:         0.088156\n",
            "Loss (Diffusion):   0.016273\n",
            "Loss (Reconstruction x20): 0.071883\n",
            "---------\n",
            "Epoch: 2612/3000\n",
            "Total loss:         0.081772\n",
            "Loss (Diffusion):   0.017109\n",
            "Loss (Reconstruction x20): 0.064662\n",
            "---------\n",
            "Epoch: 2613/3000\n",
            "Total loss:         0.093829\n",
            "Loss (Diffusion):   0.024599\n",
            "Loss (Reconstruction x20): 0.069230\n",
            "---------\n",
            "Epoch: 2614/3000\n",
            "Total loss:         0.079196\n",
            "Loss (Diffusion):   0.019961\n",
            "Loss (Reconstruction x20): 0.059235\n",
            "---------\n",
            "Epoch: 2615/3000\n",
            "Total loss:         0.083626\n",
            "Loss (Diffusion):   0.019447\n",
            "Loss (Reconstruction x20): 0.064179\n",
            "---------\n",
            "Epoch: 2616/3000\n",
            "Total loss:         0.077610\n",
            "Loss (Diffusion):   0.020580\n",
            "Loss (Reconstruction x20): 0.057030\n",
            "---------\n",
            "Epoch: 2617/3000\n",
            "Total loss:         0.103948\n",
            "Loss (Diffusion):   0.021273\n",
            "Loss (Reconstruction x20): 0.082675\n",
            "---------\n",
            "Epoch: 2618/3000\n",
            "Total loss:         0.088421\n",
            "Loss (Diffusion):   0.016754\n",
            "Loss (Reconstruction x20): 0.071666\n",
            "---------\n",
            "Epoch: 2619/3000\n",
            "Total loss:         0.086118\n",
            "Loss (Diffusion):   0.016338\n",
            "Loss (Reconstruction x20): 0.069780\n",
            "---------\n",
            "Epoch: 2620/3000\n",
            "Total loss:         0.088019\n",
            "Loss (Diffusion):   0.012610\n",
            "Loss (Reconstruction x20): 0.075409\n",
            "---------\n",
            "Epoch: 2621/3000\n",
            "Total loss:         0.088744\n",
            "Loss (Diffusion):   0.017528\n",
            "Loss (Reconstruction x20): 0.071216\n",
            "---------\n",
            "Epoch: 2622/3000\n",
            "Total loss:         0.081512\n",
            "Loss (Diffusion):   0.021009\n",
            "Loss (Reconstruction x20): 0.060504\n",
            "---------\n",
            "Epoch: 2623/3000\n",
            "Total loss:         0.083808\n",
            "Loss (Diffusion):   0.016378\n",
            "Loss (Reconstruction x20): 0.067430\n",
            "---------\n",
            "Epoch: 2624/3000\n",
            "Total loss:         0.075216\n",
            "Loss (Diffusion):   0.017842\n",
            "Loss (Reconstruction x20): 0.057374\n",
            "---------\n",
            "Epoch: 2625/3000\n",
            "Total loss:         0.078310\n",
            "Loss (Diffusion):   0.022831\n",
            "Loss (Reconstruction x20): 0.055480\n",
            "---------\n",
            "Epoch: 2626/3000\n",
            "Total loss:         0.084551\n",
            "Loss (Diffusion):   0.023266\n",
            "Loss (Reconstruction x20): 0.061285\n",
            "---------\n",
            "Epoch: 2627/3000\n",
            "Total loss:         0.091094\n",
            "Loss (Diffusion):   0.017101\n",
            "Loss (Reconstruction x20): 0.073993\n",
            "---------\n",
            "Epoch: 2628/3000\n",
            "Total loss:         0.098739\n",
            "Loss (Diffusion):   0.018792\n",
            "Loss (Reconstruction x20): 0.079947\n",
            "---------\n",
            "Epoch: 2629/3000\n",
            "Total loss:         0.084447\n",
            "Loss (Diffusion):   0.019115\n",
            "Loss (Reconstruction x20): 0.065332\n",
            "---------\n",
            "Epoch: 2630/3000\n",
            "Total loss:         0.109681\n",
            "Loss (Diffusion):   0.015797\n",
            "Loss (Reconstruction x20): 0.093884\n",
            "---------\n",
            "Epoch: 2631/3000\n",
            "Total loss:         0.086170\n",
            "Loss (Diffusion):   0.021816\n",
            "Loss (Reconstruction x20): 0.064354\n",
            "---------\n",
            "Epoch: 2632/3000\n",
            "Total loss:         0.082000\n",
            "Loss (Diffusion):   0.017032\n",
            "Loss (Reconstruction x20): 0.064968\n",
            "---------\n",
            "Epoch: 2633/3000\n",
            "Total loss:         0.085872\n",
            "Loss (Diffusion):   0.020423\n",
            "Loss (Reconstruction x20): 0.065449\n",
            "---------\n",
            "Epoch: 2634/3000\n",
            "Total loss:         0.092479\n",
            "Loss (Diffusion):   0.016462\n",
            "Loss (Reconstruction x20): 0.076017\n",
            "---------\n",
            "Epoch: 2635/3000\n",
            "Total loss:         0.086507\n",
            "Loss (Diffusion):   0.018695\n",
            "Loss (Reconstruction x20): 0.067812\n",
            "---------\n",
            "Epoch: 2636/3000\n",
            "Total loss:         0.088488\n",
            "Loss (Diffusion):   0.021308\n",
            "Loss (Reconstruction x20): 0.067180\n",
            "---------\n",
            "Epoch: 2637/3000\n",
            "Total loss:         0.111128\n",
            "Loss (Diffusion):   0.019138\n",
            "Loss (Reconstruction x20): 0.091991\n",
            "---------\n",
            "Epoch: 2638/3000\n",
            "Total loss:         0.079921\n",
            "Loss (Diffusion):   0.018469\n",
            "Loss (Reconstruction x20): 0.061452\n",
            "---------\n",
            "Epoch: 2639/3000\n",
            "Total loss:         0.102772\n",
            "Loss (Diffusion):   0.021086\n",
            "Loss (Reconstruction x20): 0.081686\n",
            "---------\n",
            "Epoch: 2640/3000\n",
            "Total loss:         0.099665\n",
            "Loss (Diffusion):   0.019635\n",
            "Loss (Reconstruction x20): 0.080030\n",
            "---------\n",
            "Epoch: 2641/3000\n",
            "Total loss:         0.092807\n",
            "Loss (Diffusion):   0.017222\n",
            "Loss (Reconstruction x20): 0.075585\n",
            "---------\n",
            "Epoch: 2642/3000\n",
            "Total loss:         0.103879\n",
            "Loss (Diffusion):   0.023749\n",
            "Loss (Reconstruction x20): 0.080130\n",
            "---------\n",
            "Epoch: 2643/3000\n",
            "Total loss:         0.117183\n",
            "Loss (Diffusion):   0.022526\n",
            "Loss (Reconstruction x20): 0.094656\n",
            "---------\n",
            "Epoch: 2644/3000\n",
            "Total loss:         0.098910\n",
            "Loss (Diffusion):   0.019154\n",
            "Loss (Reconstruction x20): 0.079756\n",
            "---------\n",
            "Epoch: 2645/3000\n",
            "Total loss:         0.100640\n",
            "Loss (Diffusion):   0.021262\n",
            "Loss (Reconstruction x20): 0.079378\n",
            "---------\n",
            "Epoch: 2646/3000\n",
            "Total loss:         0.098116\n",
            "Loss (Diffusion):   0.019355\n",
            "Loss (Reconstruction x20): 0.078761\n",
            "---------\n",
            "Epoch: 2647/3000\n",
            "Total loss:         0.107022\n",
            "Loss (Diffusion):   0.020534\n",
            "Loss (Reconstruction x20): 0.086488\n",
            "---------\n",
            "Epoch: 2648/3000\n",
            "Total loss:         0.100571\n",
            "Loss (Diffusion):   0.015296\n",
            "Loss (Reconstruction x20): 0.085275\n",
            "---------\n",
            "Epoch: 2649/3000\n",
            "Total loss:         0.095927\n",
            "Loss (Diffusion):   0.020488\n",
            "Loss (Reconstruction x20): 0.075440\n",
            "---------\n",
            "Epoch: 2650/3000\n",
            "Total loss:         0.090231\n",
            "Loss (Diffusion):   0.023004\n",
            "Loss (Reconstruction x20): 0.067226\n",
            "---------\n",
            "Epoch: 2651/3000\n",
            "Total loss:         0.082163\n",
            "Loss (Diffusion):   0.023230\n",
            "Loss (Reconstruction x20): 0.058932\n",
            "---------\n",
            "Epoch: 2652/3000\n",
            "Total loss:         0.087667\n",
            "Loss (Diffusion):   0.017684\n",
            "Loss (Reconstruction x20): 0.069983\n",
            "---------\n",
            "Epoch: 2653/3000\n",
            "Total loss:         0.095298\n",
            "Loss (Diffusion):   0.020657\n",
            "Loss (Reconstruction x20): 0.074641\n",
            "---------\n",
            "Epoch: 2654/3000\n",
            "Total loss:         0.086429\n",
            "Loss (Diffusion):   0.017719\n",
            "Loss (Reconstruction x20): 0.068710\n",
            "---------\n",
            "Epoch: 2655/3000\n",
            "Total loss:         0.094728\n",
            "Loss (Diffusion):   0.019712\n",
            "Loss (Reconstruction x20): 0.075015\n",
            "---------\n",
            "Epoch: 2656/3000\n",
            "Total loss:         0.099235\n",
            "Loss (Diffusion):   0.019013\n",
            "Loss (Reconstruction x20): 0.080222\n",
            "---------\n",
            "Epoch: 2657/3000\n",
            "Total loss:         0.096350\n",
            "Loss (Diffusion):   0.023219\n",
            "Loss (Reconstruction x20): 0.073131\n",
            "---------\n",
            "Epoch: 2658/3000\n",
            "Total loss:         0.097840\n",
            "Loss (Diffusion):   0.021806\n",
            "Loss (Reconstruction x20): 0.076034\n",
            "---------\n",
            "Epoch: 2659/3000\n",
            "Total loss:         0.086106\n",
            "Loss (Diffusion):   0.017375\n",
            "Loss (Reconstruction x20): 0.068731\n",
            "---------\n",
            "Epoch: 2660/3000\n",
            "Total loss:         0.085676\n",
            "Loss (Diffusion):   0.017601\n",
            "Loss (Reconstruction x20): 0.068075\n",
            "---------\n",
            "Epoch: 2661/3000\n",
            "Total loss:         0.094984\n",
            "Loss (Diffusion):   0.020394\n",
            "Loss (Reconstruction x20): 0.074590\n",
            "---------\n",
            "Epoch: 2662/3000\n",
            "Total loss:         0.105266\n",
            "Loss (Diffusion):   0.020353\n",
            "Loss (Reconstruction x20): 0.084914\n",
            "---------\n",
            "Epoch: 2663/3000\n",
            "Total loss:         0.093656\n",
            "Loss (Diffusion):   0.017130\n",
            "Loss (Reconstruction x20): 0.076526\n",
            "---------\n",
            "Epoch: 2664/3000\n",
            "Total loss:         0.084073\n",
            "Loss (Diffusion):   0.018824\n",
            "Loss (Reconstruction x20): 0.065249\n",
            "---------\n",
            "Epoch: 2665/3000\n",
            "Total loss:         0.080635\n",
            "Loss (Diffusion):   0.017160\n",
            "Loss (Reconstruction x20): 0.063476\n",
            "---------\n",
            "Epoch: 2666/3000\n",
            "Total loss:         0.092511\n",
            "Loss (Diffusion):   0.020991\n",
            "Loss (Reconstruction x20): 0.071519\n",
            "---------\n",
            "Epoch: 2667/3000\n",
            "Total loss:         0.096412\n",
            "Loss (Diffusion):   0.014494\n",
            "Loss (Reconstruction x20): 0.081919\n",
            "---------\n",
            "Epoch: 2668/3000\n",
            "Total loss:         0.085871\n",
            "Loss (Diffusion):   0.019691\n",
            "Loss (Reconstruction x20): 0.066180\n",
            "---------\n",
            "Epoch: 2669/3000\n",
            "Total loss:         0.086899\n",
            "Loss (Diffusion):   0.021109\n",
            "Loss (Reconstruction x20): 0.065789\n",
            "---------\n",
            "Epoch: 2670/3000\n",
            "Total loss:         0.077482\n",
            "Loss (Diffusion):   0.015626\n",
            "Loss (Reconstruction x20): 0.061856\n",
            "---------\n",
            "Epoch: 2671/3000\n",
            "Total loss:         0.078326\n",
            "Loss (Diffusion):   0.018631\n",
            "Loss (Reconstruction x20): 0.059695\n",
            "---------\n",
            "Epoch: 2672/3000\n",
            "Total loss:         0.083672\n",
            "Loss (Diffusion):   0.017581\n",
            "Loss (Reconstruction x20): 0.066090\n",
            "---------\n",
            "Epoch: 2673/3000\n",
            "Total loss:         0.081259\n",
            "Loss (Diffusion):   0.016563\n",
            "Loss (Reconstruction x20): 0.064696\n",
            "---------\n",
            "Epoch: 2674/3000\n",
            "Total loss:         0.094094\n",
            "Loss (Diffusion):   0.016405\n",
            "Loss (Reconstruction x20): 0.077689\n",
            "---------\n",
            "Epoch: 2675/3000\n",
            "Total loss:         0.106003\n",
            "Loss (Diffusion):   0.021841\n",
            "Loss (Reconstruction x20): 0.084162\n",
            "---------\n",
            "Epoch: 2676/3000\n",
            "Total loss:         0.099270\n",
            "Loss (Diffusion):   0.019899\n",
            "Loss (Reconstruction x20): 0.079371\n",
            "---------\n",
            "Epoch: 2677/3000\n",
            "Total loss:         0.100058\n",
            "Loss (Diffusion):   0.021410\n",
            "Loss (Reconstruction x20): 0.078648\n",
            "---------\n",
            "Epoch: 2678/3000\n",
            "Total loss:         0.097776\n",
            "Loss (Diffusion):   0.017473\n",
            "Loss (Reconstruction x20): 0.080303\n",
            "---------\n",
            "Epoch: 2679/3000\n",
            "Total loss:         0.099413\n",
            "Loss (Diffusion):   0.021041\n",
            "Loss (Reconstruction x20): 0.078372\n",
            "---------\n",
            "Epoch: 2680/3000\n",
            "Total loss:         0.091810\n",
            "Loss (Diffusion):   0.017127\n",
            "Loss (Reconstruction x20): 0.074683\n",
            "---------\n",
            "Epoch: 2681/3000\n",
            "Total loss:         0.094289\n",
            "Loss (Diffusion):   0.014321\n",
            "Loss (Reconstruction x20): 0.079968\n",
            "---------\n",
            "Epoch: 2682/3000\n",
            "Total loss:         0.093159\n",
            "Loss (Diffusion):   0.022575\n",
            "Loss (Reconstruction x20): 0.070584\n",
            "---------\n",
            "Epoch: 2683/3000\n",
            "Total loss:         0.083437\n",
            "Loss (Diffusion):   0.016045\n",
            "Loss (Reconstruction x20): 0.067392\n",
            "---------\n",
            "Epoch: 2684/3000\n",
            "Total loss:         0.092530\n",
            "Loss (Diffusion):   0.015538\n",
            "Loss (Reconstruction x20): 0.076992\n",
            "---------\n",
            "Epoch: 2685/3000\n",
            "Total loss:         0.093198\n",
            "Loss (Diffusion):   0.020010\n",
            "Loss (Reconstruction x20): 0.073189\n",
            "---------\n",
            "Epoch: 2686/3000\n",
            "Total loss:         0.094444\n",
            "Loss (Diffusion):   0.013995\n",
            "Loss (Reconstruction x20): 0.080450\n",
            "---------\n",
            "Epoch: 2687/3000\n",
            "Total loss:         0.088383\n",
            "Loss (Diffusion):   0.018431\n",
            "Loss (Reconstruction x20): 0.069952\n",
            "---------\n",
            "Epoch: 2688/3000\n",
            "Total loss:         0.106997\n",
            "Loss (Diffusion):   0.020585\n",
            "Loss (Reconstruction x20): 0.086412\n",
            "---------\n",
            "Epoch: 2689/3000\n",
            "Total loss:         0.083287\n",
            "Loss (Diffusion):   0.019091\n",
            "Loss (Reconstruction x20): 0.064195\n",
            "---------\n",
            "Epoch: 2690/3000\n",
            "Total loss:         0.085747\n",
            "Loss (Diffusion):   0.018557\n",
            "Loss (Reconstruction x20): 0.067190\n",
            "---------\n",
            "Epoch: 2691/3000\n",
            "Total loss:         0.091032\n",
            "Loss (Diffusion):   0.019193\n",
            "Loss (Reconstruction x20): 0.071839\n",
            "---------\n",
            "Epoch: 2692/3000\n",
            "Total loss:         0.103753\n",
            "Loss (Diffusion):   0.023446\n",
            "Loss (Reconstruction x20): 0.080307\n",
            "---------\n",
            "Epoch: 2693/3000\n",
            "Total loss:         0.076164\n",
            "Loss (Diffusion):   0.015752\n",
            "Loss (Reconstruction x20): 0.060412\n",
            "---------\n",
            "Epoch: 2694/3000\n",
            "Total loss:         0.094045\n",
            "Loss (Diffusion):   0.019522\n",
            "Loss (Reconstruction x20): 0.074523\n",
            "---------\n",
            "Epoch: 2695/3000\n",
            "Total loss:         0.098212\n",
            "Loss (Diffusion):   0.019709\n",
            "Loss (Reconstruction x20): 0.078503\n",
            "---------\n",
            "Epoch: 2696/3000\n",
            "Total loss:         0.087733\n",
            "Loss (Diffusion):   0.022794\n",
            "Loss (Reconstruction x20): 0.064940\n",
            "---------\n",
            "Epoch: 2697/3000\n",
            "Total loss:         0.107498\n",
            "Loss (Diffusion):   0.017893\n",
            "Loss (Reconstruction x20): 0.089605\n",
            "---------\n",
            "Epoch: 2698/3000\n",
            "Total loss:         0.077183\n",
            "Loss (Diffusion):   0.016087\n",
            "Loss (Reconstruction x20): 0.061096\n",
            "---------\n",
            "Epoch: 2699/3000\n",
            "Total loss:         0.079589\n",
            "Loss (Diffusion):   0.019860\n",
            "Loss (Reconstruction x20): 0.059730\n",
            "---------\n",
            "Epoch: 2700/3000\n",
            "Total loss:         0.080396\n",
            "Loss (Diffusion):   0.023837\n",
            "Loss (Reconstruction x20): 0.056559\n",
            "---------\n",
            "Epoch: 2701/3000\n",
            "Total loss:         0.078164\n",
            "Loss (Diffusion):   0.016416\n",
            "Loss (Reconstruction x20): 0.061748\n",
            "---------\n",
            "Epoch: 2702/3000\n",
            "Total loss:         0.074176\n",
            "Loss (Diffusion):   0.016448\n",
            "Loss (Reconstruction x20): 0.057728\n",
            "---------\n",
            "Epoch: 2703/3000\n",
            "Total loss:         0.076749\n",
            "Loss (Diffusion):   0.012832\n",
            "Loss (Reconstruction x20): 0.063918\n",
            "---------\n",
            "Epoch: 2704/3000\n",
            "Total loss:         0.093775\n",
            "Loss (Diffusion):   0.023269\n",
            "Loss (Reconstruction x20): 0.070506\n",
            "---------\n",
            "Epoch: 2705/3000\n",
            "Total loss:         0.084819\n",
            "Loss (Diffusion):   0.019540\n",
            "Loss (Reconstruction x20): 0.065278\n",
            "---------\n",
            "Epoch: 2706/3000\n",
            "Total loss:         0.085029\n",
            "Loss (Diffusion):   0.016462\n",
            "Loss (Reconstruction x20): 0.068567\n",
            "---------\n",
            "Epoch: 2707/3000\n",
            "Total loss:         0.092732\n",
            "Loss (Diffusion):   0.021277\n",
            "Loss (Reconstruction x20): 0.071455\n",
            "---------\n",
            "Epoch: 2708/3000\n",
            "Total loss:         0.088571\n",
            "Loss (Diffusion):   0.015327\n",
            "Loss (Reconstruction x20): 0.073244\n",
            "---------\n",
            "Epoch: 2709/3000\n",
            "Total loss:         0.075837\n",
            "Loss (Diffusion):   0.016334\n",
            "Loss (Reconstruction x20): 0.059503\n",
            "---------\n",
            "Epoch: 2710/3000\n",
            "Total loss:         0.094518\n",
            "Loss (Diffusion):   0.019293\n",
            "Loss (Reconstruction x20): 0.075225\n",
            "---------\n",
            "Epoch: 2711/3000\n",
            "Total loss:         0.085286\n",
            "Loss (Diffusion):   0.020896\n",
            "Loss (Reconstruction x20): 0.064390\n",
            "---------\n",
            "Epoch: 2712/3000\n",
            "Total loss:         0.090545\n",
            "Loss (Diffusion):   0.017573\n",
            "Loss (Reconstruction x20): 0.072972\n",
            "---------\n",
            "Epoch: 2713/3000\n",
            "Total loss:         0.091995\n",
            "Loss (Diffusion):   0.015121\n",
            "Loss (Reconstruction x20): 0.076874\n",
            "---------\n",
            "Epoch: 2714/3000\n",
            "Total loss:         0.076411\n",
            "Loss (Diffusion):   0.015409\n",
            "Loss (Reconstruction x20): 0.061002\n",
            "---------\n",
            "Epoch: 2715/3000\n",
            "Total loss:         0.105430\n",
            "Loss (Diffusion):   0.017626\n",
            "Loss (Reconstruction x20): 0.087804\n",
            "---------\n",
            "Epoch: 2716/3000\n",
            "Total loss:         0.093329\n",
            "Loss (Diffusion):   0.022727\n",
            "Loss (Reconstruction x20): 0.070602\n",
            "---------\n",
            "Epoch: 2717/3000\n",
            "Total loss:         0.088781\n",
            "Loss (Diffusion):   0.019433\n",
            "Loss (Reconstruction x20): 0.069348\n",
            "---------\n",
            "Epoch: 2718/3000\n",
            "Total loss:         0.076509\n",
            "Loss (Diffusion):   0.017260\n",
            "Loss (Reconstruction x20): 0.059249\n",
            "---------\n",
            "Epoch: 2719/3000\n",
            "Total loss:         0.089631\n",
            "Loss (Diffusion):   0.016930\n",
            "Loss (Reconstruction x20): 0.072702\n",
            "---------\n",
            "Epoch: 2720/3000\n",
            "Total loss:         0.092881\n",
            "Loss (Diffusion):   0.024640\n",
            "Loss (Reconstruction x20): 0.068242\n",
            "---------\n",
            "Epoch: 2721/3000\n",
            "Total loss:         0.106440\n",
            "Loss (Diffusion):   0.020496\n",
            "Loss (Reconstruction x20): 0.085944\n",
            "---------\n",
            "Epoch: 2722/3000\n",
            "Total loss:         0.082256\n",
            "Loss (Diffusion):   0.017005\n",
            "Loss (Reconstruction x20): 0.065252\n",
            "---------\n",
            "Epoch: 2723/3000\n",
            "Total loss:         0.093661\n",
            "Loss (Diffusion):   0.016844\n",
            "Loss (Reconstruction x20): 0.076818\n",
            "---------\n",
            "Epoch: 2724/3000\n",
            "Total loss:         0.089508\n",
            "Loss (Diffusion):   0.021645\n",
            "Loss (Reconstruction x20): 0.067863\n",
            "---------\n",
            "Epoch: 2725/3000\n",
            "Total loss:         0.087794\n",
            "Loss (Diffusion):   0.015042\n",
            "Loss (Reconstruction x20): 0.072752\n",
            "---------\n",
            "Epoch: 2726/3000\n",
            "Total loss:         0.087403\n",
            "Loss (Diffusion):   0.016352\n",
            "Loss (Reconstruction x20): 0.071051\n",
            "---------\n",
            "Epoch: 2727/3000\n",
            "Total loss:         0.086915\n",
            "Loss (Diffusion):   0.016668\n",
            "Loss (Reconstruction x20): 0.070246\n",
            "---------\n",
            "Epoch: 2728/3000\n",
            "Total loss:         0.104145\n",
            "Loss (Diffusion):   0.017744\n",
            "Loss (Reconstruction x20): 0.086401\n",
            "---------\n",
            "Epoch: 2729/3000\n",
            "Total loss:         0.089008\n",
            "Loss (Diffusion):   0.014597\n",
            "Loss (Reconstruction x20): 0.074411\n",
            "---------\n",
            "Epoch: 2730/3000\n",
            "Total loss:         0.089259\n",
            "Loss (Diffusion):   0.017187\n",
            "Loss (Reconstruction x20): 0.072073\n",
            "---------\n",
            "Epoch: 2731/3000\n",
            "Total loss:         0.091966\n",
            "Loss (Diffusion):   0.019308\n",
            "Loss (Reconstruction x20): 0.072658\n",
            "---------\n",
            "Epoch: 2732/3000\n",
            "Total loss:         0.092508\n",
            "Loss (Diffusion):   0.015719\n",
            "Loss (Reconstruction x20): 0.076789\n",
            "---------\n",
            "Epoch: 2733/3000\n",
            "Total loss:         0.081016\n",
            "Loss (Diffusion):   0.017527\n",
            "Loss (Reconstruction x20): 0.063489\n",
            "---------\n",
            "Epoch: 2734/3000\n",
            "Total loss:         0.083837\n",
            "Loss (Diffusion):   0.016484\n",
            "Loss (Reconstruction x20): 0.067353\n",
            "---------\n",
            "Epoch: 2735/3000\n",
            "Total loss:         0.093030\n",
            "Loss (Diffusion):   0.021083\n",
            "Loss (Reconstruction x20): 0.071947\n",
            "---------\n",
            "Epoch: 2736/3000\n",
            "Total loss:         0.083194\n",
            "Loss (Diffusion):   0.019762\n",
            "Loss (Reconstruction x20): 0.063432\n",
            "---------\n",
            "Epoch: 2737/3000\n",
            "Total loss:         0.083066\n",
            "Loss (Diffusion):   0.014643\n",
            "Loss (Reconstruction x20): 0.068423\n",
            "---------\n",
            "Epoch: 2738/3000\n",
            "Total loss:         0.077461\n",
            "Loss (Diffusion):   0.015594\n",
            "Loss (Reconstruction x20): 0.061866\n",
            "---------\n",
            "Epoch: 2739/3000\n",
            "Total loss:         0.085255\n",
            "Loss (Diffusion):   0.018494\n",
            "Loss (Reconstruction x20): 0.066761\n",
            "---------\n",
            "Epoch: 2740/3000\n",
            "Total loss:         0.070745\n",
            "Loss (Diffusion):   0.017517\n",
            "Loss (Reconstruction x20): 0.053228\n",
            "---------\n",
            "Epoch: 2741/3000\n",
            "Total loss:         0.096906\n",
            "Loss (Diffusion):   0.019043\n",
            "Loss (Reconstruction x20): 0.077863\n",
            "---------\n",
            "Epoch: 2742/3000\n",
            "Total loss:         0.074600\n",
            "Loss (Diffusion):   0.017895\n",
            "Loss (Reconstruction x20): 0.056705\n",
            "---------\n",
            "Epoch: 2743/3000\n",
            "Total loss:         0.093085\n",
            "Loss (Diffusion):   0.014749\n",
            "Loss (Reconstruction x20): 0.078337\n",
            "---------\n",
            "Epoch: 2744/3000\n",
            "Total loss:         0.084169\n",
            "Loss (Diffusion):   0.018676\n",
            "Loss (Reconstruction x20): 0.065492\n",
            "---------\n",
            "Epoch: 2745/3000\n",
            "Total loss:         0.089582\n",
            "Loss (Diffusion):   0.014954\n",
            "Loss (Reconstruction x20): 0.074628\n",
            "---------\n",
            "Epoch: 2746/3000\n",
            "Total loss:         0.082127\n",
            "Loss (Diffusion):   0.017024\n",
            "Loss (Reconstruction x20): 0.065103\n",
            "---------\n",
            "Epoch: 2747/3000\n",
            "Total loss:         0.074525\n",
            "Loss (Diffusion):   0.018108\n",
            "Loss (Reconstruction x20): 0.056417\n",
            "---------\n",
            "Epoch: 2748/3000\n",
            "Total loss:         0.079898\n",
            "Loss (Diffusion):   0.017149\n",
            "Loss (Reconstruction x20): 0.062749\n",
            "---------\n",
            "Epoch: 2749/3000\n",
            "Total loss:         0.087626\n",
            "Loss (Diffusion):   0.016005\n",
            "Loss (Reconstruction x20): 0.071621\n",
            "---------\n",
            "Epoch: 2750/3000\n",
            "Total loss:         0.079845\n",
            "Loss (Diffusion):   0.020042\n",
            "Loss (Reconstruction x20): 0.059802\n",
            "---------\n",
            "Epoch: 2751/3000\n",
            "Total loss:         0.087835\n",
            "Loss (Diffusion):   0.016353\n",
            "Loss (Reconstruction x20): 0.071482\n",
            "---------\n",
            "Epoch: 2752/3000\n",
            "Total loss:         0.085061\n",
            "Loss (Diffusion):   0.018048\n",
            "Loss (Reconstruction x20): 0.067013\n",
            "---------\n",
            "Epoch: 2753/3000\n",
            "Total loss:         0.076612\n",
            "Loss (Diffusion):   0.016350\n",
            "Loss (Reconstruction x20): 0.060262\n",
            "---------\n",
            "Epoch: 2754/3000\n",
            "Total loss:         0.083359\n",
            "Loss (Diffusion):   0.016864\n",
            "Loss (Reconstruction x20): 0.066495\n",
            "---------\n",
            "Epoch: 2755/3000\n",
            "Total loss:         0.071614\n",
            "Loss (Diffusion):   0.015496\n",
            "Loss (Reconstruction x20): 0.056118\n",
            "---------\n",
            "Epoch: 2756/3000\n",
            "Total loss:         0.093691\n",
            "Loss (Diffusion):   0.017961\n",
            "Loss (Reconstruction x20): 0.075730\n",
            "---------\n",
            "Epoch: 2757/3000\n",
            "Total loss:         0.084447\n",
            "Loss (Diffusion):   0.019202\n",
            "Loss (Reconstruction x20): 0.065245\n",
            "---------\n",
            "Epoch: 2758/3000\n",
            "Total loss:         0.085521\n",
            "Loss (Diffusion):   0.015281\n",
            "Loss (Reconstruction x20): 0.070241\n",
            "---------\n",
            "Epoch: 2759/3000\n",
            "Total loss:         0.111208\n",
            "Loss (Diffusion):   0.021179\n",
            "Loss (Reconstruction x20): 0.090030\n",
            "---------\n",
            "Epoch: 2760/3000\n",
            "Total loss:         0.107798\n",
            "Loss (Diffusion):   0.016388\n",
            "Loss (Reconstruction x20): 0.091409\n",
            "---------\n",
            "Epoch: 2761/3000\n",
            "Total loss:         0.086506\n",
            "Loss (Diffusion):   0.020046\n",
            "Loss (Reconstruction x20): 0.066460\n",
            "---------\n",
            "Epoch: 2762/3000\n",
            "Total loss:         0.088003\n",
            "Loss (Diffusion):   0.019027\n",
            "Loss (Reconstruction x20): 0.068977\n",
            "---------\n",
            "Epoch: 2763/3000\n",
            "Total loss:         0.087753\n",
            "Loss (Diffusion):   0.016188\n",
            "Loss (Reconstruction x20): 0.071565\n",
            "---------\n",
            "Epoch: 2764/3000\n",
            "Total loss:         0.112407\n",
            "Loss (Diffusion):   0.020354\n",
            "Loss (Reconstruction x20): 0.092053\n",
            "---------\n",
            "Epoch: 2765/3000\n",
            "Total loss:         0.115117\n",
            "Loss (Diffusion):   0.017137\n",
            "Loss (Reconstruction x20): 0.097979\n",
            "---------\n",
            "Epoch: 2766/3000\n",
            "Total loss:         0.106206\n",
            "Loss (Diffusion):   0.016331\n",
            "Loss (Reconstruction x20): 0.089875\n",
            "---------\n",
            "Epoch: 2767/3000\n",
            "Total loss:         0.093559\n",
            "Loss (Diffusion):   0.021892\n",
            "Loss (Reconstruction x20): 0.071668\n",
            "---------\n",
            "Epoch: 2768/3000\n",
            "Total loss:         0.097425\n",
            "Loss (Diffusion):   0.020694\n",
            "Loss (Reconstruction x20): 0.076731\n",
            "---------\n",
            "Epoch: 2769/3000\n",
            "Total loss:         0.096296\n",
            "Loss (Diffusion):   0.027852\n",
            "Loss (Reconstruction x20): 0.068444\n",
            "---------\n",
            "Epoch: 2770/3000\n",
            "Total loss:         0.084258\n",
            "Loss (Diffusion):   0.015877\n",
            "Loss (Reconstruction x20): 0.068382\n",
            "---------\n",
            "Epoch: 2771/3000\n",
            "Total loss:         0.089656\n",
            "Loss (Diffusion):   0.017481\n",
            "Loss (Reconstruction x20): 0.072175\n",
            "---------\n",
            "Epoch: 2772/3000\n",
            "Total loss:         0.084397\n",
            "Loss (Diffusion):   0.021765\n",
            "Loss (Reconstruction x20): 0.062632\n",
            "---------\n",
            "Epoch: 2773/3000\n",
            "Total loss:         0.106088\n",
            "Loss (Diffusion):   0.020529\n",
            "Loss (Reconstruction x20): 0.085558\n",
            "---------\n",
            "Epoch: 2774/3000\n",
            "Total loss:         0.076661\n",
            "Loss (Diffusion):   0.019032\n",
            "Loss (Reconstruction x20): 0.057629\n",
            "---------\n",
            "Epoch: 2775/3000\n",
            "Total loss:         0.098356\n",
            "Loss (Diffusion):   0.018893\n",
            "Loss (Reconstruction x20): 0.079463\n",
            "---------\n",
            "Epoch: 2776/3000\n",
            "Total loss:         0.085536\n",
            "Loss (Diffusion):   0.017147\n",
            "Loss (Reconstruction x20): 0.068389\n",
            "---------\n",
            "Epoch: 2777/3000\n",
            "Total loss:         0.093803\n",
            "Loss (Diffusion):   0.020216\n",
            "Loss (Reconstruction x20): 0.073587\n",
            "---------\n",
            "Epoch: 2778/3000\n",
            "Total loss:         0.095308\n",
            "Loss (Diffusion):   0.022031\n",
            "Loss (Reconstruction x20): 0.073277\n",
            "---------\n",
            "Epoch: 2779/3000\n",
            "Total loss:         0.075771\n",
            "Loss (Diffusion):   0.017145\n",
            "Loss (Reconstruction x20): 0.058627\n",
            "---------\n",
            "Epoch: 2780/3000\n",
            "Total loss:         0.089372\n",
            "Loss (Diffusion):   0.015451\n",
            "Loss (Reconstruction x20): 0.073921\n",
            "---------\n",
            "Epoch: 2781/3000\n",
            "Total loss:         0.090684\n",
            "Loss (Diffusion):   0.016602\n",
            "Loss (Reconstruction x20): 0.074083\n",
            "---------\n",
            "Epoch: 2782/3000\n",
            "Total loss:         0.089498\n",
            "Loss (Diffusion):   0.015180\n",
            "Loss (Reconstruction x20): 0.074318\n",
            "---------\n",
            "Epoch: 2783/3000\n",
            "Total loss:         0.091390\n",
            "Loss (Diffusion):   0.016158\n",
            "Loss (Reconstruction x20): 0.075231\n",
            "---------\n",
            "Epoch: 2784/3000\n",
            "Total loss:         0.090138\n",
            "Loss (Diffusion):   0.023286\n",
            "Loss (Reconstruction x20): 0.066851\n",
            "---------\n",
            "Epoch: 2785/3000\n",
            "Total loss:         0.091769\n",
            "Loss (Diffusion):   0.021901\n",
            "Loss (Reconstruction x20): 0.069868\n",
            "---------\n",
            "Epoch: 2786/3000\n",
            "Total loss:         0.106739\n",
            "Loss (Diffusion):   0.017899\n",
            "Loss (Reconstruction x20): 0.088840\n",
            "---------\n",
            "Epoch: 2787/3000\n",
            "Total loss:         0.093557\n",
            "Loss (Diffusion):   0.021418\n",
            "Loss (Reconstruction x20): 0.072140\n",
            "---------\n",
            "Epoch: 2788/3000\n",
            "Total loss:         0.108233\n",
            "Loss (Diffusion):   0.016669\n",
            "Loss (Reconstruction x20): 0.091564\n",
            "---------\n",
            "Epoch: 2789/3000\n",
            "Total loss:         0.086932\n",
            "Loss (Diffusion):   0.017865\n",
            "Loss (Reconstruction x20): 0.069067\n",
            "---------\n",
            "Epoch: 2790/3000\n",
            "Total loss:         0.092723\n",
            "Loss (Diffusion):   0.016637\n",
            "Loss (Reconstruction x20): 0.076086\n",
            "---------\n",
            "Epoch: 2791/3000\n",
            "Total loss:         0.087319\n",
            "Loss (Diffusion):   0.018185\n",
            "Loss (Reconstruction x20): 0.069134\n",
            "---------\n",
            "Epoch: 2792/3000\n",
            "Total loss:         0.088847\n",
            "Loss (Diffusion):   0.022088\n",
            "Loss (Reconstruction x20): 0.066759\n",
            "---------\n",
            "Epoch: 2793/3000\n",
            "Total loss:         0.100413\n",
            "Loss (Diffusion):   0.018844\n",
            "Loss (Reconstruction x20): 0.081569\n",
            "---------\n",
            "Epoch: 2794/3000\n",
            "Total loss:         0.104156\n",
            "Loss (Diffusion):   0.017609\n",
            "Loss (Reconstruction x20): 0.086547\n",
            "---------\n",
            "Epoch: 2795/3000\n",
            "Total loss:         0.108786\n",
            "Loss (Diffusion):   0.017472\n",
            "Loss (Reconstruction x20): 0.091313\n",
            "---------\n",
            "Epoch: 2796/3000\n",
            "Total loss:         0.097967\n",
            "Loss (Diffusion):   0.016935\n",
            "Loss (Reconstruction x20): 0.081032\n",
            "---------\n",
            "Epoch: 2797/3000\n",
            "Total loss:         0.089135\n",
            "Loss (Diffusion):   0.020524\n",
            "Loss (Reconstruction x20): 0.068611\n",
            "---------\n",
            "Epoch: 2798/3000\n",
            "Total loss:         0.088197\n",
            "Loss (Diffusion):   0.018388\n",
            "Loss (Reconstruction x20): 0.069809\n",
            "---------\n",
            "Epoch: 2799/3000\n",
            "Total loss:         0.109077\n",
            "Loss (Diffusion):   0.020362\n",
            "Loss (Reconstruction x20): 0.088715\n",
            "---------\n",
            "Epoch: 2800/3000\n",
            "Total loss:         0.085504\n",
            "Loss (Diffusion):   0.015660\n",
            "Loss (Reconstruction x20): 0.069844\n",
            "---------\n",
            "Epoch: 2801/3000\n",
            "Total loss:         0.098411\n",
            "Loss (Diffusion):   0.019740\n",
            "Loss (Reconstruction x20): 0.078672\n",
            "---------\n",
            "Epoch: 2802/3000\n",
            "Total loss:         0.089272\n",
            "Loss (Diffusion):   0.018234\n",
            "Loss (Reconstruction x20): 0.071037\n",
            "---------\n",
            "Epoch: 2803/3000\n",
            "Total loss:         0.081577\n",
            "Loss (Diffusion):   0.017582\n",
            "Loss (Reconstruction x20): 0.063995\n",
            "---------\n",
            "Epoch: 2804/3000\n",
            "Total loss:         0.081132\n",
            "Loss (Diffusion):   0.016174\n",
            "Loss (Reconstruction x20): 0.064958\n",
            "---------\n",
            "Epoch: 2805/3000\n",
            "Total loss:         0.096818\n",
            "Loss (Diffusion):   0.015756\n",
            "Loss (Reconstruction x20): 0.081062\n",
            "---------\n",
            "Epoch: 2806/3000\n",
            "Total loss:         0.089546\n",
            "Loss (Diffusion):   0.017931\n",
            "Loss (Reconstruction x20): 0.071615\n",
            "---------\n",
            "Epoch: 2807/3000\n",
            "Total loss:         0.093545\n",
            "Loss (Diffusion):   0.023022\n",
            "Loss (Reconstruction x20): 0.070523\n",
            "---------\n",
            "Epoch: 2808/3000\n",
            "Total loss:         0.074600\n",
            "Loss (Diffusion):   0.016207\n",
            "Loss (Reconstruction x20): 0.058393\n",
            "---------\n",
            "Epoch: 2809/3000\n",
            "Total loss:         0.087647\n",
            "Loss (Diffusion):   0.016264\n",
            "Loss (Reconstruction x20): 0.071383\n",
            "---------\n",
            "Epoch: 2810/3000\n",
            "Total loss:         0.090010\n",
            "Loss (Diffusion):   0.016351\n",
            "Loss (Reconstruction x20): 0.073659\n",
            "---------\n",
            "Epoch: 2811/3000\n",
            "Total loss:         0.094296\n",
            "Loss (Diffusion):   0.020000\n",
            "Loss (Reconstruction x20): 0.074296\n",
            "---------\n",
            "Epoch: 2812/3000\n",
            "Total loss:         0.084674\n",
            "Loss (Diffusion):   0.017265\n",
            "Loss (Reconstruction x20): 0.067409\n",
            "---------\n",
            "Epoch: 2813/3000\n",
            "Total loss:         0.075710\n",
            "Loss (Diffusion):   0.016932\n",
            "Loss (Reconstruction x20): 0.058778\n",
            "---------\n",
            "Epoch: 2814/3000\n",
            "Total loss:         0.077300\n",
            "Loss (Diffusion):   0.015422\n",
            "Loss (Reconstruction x20): 0.061877\n",
            "---------\n",
            "Epoch: 2815/3000\n",
            "Total loss:         0.085874\n",
            "Loss (Diffusion):   0.016314\n",
            "Loss (Reconstruction x20): 0.069560\n",
            "---------\n",
            "Epoch: 2816/3000\n",
            "Total loss:         0.093015\n",
            "Loss (Diffusion):   0.020245\n",
            "Loss (Reconstruction x20): 0.072770\n",
            "---------\n",
            "Epoch: 2817/3000\n",
            "Total loss:         0.098163\n",
            "Loss (Diffusion):   0.015220\n",
            "Loss (Reconstruction x20): 0.082942\n",
            "---------\n",
            "Epoch: 2818/3000\n",
            "Total loss:         0.084157\n",
            "Loss (Diffusion):   0.021329\n",
            "Loss (Reconstruction x20): 0.062828\n",
            "---------\n",
            "Epoch: 2819/3000\n",
            "Total loss:         0.093663\n",
            "Loss (Diffusion):   0.017274\n",
            "Loss (Reconstruction x20): 0.076389\n",
            "---------\n",
            "Epoch: 2820/3000\n",
            "Total loss:         0.093607\n",
            "Loss (Diffusion):   0.016891\n",
            "Loss (Reconstruction x20): 0.076716\n",
            "---------\n",
            "Epoch: 2821/3000\n",
            "Total loss:         0.093129\n",
            "Loss (Diffusion):   0.013521\n",
            "Loss (Reconstruction x20): 0.079608\n",
            "---------\n",
            "Epoch: 2822/3000\n",
            "Total loss:         0.095785\n",
            "Loss (Diffusion):   0.014821\n",
            "Loss (Reconstruction x20): 0.080964\n",
            "---------\n",
            "Epoch: 2823/3000\n",
            "Total loss:         0.083534\n",
            "Loss (Diffusion):   0.021077\n",
            "Loss (Reconstruction x20): 0.062458\n",
            "---------\n",
            "Epoch: 2824/3000\n",
            "Total loss:         0.098529\n",
            "Loss (Diffusion):   0.022653\n",
            "Loss (Reconstruction x20): 0.075876\n",
            "---------\n",
            "Epoch: 2825/3000\n",
            "Total loss:         0.071772\n",
            "Loss (Diffusion):   0.015671\n",
            "Loss (Reconstruction x20): 0.056101\n",
            "---------\n",
            "Epoch: 2826/3000\n",
            "Total loss:         0.085322\n",
            "Loss (Diffusion):   0.017404\n",
            "Loss (Reconstruction x20): 0.067918\n",
            "---------\n",
            "Epoch: 2827/3000\n",
            "Total loss:         0.086081\n",
            "Loss (Diffusion):   0.019857\n",
            "Loss (Reconstruction x20): 0.066225\n",
            "---------\n",
            "Epoch: 2828/3000\n",
            "Total loss:         0.083101\n",
            "Loss (Diffusion):   0.015000\n",
            "Loss (Reconstruction x20): 0.068101\n",
            "---------\n",
            "Epoch: 2829/3000\n",
            "Total loss:         0.087479\n",
            "Loss (Diffusion):   0.020128\n",
            "Loss (Reconstruction x20): 0.067351\n",
            "---------\n",
            "Epoch: 2830/3000\n",
            "Total loss:         0.079148\n",
            "Loss (Diffusion):   0.014411\n",
            "Loss (Reconstruction x20): 0.064737\n",
            "---------\n",
            "Epoch: 2831/3000\n",
            "Total loss:         0.086525\n",
            "Loss (Diffusion):   0.016666\n",
            "Loss (Reconstruction x20): 0.069859\n",
            "---------\n",
            "Epoch: 2832/3000\n",
            "Total loss:         0.088015\n",
            "Loss (Diffusion):   0.014210\n",
            "Loss (Reconstruction x20): 0.073806\n",
            "---------\n",
            "Epoch: 2833/3000\n",
            "Total loss:         0.085693\n",
            "Loss (Diffusion):   0.013790\n",
            "Loss (Reconstruction x20): 0.071904\n",
            "---------\n",
            "Epoch: 2834/3000\n",
            "Total loss:         0.073076\n",
            "Loss (Diffusion):   0.014394\n",
            "Loss (Reconstruction x20): 0.058682\n",
            "---------\n",
            "Epoch: 2835/3000\n",
            "Total loss:         0.086932\n",
            "Loss (Diffusion):   0.015879\n",
            "Loss (Reconstruction x20): 0.071053\n",
            "---------\n",
            "Epoch: 2836/3000\n",
            "Total loss:         0.087483\n",
            "Loss (Diffusion):   0.019075\n",
            "Loss (Reconstruction x20): 0.068408\n",
            "---------\n",
            "Epoch: 2837/3000\n",
            "Total loss:         0.082465\n",
            "Loss (Diffusion):   0.017782\n",
            "Loss (Reconstruction x20): 0.064684\n",
            "---------\n",
            "Epoch: 2838/3000\n",
            "Total loss:         0.070787\n",
            "Loss (Diffusion):   0.014250\n",
            "Loss (Reconstruction x20): 0.056537\n",
            "---------\n",
            "Epoch: 2839/3000\n",
            "Total loss:         0.085061\n",
            "Loss (Diffusion):   0.017644\n",
            "Loss (Reconstruction x20): 0.067417\n",
            "---------\n",
            "Epoch: 2840/3000\n",
            "Total loss:         0.088111\n",
            "Loss (Diffusion):   0.019274\n",
            "Loss (Reconstruction x20): 0.068837\n",
            "---------\n",
            "Epoch: 2841/3000\n",
            "Total loss:         0.083687\n",
            "Loss (Diffusion):   0.012218\n",
            "Loss (Reconstruction x20): 0.071469\n",
            "---------\n",
            "Epoch: 2842/3000\n",
            "Total loss:         0.084574\n",
            "Loss (Diffusion):   0.015916\n",
            "Loss (Reconstruction x20): 0.068658\n",
            "---------\n",
            "Epoch: 2843/3000\n",
            "Total loss:         0.083429\n",
            "Loss (Diffusion):   0.015422\n",
            "Loss (Reconstruction x20): 0.068007\n",
            "---------\n",
            "Epoch: 2844/3000\n",
            "Total loss:         0.094429\n",
            "Loss (Diffusion):   0.019324\n",
            "Loss (Reconstruction x20): 0.075104\n",
            "---------\n",
            "Epoch: 2845/3000\n",
            "Total loss:         0.098625\n",
            "Loss (Diffusion):   0.022026\n",
            "Loss (Reconstruction x20): 0.076600\n",
            "---------\n",
            "Epoch: 2846/3000\n",
            "Total loss:         0.097703\n",
            "Loss (Diffusion):   0.020098\n",
            "Loss (Reconstruction x20): 0.077604\n",
            "---------\n",
            "Epoch: 2847/3000\n",
            "Total loss:         0.085274\n",
            "Loss (Diffusion):   0.021891\n",
            "Loss (Reconstruction x20): 0.063383\n",
            "---------\n",
            "Epoch: 2848/3000\n",
            "Total loss:         0.106690\n",
            "Loss (Diffusion):   0.015833\n",
            "Loss (Reconstruction x20): 0.090857\n",
            "---------\n",
            "Epoch: 2849/3000\n",
            "Total loss:         0.101287\n",
            "Loss (Diffusion):   0.018315\n",
            "Loss (Reconstruction x20): 0.082972\n",
            "---------\n",
            "Epoch: 2850/3000\n",
            "Total loss:         0.111768\n",
            "Loss (Diffusion):   0.018477\n",
            "Loss (Reconstruction x20): 0.093291\n",
            "---------\n",
            "Epoch: 2851/3000\n",
            "Total loss:         0.107543\n",
            "Loss (Diffusion):   0.026980\n",
            "Loss (Reconstruction x20): 0.080563\n",
            "---------\n",
            "Epoch: 2852/3000\n",
            "Total loss:         0.104147\n",
            "Loss (Diffusion):   0.022733\n",
            "Loss (Reconstruction x20): 0.081414\n",
            "---------\n",
            "Epoch: 2853/3000\n",
            "Total loss:         0.097582\n",
            "Loss (Diffusion):   0.020277\n",
            "Loss (Reconstruction x20): 0.077306\n",
            "---------\n",
            "Epoch: 2854/3000\n",
            "Total loss:         0.091579\n",
            "Loss (Diffusion):   0.021741\n",
            "Loss (Reconstruction x20): 0.069838\n",
            "---------\n",
            "Epoch: 2855/3000\n",
            "Total loss:         0.082601\n",
            "Loss (Diffusion):   0.020747\n",
            "Loss (Reconstruction x20): 0.061854\n",
            "---------\n",
            "Epoch: 2856/3000\n",
            "Total loss:         0.116283\n",
            "Loss (Diffusion):   0.024438\n",
            "Loss (Reconstruction x20): 0.091845\n",
            "---------\n",
            "Epoch: 2857/3000\n",
            "Total loss:         0.099632\n",
            "Loss (Diffusion):   0.018529\n",
            "Loss (Reconstruction x20): 0.081103\n",
            "---------\n",
            "Epoch: 2858/3000\n",
            "Total loss:         0.091318\n",
            "Loss (Diffusion):   0.020070\n",
            "Loss (Reconstruction x20): 0.071248\n",
            "---------\n",
            "Epoch: 2859/3000\n",
            "Total loss:         0.103200\n",
            "Loss (Diffusion):   0.020980\n",
            "Loss (Reconstruction x20): 0.082220\n",
            "---------\n",
            "Epoch: 2860/3000\n",
            "Total loss:         0.094936\n",
            "Loss (Diffusion):   0.017934\n",
            "Loss (Reconstruction x20): 0.077002\n",
            "---------\n",
            "Epoch: 2861/3000\n",
            "Total loss:         0.091593\n",
            "Loss (Diffusion):   0.022661\n",
            "Loss (Reconstruction x20): 0.068933\n",
            "---------\n",
            "Epoch: 2862/3000\n",
            "Total loss:         0.085547\n",
            "Loss (Diffusion):   0.019572\n",
            "Loss (Reconstruction x20): 0.065975\n",
            "---------\n",
            "Epoch: 2863/3000\n",
            "Total loss:         0.088495\n",
            "Loss (Diffusion):   0.013488\n",
            "Loss (Reconstruction x20): 0.075007\n",
            "---------\n",
            "Epoch: 2864/3000\n",
            "Total loss:         0.107842\n",
            "Loss (Diffusion):   0.019773\n",
            "Loss (Reconstruction x20): 0.088069\n",
            "---------\n",
            "Epoch: 2865/3000\n",
            "Total loss:         0.089303\n",
            "Loss (Diffusion):   0.025291\n",
            "Loss (Reconstruction x20): 0.064012\n",
            "---------\n",
            "Epoch: 2866/3000\n",
            "Total loss:         0.083874\n",
            "Loss (Diffusion):   0.016326\n",
            "Loss (Reconstruction x20): 0.067548\n",
            "---------\n",
            "Epoch: 2867/3000\n",
            "Total loss:         0.090784\n",
            "Loss (Diffusion):   0.017311\n",
            "Loss (Reconstruction x20): 0.073472\n",
            "---------\n",
            "Epoch: 2868/3000\n",
            "Total loss:         0.090118\n",
            "Loss (Diffusion):   0.019687\n",
            "Loss (Reconstruction x20): 0.070430\n",
            "---------\n",
            "Epoch: 2869/3000\n",
            "Total loss:         0.093373\n",
            "Loss (Diffusion):   0.019392\n",
            "Loss (Reconstruction x20): 0.073981\n",
            "---------\n",
            "Epoch: 2870/3000\n",
            "Total loss:         0.079785\n",
            "Loss (Diffusion):   0.017291\n",
            "Loss (Reconstruction x20): 0.062494\n",
            "---------\n",
            "Epoch: 2871/3000\n",
            "Total loss:         0.085681\n",
            "Loss (Diffusion):   0.017066\n",
            "Loss (Reconstruction x20): 0.068615\n",
            "---------\n",
            "Epoch: 2872/3000\n",
            "Total loss:         0.092561\n",
            "Loss (Diffusion):   0.018589\n",
            "Loss (Reconstruction x20): 0.073972\n",
            "---------\n",
            "Epoch: 2873/3000\n",
            "Total loss:         0.086054\n",
            "Loss (Diffusion):   0.016049\n",
            "Loss (Reconstruction x20): 0.070004\n",
            "---------\n",
            "Epoch: 2874/3000\n",
            "Total loss:         0.074768\n",
            "Loss (Diffusion):   0.014719\n",
            "Loss (Reconstruction x20): 0.060049\n",
            "---------\n",
            "Epoch: 2875/3000\n",
            "Total loss:         0.079004\n",
            "Loss (Diffusion):   0.015653\n",
            "Loss (Reconstruction x20): 0.063351\n",
            "---------\n",
            "Epoch: 2876/3000\n",
            "Total loss:         0.090762\n",
            "Loss (Diffusion):   0.016192\n",
            "Loss (Reconstruction x20): 0.074570\n",
            "---------\n",
            "Epoch: 2877/3000\n",
            "Total loss:         0.082024\n",
            "Loss (Diffusion):   0.013116\n",
            "Loss (Reconstruction x20): 0.068907\n",
            "---------\n",
            "Epoch: 2878/3000\n",
            "Total loss:         0.097375\n",
            "Loss (Diffusion):   0.016695\n",
            "Loss (Reconstruction x20): 0.080680\n",
            "---------\n",
            "Epoch: 2879/3000\n",
            "Total loss:         0.099135\n",
            "Loss (Diffusion):   0.015725\n",
            "Loss (Reconstruction x20): 0.083410\n",
            "---------\n",
            "Epoch: 2880/3000\n",
            "Total loss:         0.096480\n",
            "Loss (Diffusion):   0.015804\n",
            "Loss (Reconstruction x20): 0.080675\n",
            "---------\n",
            "Epoch: 2881/3000\n",
            "Total loss:         0.096662\n",
            "Loss (Diffusion):   0.017985\n",
            "Loss (Reconstruction x20): 0.078677\n",
            "---------\n",
            "Epoch: 2882/3000\n",
            "Total loss:         0.102901\n",
            "Loss (Diffusion):   0.021386\n",
            "Loss (Reconstruction x20): 0.081515\n",
            "---------\n",
            "Epoch: 2883/3000\n",
            "Total loss:         0.099339\n",
            "Loss (Diffusion):   0.018818\n",
            "Loss (Reconstruction x20): 0.080521\n",
            "---------\n",
            "Epoch: 2884/3000\n",
            "Total loss:         0.088811\n",
            "Loss (Diffusion):   0.021741\n",
            "Loss (Reconstruction x20): 0.067070\n",
            "---------\n",
            "Epoch: 2885/3000\n",
            "Total loss:         0.112296\n",
            "Loss (Diffusion):   0.015035\n",
            "Loss (Reconstruction x20): 0.097261\n",
            "---------\n",
            "Epoch: 2886/3000\n",
            "Total loss:         0.088410\n",
            "Loss (Diffusion):   0.019655\n",
            "Loss (Reconstruction x20): 0.068754\n",
            "---------\n",
            "Epoch: 2887/3000\n",
            "Total loss:         0.096789\n",
            "Loss (Diffusion):   0.018243\n",
            "Loss (Reconstruction x20): 0.078546\n",
            "---------\n",
            "Epoch: 2888/3000\n",
            "Total loss:         0.093693\n",
            "Loss (Diffusion):   0.018863\n",
            "Loss (Reconstruction x20): 0.074830\n",
            "---------\n",
            "Epoch: 2889/3000\n",
            "Total loss:         0.094843\n",
            "Loss (Diffusion):   0.017618\n",
            "Loss (Reconstruction x20): 0.077225\n",
            "---------\n",
            "Epoch: 2890/3000\n",
            "Total loss:         0.092576\n",
            "Loss (Diffusion):   0.015625\n",
            "Loss (Reconstruction x20): 0.076952\n",
            "---------\n",
            "Epoch: 2891/3000\n",
            "Total loss:         0.084157\n",
            "Loss (Diffusion):   0.017113\n",
            "Loss (Reconstruction x20): 0.067044\n",
            "---------\n",
            "Epoch: 2892/3000\n",
            "Total loss:         0.097567\n",
            "Loss (Diffusion):   0.021200\n",
            "Loss (Reconstruction x20): 0.076366\n",
            "---------\n",
            "Epoch: 2893/3000\n",
            "Total loss:         0.116254\n",
            "Loss (Diffusion):   0.017306\n",
            "Loss (Reconstruction x20): 0.098948\n",
            "---------\n",
            "Epoch: 2894/3000\n",
            "Total loss:         0.082542\n",
            "Loss (Diffusion):   0.021472\n",
            "Loss (Reconstruction x20): 0.061070\n",
            "---------\n",
            "Epoch: 2895/3000\n",
            "Total loss:         0.078042\n",
            "Loss (Diffusion):   0.018805\n",
            "Loss (Reconstruction x20): 0.059238\n",
            "---------\n",
            "Epoch: 2896/3000\n",
            "Total loss:         0.090719\n",
            "Loss (Diffusion):   0.017297\n",
            "Loss (Reconstruction x20): 0.073423\n",
            "---------\n",
            "Epoch: 2897/3000\n",
            "Total loss:         0.087968\n",
            "Loss (Diffusion):   0.019659\n",
            "Loss (Reconstruction x20): 0.068308\n",
            "---------\n",
            "Epoch: 2898/3000\n",
            "Total loss:         0.093399\n",
            "Loss (Diffusion):   0.018721\n",
            "Loss (Reconstruction x20): 0.074678\n",
            "---------\n",
            "Epoch: 2899/3000\n",
            "Total loss:         0.088875\n",
            "Loss (Diffusion):   0.017183\n",
            "Loss (Reconstruction x20): 0.071691\n",
            "---------\n",
            "Epoch: 2900/3000\n",
            "Total loss:         0.099198\n",
            "Loss (Diffusion):   0.014769\n",
            "Loss (Reconstruction x20): 0.084429\n",
            "---------\n",
            "Epoch: 2901/3000\n",
            "Total loss:         0.091314\n",
            "Loss (Diffusion):   0.016254\n",
            "Loss (Reconstruction x20): 0.075060\n",
            "---------\n",
            "Epoch: 2902/3000\n",
            "Total loss:         0.082914\n",
            "Loss (Diffusion):   0.016766\n",
            "Loss (Reconstruction x20): 0.066148\n",
            "---------\n",
            "Epoch: 2903/3000\n",
            "Total loss:         0.111468\n",
            "Loss (Diffusion):   0.020127\n",
            "Loss (Reconstruction x20): 0.091341\n",
            "---------\n",
            "Epoch: 2904/3000\n",
            "Total loss:         0.108442\n",
            "Loss (Diffusion):   0.013715\n",
            "Loss (Reconstruction x20): 0.094727\n",
            "---------\n",
            "Epoch: 2905/3000\n",
            "Total loss:         0.095380\n",
            "Loss (Diffusion):   0.017783\n",
            "Loss (Reconstruction x20): 0.077598\n",
            "---------\n",
            "Epoch: 2906/3000\n",
            "Total loss:         0.093009\n",
            "Loss (Diffusion):   0.018398\n",
            "Loss (Reconstruction x20): 0.074612\n",
            "---------\n",
            "Epoch: 2907/3000\n",
            "Total loss:         0.078734\n",
            "Loss (Diffusion):   0.015364\n",
            "Loss (Reconstruction x20): 0.063370\n",
            "---------\n",
            "Epoch: 2908/3000\n",
            "Total loss:         0.089276\n",
            "Loss (Diffusion):   0.015801\n",
            "Loss (Reconstruction x20): 0.073474\n",
            "---------\n",
            "Epoch: 2909/3000\n",
            "Total loss:         0.092448\n",
            "Loss (Diffusion):   0.020557\n",
            "Loss (Reconstruction x20): 0.071891\n",
            "---------\n",
            "Epoch: 2910/3000\n",
            "Total loss:         0.081193\n",
            "Loss (Diffusion):   0.015698\n",
            "Loss (Reconstruction x20): 0.065494\n",
            "---------\n",
            "Epoch: 2911/3000\n",
            "Total loss:         0.105143\n",
            "Loss (Diffusion):   0.019367\n",
            "Loss (Reconstruction x20): 0.085776\n",
            "---------\n",
            "Epoch: 2912/3000\n",
            "Total loss:         0.099802\n",
            "Loss (Diffusion):   0.019014\n",
            "Loss (Reconstruction x20): 0.080789\n",
            "---------\n",
            "Epoch: 2913/3000\n",
            "Total loss:         0.093079\n",
            "Loss (Diffusion):   0.016047\n",
            "Loss (Reconstruction x20): 0.077032\n",
            "---------\n",
            "Epoch: 2914/3000\n",
            "Total loss:         0.090046\n",
            "Loss (Diffusion):   0.015019\n",
            "Loss (Reconstruction x20): 0.075028\n",
            "---------\n",
            "Epoch: 2915/3000\n",
            "Total loss:         0.097823\n",
            "Loss (Diffusion):   0.016575\n",
            "Loss (Reconstruction x20): 0.081248\n",
            "---------\n",
            "Epoch: 2916/3000\n",
            "Total loss:         0.087914\n",
            "Loss (Diffusion):   0.016189\n",
            "Loss (Reconstruction x20): 0.071725\n",
            "---------\n",
            "Epoch: 2917/3000\n",
            "Total loss:         0.092415\n",
            "Loss (Diffusion):   0.017811\n",
            "Loss (Reconstruction x20): 0.074605\n",
            "---------\n",
            "Epoch: 2918/3000\n",
            "Total loss:         0.076365\n",
            "Loss (Diffusion):   0.021922\n",
            "Loss (Reconstruction x20): 0.054443\n",
            "---------\n",
            "Epoch: 2919/3000\n",
            "Total loss:         0.079157\n",
            "Loss (Diffusion):   0.019467\n",
            "Loss (Reconstruction x20): 0.059689\n",
            "---------\n",
            "Epoch: 2920/3000\n",
            "Total loss:         0.086089\n",
            "Loss (Diffusion):   0.013241\n",
            "Loss (Reconstruction x20): 0.072847\n",
            "---------\n",
            "Epoch: 2921/3000\n",
            "Total loss:         0.079303\n",
            "Loss (Diffusion):   0.020311\n",
            "Loss (Reconstruction x20): 0.058992\n",
            "---------\n",
            "Epoch: 2922/3000\n",
            "Total loss:         0.090722\n",
            "Loss (Diffusion):   0.017488\n",
            "Loss (Reconstruction x20): 0.073235\n",
            "---------\n",
            "Epoch: 2923/3000\n",
            "Total loss:         0.092899\n",
            "Loss (Diffusion):   0.019100\n",
            "Loss (Reconstruction x20): 0.073799\n",
            "---------\n",
            "Epoch: 2924/3000\n",
            "Total loss:         0.081081\n",
            "Loss (Diffusion):   0.017601\n",
            "Loss (Reconstruction x20): 0.063480\n",
            "---------\n",
            "Epoch: 2925/3000\n",
            "Total loss:         0.105369\n",
            "Loss (Diffusion):   0.016542\n",
            "Loss (Reconstruction x20): 0.088827\n",
            "---------\n",
            "Epoch: 2926/3000\n",
            "Total loss:         0.071592\n",
            "Loss (Diffusion):   0.017283\n",
            "Loss (Reconstruction x20): 0.054309\n",
            "---------\n",
            "Epoch: 2927/3000\n",
            "Total loss:         0.087190\n",
            "Loss (Diffusion):   0.016599\n",
            "Loss (Reconstruction x20): 0.070591\n",
            "---------\n",
            "Epoch: 2928/3000\n",
            "Total loss:         0.087668\n",
            "Loss (Diffusion):   0.014809\n",
            "Loss (Reconstruction x20): 0.072860\n",
            "---------\n",
            "Epoch: 2929/3000\n",
            "Total loss:         0.086720\n",
            "Loss (Diffusion):   0.016645\n",
            "Loss (Reconstruction x20): 0.070075\n",
            "---------\n",
            "Epoch: 2930/3000\n",
            "Total loss:         0.080186\n",
            "Loss (Diffusion):   0.017261\n",
            "Loss (Reconstruction x20): 0.062926\n",
            "---------\n",
            "Epoch: 2931/3000\n",
            "Total loss:         0.076973\n",
            "Loss (Diffusion):   0.015316\n",
            "Loss (Reconstruction x20): 0.061657\n",
            "---------\n",
            "Epoch: 2932/3000\n",
            "Total loss:         0.075443\n",
            "Loss (Diffusion):   0.017133\n",
            "Loss (Reconstruction x20): 0.058310\n",
            "---------\n",
            "Epoch: 2933/3000\n",
            "Total loss:         0.078935\n",
            "Loss (Diffusion):   0.018099\n",
            "Loss (Reconstruction x20): 0.060836\n",
            "---------\n",
            "Epoch: 2934/3000\n",
            "Total loss:         0.079782\n",
            "Loss (Diffusion):   0.016603\n",
            "Loss (Reconstruction x20): 0.063178\n",
            "---------\n",
            "Epoch: 2935/3000\n",
            "Total loss:         0.079255\n",
            "Loss (Diffusion):   0.018978\n",
            "Loss (Reconstruction x20): 0.060277\n",
            "---------\n",
            "Epoch: 2936/3000\n",
            "Total loss:         0.072319\n",
            "Loss (Diffusion):   0.016871\n",
            "Loss (Reconstruction x20): 0.055447\n",
            "---------\n",
            "Epoch: 2937/3000\n",
            "Total loss:         0.082381\n",
            "Loss (Diffusion):   0.019270\n",
            "Loss (Reconstruction x20): 0.063111\n",
            "---------\n",
            "Epoch: 2938/3000\n",
            "Total loss:         0.091942\n",
            "Loss (Diffusion):   0.013497\n",
            "Loss (Reconstruction x20): 0.078445\n",
            "---------\n",
            "Epoch: 2939/3000\n",
            "Total loss:         0.081058\n",
            "Loss (Diffusion):   0.017626\n",
            "Loss (Reconstruction x20): 0.063432\n",
            "---------\n",
            "Epoch: 2940/3000\n",
            "Total loss:         0.077985\n",
            "Loss (Diffusion):   0.014053\n",
            "Loss (Reconstruction x20): 0.063931\n",
            "---------\n",
            "Epoch: 2941/3000\n",
            "Total loss:         0.076373\n",
            "Loss (Diffusion):   0.016063\n",
            "Loss (Reconstruction x20): 0.060310\n",
            "---------\n",
            "Epoch: 2942/3000\n",
            "Total loss:         0.067439\n",
            "Loss (Diffusion):   0.015807\n",
            "Loss (Reconstruction x20): 0.051632\n",
            "---------\n",
            "Epoch: 2943/3000\n",
            "Total loss:         0.086470\n",
            "Loss (Diffusion):   0.016872\n",
            "Loss (Reconstruction x20): 0.069598\n",
            "---------\n",
            "Epoch: 2944/3000\n",
            "Total loss:         0.097136\n",
            "Loss (Diffusion):   0.017029\n",
            "Loss (Reconstruction x20): 0.080106\n",
            "---------\n",
            "Epoch: 2945/3000\n",
            "Total loss:         0.078970\n",
            "Loss (Diffusion):   0.015056\n",
            "Loss (Reconstruction x20): 0.063914\n",
            "---------\n",
            "Epoch: 2946/3000\n",
            "Total loss:         0.094505\n",
            "Loss (Diffusion):   0.015883\n",
            "Loss (Reconstruction x20): 0.078622\n",
            "---------\n",
            "Epoch: 2947/3000\n",
            "Total loss:         0.100072\n",
            "Loss (Diffusion):   0.019667\n",
            "Loss (Reconstruction x20): 0.080405\n",
            "---------\n",
            "Epoch: 2948/3000\n",
            "Total loss:         0.082458\n",
            "Loss (Diffusion):   0.017060\n",
            "Loss (Reconstruction x20): 0.065398\n",
            "---------\n",
            "Epoch: 2949/3000\n",
            "Total loss:         0.095835\n",
            "Loss (Diffusion):   0.018355\n",
            "Loss (Reconstruction x20): 0.077480\n",
            "---------\n",
            "Epoch: 2950/3000\n",
            "Total loss:         0.076274\n",
            "Loss (Diffusion):   0.017186\n",
            "Loss (Reconstruction x20): 0.059088\n",
            "---------\n",
            "Epoch: 2951/3000\n",
            "Total loss:         0.119754\n",
            "Loss (Diffusion):   0.018211\n",
            "Loss (Reconstruction x20): 0.101544\n",
            "---------\n",
            "Epoch: 2952/3000\n",
            "Total loss:         0.103958\n",
            "Loss (Diffusion):   0.025056\n",
            "Loss (Reconstruction x20): 0.078902\n",
            "---------\n",
            "Epoch: 2953/3000\n",
            "Total loss:         0.097550\n",
            "Loss (Diffusion):   0.024558\n",
            "Loss (Reconstruction x20): 0.072992\n",
            "---------\n",
            "Epoch: 2954/3000\n",
            "Total loss:         0.104978\n",
            "Loss (Diffusion):   0.021312\n",
            "Loss (Reconstruction x20): 0.083666\n",
            "---------\n",
            "Epoch: 2955/3000\n",
            "Total loss:         0.122837\n",
            "Loss (Diffusion):   0.022463\n",
            "Loss (Reconstruction x20): 0.100374\n",
            "---------\n",
            "Epoch: 2956/3000\n",
            "Total loss:         0.108325\n",
            "Loss (Diffusion):   0.022161\n",
            "Loss (Reconstruction x20): 0.086164\n",
            "---------\n",
            "Epoch: 2957/3000\n",
            "Total loss:         0.092529\n",
            "Loss (Diffusion):   0.020081\n",
            "Loss (Reconstruction x20): 0.072448\n",
            "---------\n",
            "Epoch: 2958/3000\n",
            "Total loss:         0.085273\n",
            "Loss (Diffusion):   0.018322\n",
            "Loss (Reconstruction x20): 0.066951\n",
            "---------\n",
            "Epoch: 2959/3000\n",
            "Total loss:         0.092700\n",
            "Loss (Diffusion):   0.018188\n",
            "Loss (Reconstruction x20): 0.074512\n",
            "---------\n",
            "Epoch: 2960/3000\n",
            "Total loss:         0.105955\n",
            "Loss (Diffusion):   0.023620\n",
            "Loss (Reconstruction x20): 0.082334\n",
            "---------\n",
            "Epoch: 2961/3000\n",
            "Total loss:         0.110598\n",
            "Loss (Diffusion):   0.023681\n",
            "Loss (Reconstruction x20): 0.086917\n",
            "---------\n",
            "Epoch: 2962/3000\n",
            "Total loss:         0.097352\n",
            "Loss (Diffusion):   0.019406\n",
            "Loss (Reconstruction x20): 0.077946\n",
            "---------\n",
            "Epoch: 2963/3000\n",
            "Total loss:         0.105515\n",
            "Loss (Diffusion):   0.018008\n",
            "Loss (Reconstruction x20): 0.087508\n",
            "---------\n",
            "Epoch: 2964/3000\n",
            "Total loss:         0.092279\n",
            "Loss (Diffusion):   0.018086\n",
            "Loss (Reconstruction x20): 0.074192\n",
            "---------\n",
            "Epoch: 2965/3000\n",
            "Total loss:         0.095448\n",
            "Loss (Diffusion):   0.020180\n",
            "Loss (Reconstruction x20): 0.075268\n",
            "---------\n",
            "Epoch: 2966/3000\n",
            "Total loss:         0.114339\n",
            "Loss (Diffusion):   0.021687\n",
            "Loss (Reconstruction x20): 0.092652\n",
            "---------\n",
            "Epoch: 2967/3000\n",
            "Total loss:         0.105231\n",
            "Loss (Diffusion):   0.017436\n",
            "Loss (Reconstruction x20): 0.087796\n",
            "---------\n",
            "Epoch: 2968/3000\n",
            "Total loss:         0.093416\n",
            "Loss (Diffusion):   0.019998\n",
            "Loss (Reconstruction x20): 0.073419\n",
            "---------\n",
            "Epoch: 2969/3000\n",
            "Total loss:         0.098757\n",
            "Loss (Diffusion):   0.019545\n",
            "Loss (Reconstruction x20): 0.079212\n",
            "---------\n",
            "Epoch: 2970/3000\n",
            "Total loss:         0.087819\n",
            "Loss (Diffusion):   0.016620\n",
            "Loss (Reconstruction x20): 0.071200\n",
            "---------\n",
            "Epoch: 2971/3000\n",
            "Total loss:         0.096574\n",
            "Loss (Diffusion):   0.015694\n",
            "Loss (Reconstruction x20): 0.080879\n",
            "---------\n",
            "Epoch: 2972/3000\n",
            "Total loss:         0.091480\n",
            "Loss (Diffusion):   0.017034\n",
            "Loss (Reconstruction x20): 0.074446\n",
            "---------\n",
            "Epoch: 2973/3000\n",
            "Total loss:         0.088512\n",
            "Loss (Diffusion):   0.019003\n",
            "Loss (Reconstruction x20): 0.069509\n",
            "---------\n",
            "Epoch: 2974/3000\n",
            "Total loss:         0.101736\n",
            "Loss (Diffusion):   0.022578\n",
            "Loss (Reconstruction x20): 0.079158\n",
            "---------\n",
            "Epoch: 2975/3000\n",
            "Total loss:         0.095497\n",
            "Loss (Diffusion):   0.018010\n",
            "Loss (Reconstruction x20): 0.077486\n",
            "---------\n",
            "Epoch: 2976/3000\n",
            "Total loss:         0.081078\n",
            "Loss (Diffusion):   0.016898\n",
            "Loss (Reconstruction x20): 0.064180\n",
            "---------\n",
            "Epoch: 2977/3000\n",
            "Total loss:         0.082346\n",
            "Loss (Diffusion):   0.020284\n",
            "Loss (Reconstruction x20): 0.062063\n",
            "---------\n",
            "Epoch: 2978/3000\n",
            "Total loss:         0.093400\n",
            "Loss (Diffusion):   0.016001\n",
            "Loss (Reconstruction x20): 0.077399\n",
            "---------\n",
            "Epoch: 2979/3000\n",
            "Total loss:         0.087764\n",
            "Loss (Diffusion):   0.017138\n",
            "Loss (Reconstruction x20): 0.070626\n",
            "---------\n",
            "Epoch: 2980/3000\n",
            "Total loss:         0.083767\n",
            "Loss (Diffusion):   0.018353\n",
            "Loss (Reconstruction x20): 0.065414\n",
            "---------\n",
            "Epoch: 2981/3000\n",
            "Total loss:         0.090615\n",
            "Loss (Diffusion):   0.018160\n",
            "Loss (Reconstruction x20): 0.072455\n",
            "---------\n",
            "Epoch: 2982/3000\n",
            "Total loss:         0.090894\n",
            "Loss (Diffusion):   0.020860\n",
            "Loss (Reconstruction x20): 0.070034\n",
            "---------\n",
            "Epoch: 2983/3000\n",
            "Total loss:         0.088409\n",
            "Loss (Diffusion):   0.018649\n",
            "Loss (Reconstruction x20): 0.069760\n",
            "---------\n",
            "Epoch: 2984/3000\n",
            "Total loss:         0.097031\n",
            "Loss (Diffusion):   0.016000\n",
            "Loss (Reconstruction x20): 0.081031\n",
            "---------\n",
            "Epoch: 2985/3000\n",
            "Total loss:         0.102277\n",
            "Loss (Diffusion):   0.019457\n",
            "Loss (Reconstruction x20): 0.082820\n",
            "---------\n",
            "Epoch: 2986/3000\n",
            "Total loss:         0.090618\n",
            "Loss (Diffusion):   0.021854\n",
            "Loss (Reconstruction x20): 0.068765\n",
            "---------\n",
            "Epoch: 2987/3000\n",
            "Total loss:         0.093705\n",
            "Loss (Diffusion):   0.016154\n",
            "Loss (Reconstruction x20): 0.077551\n",
            "---------\n",
            "Epoch: 2988/3000\n",
            "Total loss:         0.072184\n",
            "Loss (Diffusion):   0.016208\n",
            "Loss (Reconstruction x20): 0.055976\n",
            "---------\n",
            "Epoch: 2989/3000\n",
            "Total loss:         0.104194\n",
            "Loss (Diffusion):   0.019594\n",
            "Loss (Reconstruction x20): 0.084600\n",
            "---------\n",
            "Epoch: 2990/3000\n",
            "Total loss:         0.083787\n",
            "Loss (Diffusion):   0.015550\n",
            "Loss (Reconstruction x20): 0.068237\n",
            "---------\n",
            "Epoch: 2991/3000\n",
            "Total loss:         0.105686\n",
            "Loss (Diffusion):   0.021762\n",
            "Loss (Reconstruction x20): 0.083923\n",
            "---------\n",
            "Epoch: 2992/3000\n",
            "Total loss:         0.090485\n",
            "Loss (Diffusion):   0.017927\n",
            "Loss (Reconstruction x20): 0.072558\n",
            "---------\n",
            "Epoch: 2993/3000\n",
            "Total loss:         0.116897\n",
            "Loss (Diffusion):   0.018780\n",
            "Loss (Reconstruction x20): 0.098116\n",
            "---------\n",
            "Epoch: 2994/3000\n",
            "Total loss:         0.092582\n",
            "Loss (Diffusion):   0.017790\n",
            "Loss (Reconstruction x20): 0.074792\n",
            "---------\n",
            "Epoch: 2995/3000\n",
            "Total loss:         0.087007\n",
            "Loss (Diffusion):   0.018334\n",
            "Loss (Reconstruction x20): 0.068673\n",
            "---------\n",
            "Epoch: 2996/3000\n",
            "Total loss:         0.088024\n",
            "Loss (Diffusion):   0.016297\n",
            "Loss (Reconstruction x20): 0.071726\n",
            "---------\n",
            "Epoch: 2997/3000\n",
            "Total loss:         0.093152\n",
            "Loss (Diffusion):   0.020855\n",
            "Loss (Reconstruction x20): 0.072297\n",
            "---------\n",
            "Epoch: 2998/3000\n",
            "Total loss:         0.090465\n",
            "Loss (Diffusion):   0.017574\n",
            "Loss (Reconstruction x20): 0.072891\n",
            "---------\n",
            "Epoch: 2999/3000\n",
            "Total loss:         0.091699\n",
            "Loss (Diffusion):   0.013503\n",
            "Loss (Reconstruction x20): 0.078196\n",
            "---------\n",
            "Epoch: 3000/3000\n",
            "Total loss:         0.081718\n",
            "Loss (Diffusion):   0.017555\n",
            "Loss (Reconstruction x20): 0.064163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6GpgrvpYAtnC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fY44M1A0AtpS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-_Kz4W-7AtsB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_all()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# helpers to read spans from dtype (no more hard-coded slices)\n",
        "# ------------------------------------------------------------\n",
        "def _dtype_spans(dtype: pd.DataFrame):\n",
        "    real_cols = []\n",
        "    cat_groups = []  # list of (name, start, end)\n",
        "    for _, r in dtype.iterrows():\n",
        "        s, e = int(r[\"index_start\"]), int(r[\"index_end\"])\n",
        "        if r[\"type\"] == \"real\":\n",
        "            real_cols.append((r[\"name\"], s, e))  # e = s+1 for reals\n",
        "        else:\n",
        "            cat_groups.append((r[\"name\"], s, e))\n",
        "    return real_cols, cat_groups\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# ExecuteC002_InspectAppropriateness\n",
        "#   Filters sequences whose *real features* lie in [-1,1] over all time,\n",
        "#   rescales reals from [-1,1] -> [0,1], concatenates with categoricals.\n",
        "#   Schema-driven (dtype), device-agnostic, vectorised.\n",
        "# =====================================================================\n",
        "@torch.no_grad()\n",
        "def ExecuteC002_InspectAppropriateness(\n",
        "    noisy_X: torch.Tensor,\n",
        "    dtype: Optional[pd.DataFrame] = None,\n",
        "    valid_len: Optional[int] = None,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      noisy_X : [B, 1, D, T] tensor in model space (reals in [-1,1], cats ~ one-hot-ish)\n",
        "      dtype   : schema manifest to locate real/categorical spans (recommended)\n",
        "      valid_len: if provided, keep only the first valid_len time steps\n",
        "\n",
        "    Returns:\n",
        "      filtered tensor [K, 1, D, T_used] with reals mapped to [0,1]\n",
        "    \"\"\"\n",
        "    assert noisy_X.ndim == 4 and noisy_X.size(1) == 1, \"Expected [B,1,D,T]\"\n",
        "\n",
        "    B, _, D, T = noisy_X.shape\n",
        "    if valid_len is not None:\n",
        "        T_used = min(valid_len, T)\n",
        "        noisy_X = noisy_X[..., :T_used]\n",
        "    else:\n",
        "        T_used = T\n",
        "\n",
        "    device = noisy_X.device\n",
        "\n",
        "    # figure spans\n",
        "    if dtype is not None:\n",
        "        real_cols, cat_groups = _dtype_spans(dtype)\n",
        "        real_slices = [slice(s, e) for _, s, e in real_cols]\n",
        "        cat_slices  = [slice(s, e) for _, s, e in cat_groups]\n",
        "    else:\n",
        "        # fallback to legacy layout: first 2 are reals, rest categoricals\n",
        "        real_slices = [slice(0, 1), slice(1, 2)]\n",
        "        cat_slices  = [slice(2, D)]\n",
        "\n",
        "    # stack reals into one tensor [B, 1, R, T]\n",
        "    real_idx = torch.cat([torch.arange(s.start, s.stop, device=device) for s in real_slices])\n",
        "    reals = noisy_X[:, :, real_idx, :]  # [B,1,R,T]\n",
        "    cats  = torch.cat([noisy_X[:, :, s, :] for s in cat_slices], dim=2) if cat_slices else noisy_X.new_zeros((B,1,0,T_used))\n",
        "\n",
        "    # filter: min >= -1 and max <= 1 across time for all real channels\n",
        "    min_ok = reals.amin(dim=3) >= -1.0    # [B,1,R]\n",
        "    max_ok = reals.amax(dim=3) <=  1.0    # [B,1,R]\n",
        "    good   = (min_ok & max_ok).sum(dim=2).squeeze(1) == reals.size(2)  # [B]\n",
        "\n",
        "    reals  = reals[good]\n",
        "    cats   = cats[good]\n",
        "    if reals.numel() == 0:\n",
        "        return noisy_X[:0, :, :, :T_used]\n",
        "\n",
        "    # map reals [-1,1] -> [0,1]\n",
        "    reals = (reals + 1.0) * 0.5\n",
        "\n",
        "    # reassemble in original feature order D\n",
        "    out = noisy_X.new_zeros((reals.size(0), 1, D, T_used))\n",
        "    # place reals\n",
        "    for k, s in enumerate(real_slices):\n",
        "        out[:, :, s, :] = reals[:, :, k:k+1, :]\n",
        "    # place categoricals\n",
        "    cat_cursor = 0\n",
        "    for s in cat_slices:\n",
        "        width = s.stop - s.start\n",
        "        out[:, :, s, :] = cats[:, :, cat_cursor:cat_cursor+width, :]\n",
        "        cat_cursor += width\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# ExecuteC002_BTS  (Back To Structured)\n",
        "#   Decodes one sampled *time step* per row into a tabular DataFrame.\n",
        "#   Reals: prefer unified inverse via bc_params; otherwise optional legacy decode.\n",
        "#   Categoricals: argmax within each group (dtype-driven).\n",
        "# =====================================================================\n",
        "\n",
        "# Legacy decode table (exact numbers from previous implementation)\n",
        "LEGACY_REAL_DECODE: Dict[str, Dict[str, float]] = {\n",
        "    # y_legacy = boxcox(VL+1, -0.1186) / 6.278682  -> inverse: inv_boxcox(y*6.278682, -0.1186) - 1\n",
        "    \"VL\":  {\"kind\": \"boxcox\", \"lambda\": -0.11862349303078497, \"scale\": 6.278682, \"post_sub\": 1.0},\n",
        "    # y_legacy = log(CD4+1) / 13.311331132544721   -> inverse: exp(y*13.311331132544721) - 1\n",
        "    \"CD4\": {\"kind\": \"log\",    \"scale\": 13.311331132544721,     \"post_sub\": 1.0},\n",
        "}\n",
        "\n",
        "@torch.no_grad()\n",
        "def ExecuteC002_BTS(\n",
        "    DataType: pd.DataFrame,             # dtype manifest\n",
        "    FD: torch.Tensor,                   # [N, D] in model space (single time step)\n",
        "    feature_names: Optional[list] = None,\n",
        "    transform_params: Optional[Dict[str, dict]] = None,  # from compute_boxcox_params\n",
        "    use_legacy_numbers: bool = False,                    # << keep old numbers if no bc_params\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      DataType         : dtype (with 'name','type','index_start','index_end')\n",
        "      FD               : [N, D] float tensor for a single time slice\n",
        "      feature_names    : expanded names for safety checks (optional)\n",
        "      transform_params : dict from `compute_boxcox_params`; preferred for decoding reals\n",
        "      use_legacy_numbers: if True and transform_params is None, decode reals with legacy constants\n",
        "\n",
        "    Returns:\n",
        "      Pandas DataFrame with columns:\n",
        "        [VL, CD4, Gender, Ethnic, Base_Drug_Combo, Extra_PI, Extra_pk_En]\n",
        "      Reals in original units; categoricals as integer codes (0..K-1).\n",
        "    \"\"\"\n",
        "    assert FD.ndim == 2, \"FD must be [N, D]\"\n",
        "    N, D = FD.shape\n",
        "\n",
        "    real_cols, cat_groups = _dtype_spans(DataType)\n",
        "\n",
        "    # Work on CPU for DataFrame creation\n",
        "    X = FD.detach().to(\"cpu\").clone()  # [N, D]\n",
        "\n",
        "    # ---- Decode reals ----\n",
        "    for name, s, e in real_cols:\n",
        "        col = X[:, s:e].squeeze(1)  # [N]\n",
        "        # Preferred: unified params (Box–Cox->[0,1] inverse)\n",
        "        if transform_params is not None and name in transform_params:\n",
        "            p = transform_params[name]\n",
        "            # unscale to Box–Cox space\n",
        "            bc = col * p[\"range\"] + p[\"min\"]\n",
        "            # inverse Box–Cox\n",
        "            if p[\"lambda\"] == 0.0:\n",
        "                raw = torch.exp(bc) - p[\"eps\"]\n",
        "            else:\n",
        "                raw = torch.clamp(p[\"lambda\"] * bc + 1.0, min=1e-12).pow(1.0 / p[\"lambda\"]) - p[\"eps\"]\n",
        "            X[:, s:e] = raw.unsqueeze(1)\n",
        "        # Legacy fallback (only if asked, and only when bc_params not provided)\n",
        "        elif use_legacy_numbers and name in LEGACY_REAL_DECODE:\n",
        "            cfg = LEGACY_REAL_DECODE[name]\n",
        "            if cfg[\"kind\"] == \"boxcox\":\n",
        "                lam = float(cfg[\"lambda\"])\n",
        "                scale = float(cfg[\"scale\"])\n",
        "                bc = col * scale\n",
        "                raw = torch.clamp(lam * bc + 1.0, min=1e-12).pow(1.0 / lam) - float(cfg.get(\"post_sub\", 0.0))\n",
        "            elif cfg[\"kind\"] == \"log\":\n",
        "                scale = float(cfg[\"scale\"])\n",
        "                raw = torch.exp(col * scale) - float(cfg.get(\"post_sub\", 0.0))\n",
        "            else:\n",
        "                raw = col  # unknown kind: passthrough\n",
        "            X[:, s:e] = raw.unsqueeze(1)\n",
        "        # else: leave as-is (already in original units)\n",
        "\n",
        "    # ---- Decode categoricals by argmax within each group ----\n",
        "    for name, s, e in cat_groups:\n",
        "        block = X[:, s:e]                     # [N, K]\n",
        "        idx = torch.argmax(block, dim=1)      # [N] codes 0..K-1\n",
        "        X[:, s] = idx.to(X.dtype)             # store code in first slot\n",
        "        if e - s > 1:\n",
        "            X[:, s+1:e] = 0                   # tidy up (optional)\n",
        "\n",
        "    # ---- Build final tidy DataFrame ----\n",
        "    out = {}\n",
        "    for name, s, e in real_cols:\n",
        "        out[name] = X[:, s:e].squeeze(1).numpy()\n",
        "    for name, s, e in cat_groups:\n",
        "        out[name] = X[:, s].numpy().astype(\"int64\")\n",
        "\n",
        "    return pd.DataFrame(out)\n"
      ],
      "metadata": {
        "id": "0FgwPim0Atwq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_all()\n",
        "\n",
        "def ExecuteB004(\n",
        "    data_types,                      # dtype manifest (schema)\n",
        "    My_Model,                        # trained ExecuteB003 instance\n",
        "    Hyper006_NEpochs,                # for display only (kept for parity)\n",
        "    *,\n",
        "    To_Synth: int = 300,             # target: number of accepted synthetic patients\n",
        "    Synth_BS: int = 100,             # denoise attempts per batch\n",
        "    Feat_Dim: int = 22,              # feature channels (D)\n",
        "    Series_Len: int = 100,           # model time length used during sampling\n",
        "    Generated_Len: int = 60,         # keep only the first 60 steps in output\n",
        "    transform_params: Optional[Dict[str, dict]] = None,  # from compute_boxcox_params\n",
        "    use_legacy_numbers: bool = False # True -> decode reals with legacy constants\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        Pandas DataFrame with logical columns decoded to original units (reals) and\n",
        "        integer codes for categoricals; length is To_Synth * Generated_Len rows\n",
        "        (one row per time step per accepted patient).\n",
        "    \"\"\"\n",
        "\n",
        "    device = next(My_Model.UNet.parameters()).device\n",
        "    My_Model.UNet.eval()\n",
        "\n",
        "    # ---- Resolve schedule and step count (new buffers first, legacy fallback) ----\n",
        "    betas       = getattr(My_Model, \"betas\",       getattr(My_Model, \"All_Betas\",        None))\n",
        "    alphas      = getattr(My_Model, \"alphas\",      getattr(My_Model, \"All_Alphas\",       None))\n",
        "    alphas_bar  = getattr(My_Model, \"alphas_bar\",  getattr(My_Model, \"All_Prod_Alphas\",  None))\n",
        "    if betas is None or alphas is None or alphas_bar is None:\n",
        "        raise AttributeError(\"Model missing diffusion buffers (betas/alphas/alphas_bar).\")\n",
        "\n",
        "    # Ensure they live on the same device as sampling tensors\n",
        "    betas      = betas.to(device)\n",
        "    alphas     = alphas.to(device)\n",
        "    alphas_bar = alphas_bar.to(device)\n",
        "\n",
        "    n_steps = getattr(getattr(My_Model, \"cfg\", None), \"n_steps\", None)\n",
        "    if n_steps is None:\n",
        "        n_steps = getattr(My_Model, \"N_Steps\", None)\n",
        "    if n_steps is None:\n",
        "        n_steps = int(len(betas))\n",
        "\n",
        "    collected = []\n",
        "    total_acc = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        while total_acc < To_Synth:\n",
        "            # x_T ~ N(0, I)\n",
        "            x_t = torch.randn(Synth_BS, 1, Feat_Dim, Series_Len, device=device)\n",
        "\n",
        "            # DDPM ancestral sampling: t = T-1 ... 0\n",
        "            for idx, t in enumerate(reversed(range(n_steps))):\n",
        "                # time index per sample (UNet expects [B] long)\n",
        "                Cur_T = torch.full((Synth_BS,), t, device=device, dtype=torch.long)\n",
        "\n",
        "                # predict noise eps_theta(x_t, t)\n",
        "                eps_hat = My_Model.Network_Backward(x_t, Cur_T)\n",
        "\n",
        "                # scalars for this t\n",
        "                alpha_t  = alphas[t].view(1, 1, 1, 1)\n",
        "                abar_t   = alphas_bar[t].view(1, 1, 1, 1)\n",
        "                beta_t   = betas[t].view(1, 1, 1, 1)\n",
        "\n",
        "                # x_{t-1} mean\n",
        "                x_mean = (x_t - ((1 - alpha_t) / (1 - abar_t).sqrt()) * eps_hat) / alpha_t.sqrt()\n",
        "\n",
        "                # add noise if t > 0\n",
        "                if t > 0:\n",
        "                    z = torch.randn_like(x_t)\n",
        "                    sigma_t = beta_t.sqrt()\n",
        "                    x_t = x_mean + sigma_t * z\n",
        "                else:\n",
        "                    x_t = x_mean\n",
        "\n",
        "                if (idx + 1) % 50 == 0:\n",
        "                    print(f\"Currently Denoising: {idx + 1}/{n_steps}\")\n",
        "\n",
        "            # keep only the first Generated_Len time steps\n",
        "            x_t = x_t[..., :Generated_Len]  # [Synth_BS, 1, D, Generated_Len]\n",
        "\n",
        "            # filter + bring reals back to [0,1] for decode\n",
        "            accepted = ExecuteC002_InspectAppropriateness(\n",
        "                x_t, dtype=data_types, valid_len=Generated_Len\n",
        "            )  # [K, 1, D, Generated_Len]\n",
        "\n",
        "            if accepted.shape[0] > 0:\n",
        "                collected.append(accepted)\n",
        "                total_acc += accepted.shape[0]\n",
        "\n",
        "            print(\"---\" * 3)\n",
        "            print(f\"Currently Epoch {Hyper006_NEpochs}\")\n",
        "            print(f\"All_Fin shape: {min(total_acc, To_Synth)}/{To_Synth}\")\n",
        "\n",
        "    # stack and truncate to exactly To_Synth\n",
        "    All_Fin = torch.cat(collected, dim=0)[:To_Synth, :, :, :]   # [To_Synth, 1, D, L]\n",
        "\n",
        "    # ---- reshape to [To_Synth * L, D] (one row per time step) ----\n",
        "    # [N,1,D,L] -> [N,L,D] -> [N*L, D]\n",
        "    seq = All_Fin.transpose(2, 3).squeeze(1)                    # [N, L, D]\n",
        "    FD  = seq.reshape(-1, seq.shape[-1]).contiguous()           # [N*L, D]\n",
        "\n",
        "    # ---- decode to tidy DataFrame (reals to original units; cats to codes) ----\n",
        "    Fake_Data = ExecuteC002_BTS(\n",
        "        DataType=data_types,\n",
        "        FD=FD,\n",
        "        transform_params=transform_params,        # preferred unified decode\n",
        "        use_legacy_numbers=use_legacy_numbers,    # exact legacy numbers if no bc_params\n",
        "    )\n",
        "\n",
        "    return Fake_Data\n"
      ],
      "metadata": {
        "id": "1xUAIeYPE6g6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_all()\n",
        "\n",
        "df_fake = ExecuteB004(\n",
        "    data_types=dtype,\n",
        "    My_Model=My_Model,\n",
        "    Hyper006_NEpochs=EPOCHS,\n",
        "    To_Synth=300,\n",
        "    Synth_BS=100,\n",
        "    Generated_Len=60,\n",
        "    transform_params = bc_params\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRO5mGCFFC8g",
        "outputId": "c33b4280-3e90-4161-89ff-fce53ba9468c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently Denoising: 50/500\n",
            "Currently Denoising: 100/500\n",
            "Currently Denoising: 150/500\n",
            "Currently Denoising: 200/500\n",
            "Currently Denoising: 250/500\n",
            "Currently Denoising: 300/500\n",
            "Currently Denoising: 350/500\n",
            "Currently Denoising: 400/500\n",
            "Currently Denoising: 450/500\n",
            "Currently Denoising: 500/500\n",
            "---------\n",
            "Currently Epoch 3000\n",
            "All_Fin shape: 66/300\n",
            "Currently Denoising: 50/500\n",
            "Currently Denoising: 100/500\n",
            "Currently Denoising: 150/500\n",
            "Currently Denoising: 200/500\n",
            "Currently Denoising: 250/500\n",
            "Currently Denoising: 300/500\n",
            "Currently Denoising: 350/500\n",
            "Currently Denoising: 400/500\n",
            "Currently Denoising: 450/500\n",
            "Currently Denoising: 500/500\n",
            "---------\n",
            "Currently Epoch 3000\n",
            "All_Fin shape: 140/300\n",
            "Currently Denoising: 50/500\n",
            "Currently Denoising: 100/500\n",
            "Currently Denoising: 150/500\n",
            "Currently Denoising: 200/500\n",
            "Currently Denoising: 250/500\n",
            "Currently Denoising: 300/500\n",
            "Currently Denoising: 350/500\n",
            "Currently Denoising: 400/500\n",
            "Currently Denoising: 450/500\n",
            "Currently Denoising: 500/500\n",
            "---------\n",
            "Currently Epoch 3000\n",
            "All_Fin shape: 205/300\n",
            "Currently Denoising: 50/500\n",
            "Currently Denoising: 100/500\n",
            "Currently Denoising: 150/500\n",
            "Currently Denoising: 200/500\n",
            "Currently Denoising: 250/500\n",
            "Currently Denoising: 300/500\n",
            "Currently Denoising: 350/500\n",
            "Currently Denoising: 400/500\n",
            "Currently Denoising: 450/500\n",
            "Currently Denoising: 500/500\n",
            "---------\n",
            "Currently Epoch 3000\n",
            "All_Fin shape: 277/300\n",
            "Currently Denoising: 50/500\n",
            "Currently Denoising: 100/500\n",
            "Currently Denoising: 150/500\n",
            "Currently Denoising: 200/500\n",
            "Currently Denoising: 250/500\n",
            "Currently Denoising: 300/500\n",
            "Currently Denoising: 350/500\n",
            "Currently Denoising: 400/500\n",
            "Currently Denoising: 450/500\n",
            "Currently Denoising: 500/500\n",
            "---------\n",
            "Currently Epoch 3000\n",
            "All_Fin shape: 300/300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_fake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ukZ6zsvxGpkA",
        "outputId": "6d9ebf35-dff3-484c-c856-30616383bae5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                VL         CD4  Gender  Ethnic  Base_Drug_Combo  Extra_PI  \\\n",
              "0       589.146362  759.217163       1       3                0         5   \n",
              "1      1733.681519  449.944427       1       3                0         5   \n",
              "2       122.569893  452.122803       1       3                0         5   \n",
              "3       567.998718  341.406433       1       3                0         5   \n",
              "4       773.000366  279.551514       1       3                0         5   \n",
              "...            ...         ...     ...     ...              ...       ...   \n",
              "17995     3.259575  176.452438       0       2                0         5   \n",
              "17996     1.071616  129.450653       0       2                0         5   \n",
              "17997     4.677550  357.872192       0       2                0         5   \n",
              "17998     2.824970  151.234024       0       2                0         5   \n",
              "17999     2.421133  264.538147       0       2                0         5   \n",
              "\n",
              "       Extra_pk_En  \n",
              "0                0  \n",
              "1                0  \n",
              "2                0  \n",
              "3                0  \n",
              "4                0  \n",
              "...            ...  \n",
              "17995            0  \n",
              "17996            0  \n",
              "17997            0  \n",
              "17998            0  \n",
              "17999            0  \n",
              "\n",
              "[18000 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1baca612-bc9a-4b9c-a401-1cb05ff97640\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VL</th>\n",
              "      <th>CD4</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Ethnic</th>\n",
              "      <th>Base_Drug_Combo</th>\n",
              "      <th>Extra_PI</th>\n",
              "      <th>Extra_pk_En</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>589.146362</td>\n",
              "      <td>759.217163</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1733.681519</td>\n",
              "      <td>449.944427</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>122.569893</td>\n",
              "      <td>452.122803</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>567.998718</td>\n",
              "      <td>341.406433</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>773.000366</td>\n",
              "      <td>279.551514</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17995</th>\n",
              "      <td>3.259575</td>\n",
              "      <td>176.452438</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17996</th>\n",
              "      <td>1.071616</td>\n",
              "      <td>129.450653</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17997</th>\n",
              "      <td>4.677550</td>\n",
              "      <td>357.872192</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17998</th>\n",
              "      <td>2.824970</td>\n",
              "      <td>151.234024</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17999</th>\n",
              "      <td>2.421133</td>\n",
              "      <td>264.538147</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18000 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1baca612-bc9a-4b9c-a401-1cb05ff97640')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1baca612-bc9a-4b9c-a401-1cb05ff97640 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1baca612-bc9a-4b9c-a401-1cb05ff97640');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-558ddba3-2380-4980-8ead-9c438e021f77\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-558ddba3-2380-4980-8ead-9c438e021f77')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-558ddba3-2380-4980-8ead-9c438e021f77 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_4ca29885-8797-4229-8825-9048d1da8db5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_fake')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4ca29885-8797-4229-8825-9048d1da8db5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_fake');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_fake",
              "summary": "{\n  \"name\": \"df_fake\",\n  \"rows\": 18000,\n  \"fields\": [\n    {\n      \"column\": \"VL\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 17982,\n        \"samples\": [\n          0.520214855670929,\n          2.274465799331665,\n          1.7843950986862183\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CD4\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 17935,\n        \"samples\": [\n          214.63882446289062,\n          315.6548156738281,\n          212.5238494873047\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gender\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ethnic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Base_Drug_Combo\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Extra_PI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          5,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Extra_pk_En\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "juHkKYVbHtlg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ugVzsdqCHtoI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9DWDGm8XHtrW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_all()\n",
        "\n",
        "# ===========================\n",
        "# Helpers: labels & builders\n",
        "# ===========================\n",
        "\n",
        "# canonical internal (snake_case) column order\n",
        "INTERNAL_COLS = [\n",
        "    \"VL\",\n",
        "    \"CD4\",\n",
        "    \"Gender\",\n",
        "    \"Ethnic\",\n",
        "    \"Base_Drug_Combo\",\n",
        "    \"Extra_PI\",\n",
        "    \"Extra_pk_En\",\n",
        "]\n",
        "\n",
        "# mapping to pretty labels for plotting\n",
        "INTERNAL_TO_PRETTY = {\n",
        "    \"VL\": \"VL [copies/mL]\",\n",
        "    \"CD4\": \"CD4 [cells/μL]\",\n",
        "    \"Gender\": \"Gender\",\n",
        "    \"Ethnic\": \"Ethnic\",\n",
        "    \"Base_Drug_Combo\": \"Base Drug Combo\",\n",
        "    \"Extra_PI\": \"Extra PI\",\n",
        "    \"Extra_pk_En\": \"Extra pk-En\",\n",
        "}\n",
        "\n",
        "# pretty labels we consider \"real-valued\" (KDE)\n",
        "REAL_NAMES = {\"VL [copies/mL]\", \"CD4 [cells/μL]\"}\n",
        "\n",
        "# categorical tick labels keyed by pretty label\n",
        "CATEGORY_LABELS: dict[str, list[str]] = {\n",
        "    \"Gender\": [\"Male\", \"Female\"],\n",
        "    \"Ethnic\": [\"Asian\", \"African\", \"Caucasian\", \"Other\"],\n",
        "    \"Base Drug Combo\": [\"FTC + TDF\", \"3TC + ABC\", \"FTC + TAF\",\n",
        "                        \"DRV + FTC + TDF\", \"FTC + RTVB + TDF\", \"Other\"],\n",
        "    \"Extra PI\": [\"DRV\", \"RTVB\", \"LPV\", \"RTV\", \"ATV\", \"Not Applied\"],\n",
        "    \"Extra pk-En\": [\"False\", \"True\"],\n",
        "}\n",
        "\n",
        "def Execute_C008(_: Optional[pd.DataFrame] = None) -> tuple[list[str], list[str]]:\n",
        "    \"\"\"Return (pretty_labels, internal_names) aligned by index.\"\"\"\n",
        "    pretty = [INTERNAL_TO_PRETTY[c] for c in INTERNAL_COLS]\n",
        "    internal = INTERNAL_COLS.copy()\n",
        "    return pretty, internal\n",
        "\n",
        "def set_categorical_with_order(df: pd.DataFrame, col: str, ordered_labels: Sequence[str]) -> None:\n",
        "    ctype = CategoricalDtype(categories=list(ordered_labels), ordered=True)\n",
        "    df[col] = df[col].astype(str).astype(ctype)\n",
        "\n",
        "def make_df(\n",
        "    col_pretty: str,\n",
        "    real_col: np.ndarray,\n",
        "    fake_col: np.ndarray,\n",
        "    categorical: bool,\n",
        "    cat_labels_map: Mapping[str, list[str]] = CATEGORY_LABELS,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build a tidy dataframe with a 'Type' column to distinguish Real vs Synthetic.\n",
        "    If categorical=True, values are integer codes 0..K-1 which will be mapped to labels.\n",
        "    \"\"\"\n",
        "    if categorical:\n",
        "        df = pd.concat([\n",
        "            pd.DataFrame({col_pretty: fake_col.astype(int), \"Type\": \"Synthetic\"}),\n",
        "            pd.DataFrame({col_pretty: real_col.astype(int), \"Type\": \"Real\"}),\n",
        "        ], ignore_index=True)\n",
        "        labels = cat_labels_map.get(col_pretty)\n",
        "        if labels:\n",
        "            df[col_pretty] = df[col_pretty].map(dict(enumerate(labels)))\n",
        "            set_categorical_with_order(df, col_pretty, labels)\n",
        "        return df\n",
        "    else:\n",
        "        return pd.concat([\n",
        "            pd.DataFrame({col_pretty: fake_col.astype(float), \"Type\": \"Synthetic\"}),\n",
        "            pd.DataFrame({col_pretty: real_col.astype(float), \"Type\": \"Real\"}),\n",
        "        ], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "PTyWMQfJHtyY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_all()\n",
        "\n",
        "# ===========================\n",
        "# Comparison plot\n",
        "# ===========================\n",
        "# Inputs expected:\n",
        "# - Sub_Data : real dataframe (has snake_case cols)\n",
        "# - df_fake  : synthetic dataframe (same snake_case cols)\n",
        "\n",
        "sns.set_context(\"talk\")\n",
        "plt.rcParams.update({\"font.size\": 35, \"legend.loc\": \"upper right\"})\n",
        "\n",
        "# 1) Align columns and labels (snake_case → pretty)\n",
        "pretty_labels, internal_names = Execute_C008()\n",
        "\n",
        "# sanity: make sure both frames have needed snake_case columns\n",
        "missing_real = [c for c in internal_names if c not in Sub_Data.columns]\n",
        "missing_fake = [c for c in internal_names if c not in df_fake.columns]\n",
        "if missing_real:\n",
        "    # can safely ignore PatientID/Timestep—they're not in INTERNAL_COLS\n",
        "    raise KeyError(f\"Missing in Sub_Data: {missing_real}\")\n",
        "if missing_fake:\n",
        "    raise KeyError(f\"Missing in df_fake: {missing_fake}\")\n",
        "\n",
        "Real_Data = Sub_Data[internal_names].copy()\n",
        "Fake_Data = df_fake[internal_names].copy()\n",
        "\n",
        "# 2) Grid\n",
        "n_features = len(pretty_labels)\n",
        "ncols = 4\n",
        "nrows = (n_features + ncols - 1) // ncols\n",
        "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(45, 10 * nrows))\n",
        "axes = np.atleast_2d(axes)\n",
        "\n",
        "# 3) Plot per feature\n",
        "for k, pretty in enumerate(pretty_labels):\n",
        "    i, j = divmod(k, ncols)\n",
        "    ax = axes[i, j]\n",
        "    internal = internal_names[k]\n",
        "\n",
        "    cur_fake = Fake_Data[internal].to_numpy()\n",
        "    cur_real = Real_Data[internal].to_numpy()\n",
        "\n",
        "    if pretty in REAL_NAMES:\n",
        "        df_all = make_df(pretty, cur_real, cur_fake, categorical=False)\n",
        "        sns.kdeplot(\n",
        "            data=df_all, x=pretty, hue=\"Type\",\n",
        "            fill=True, common_norm=False, ax=ax, legend=(k == 0)\n",
        "        )\n",
        "        if ax.legend_:\n",
        "            ax.legend_.set_title(None)\n",
        "        ax.yaxis.set_visible(False)\n",
        "        ax.set_xlabel(pretty)\n",
        "    else:\n",
        "        df_all = make_df(pretty, cur_real, cur_fake, categorical=True)\n",
        "        sns.histplot(\n",
        "            data=df_all, x=pretty, hue=\"Type\",\n",
        "            multiple=\"dodge\", stat=\"probability\", shrink=0.8,\n",
        "            alpha=0.35, linewidth=0, ax=ax, legend=False, discrete=True\n",
        "        )\n",
        "        if pretty in CATEGORY_LABELS and len(CATEGORY_LABELS[pretty]) > 3:\n",
        "            ax.tick_params(axis=\"x\", rotation=-30)\n",
        "        ax.yaxis.set_visible(False)\n",
        "        ax.set_xlabel(pretty)\n",
        "\n",
        "# 4) Hide unused panels\n",
        "for k in range(n_features, nrows * ncols):\n",
        "    i, j = divmod(k, ncols)\n",
        "    axes[i, j].axis(\"off\")\n",
        "\n",
        "plt.tight_layout(pad=1.0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "nsAkpSvaHwUY",
        "outputId": "d928da1a-8e86-4558-e17c-d2130b96bb21"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 4500x2000 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
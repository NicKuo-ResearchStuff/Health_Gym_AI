{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LATbLmmiGHL",
        "outputId": "de322ce8-0731-4ae5-f16c-52e491cedb52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(pad_time) right-pad from L=60 to 100 (+40)\n",
            "Input (after padding) : (2, 1, 22, 100)\n",
            "After bt1/in_proj    : (2, 64, 100)\n",
            "Down 1 (resblock)  : (2, 64, 100)\n",
            "Down 1 (downsample): (2, 64, 50)\n",
            "Down 2 (resblock)  : (2, 64, 50)\n",
            "Down 2 (downsample): (2, 64, 25)\n",
            "Down 3 (resblock)  : (2, 64, 25)\n",
            "Down 3 (downsample): (2, 64, 12)\n",
            "Mid (2x resblocks)   : (2, 64, 12)\n",
            "Up 1 (upsample)    : (2, 64, 24)\n",
            "Align (pad)         : (2, 64, 24) -> +1 to 25\n",
            "Up 1 (+skip,resblk): (2, 64, 25)\n",
            "Up 2 (upsample)    : (2, 64, 50)\n",
            "Up 2 (+skip,resblk): (2, 64, 50)\n",
            "Up 3 (upsample)    : (2, 64, 100)\n",
            "Up 3 (+skip,resblk): (2, 64, 100)\n",
            "After out_conv       : (2, 64, 100)\n",
            "After bt2/out_proj   : (2, 22, 100)\n",
            "Final output         : (2, 1, 22, 100)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "FEAT_DIM   = 22\n",
        "HIDDEN     = 64\n",
        "SEQ_LEN    = 60\n",
        "TARGET_LEN = 100\n",
        "LEVELS     = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def pad_time(x: torch.Tensor, tgt_len: int):\n",
        "    B, _, C, L = x.shape\n",
        "    if L == tgt_len:\n",
        "        mask = x.new_ones((B,1,1,L))\n",
        "        print(f\"(pad_time) exact, no pad. L={L}\")\n",
        "        return x, mask\n",
        "    if L > tgt_len:\n",
        "        print(f\"(pad_time) truncate from L={L} to {tgt_len}\")\n",
        "        x = x[..., :tgt_len]\n",
        "        mask = x.new_ones((B,1,1,tgt_len))\n",
        "        return x, mask\n",
        "    pad = tgt_len - L\n",
        "    print(f\"(pad_time) right-pad from L={L} to {tgt_len} (+{pad})\")\n",
        "    x = F.pad(x, (0, pad))\n",
        "    mask = torch.cat([x.new_ones((B,1,1,L)), x.new_zeros((B,1,1,pad))], dim=-1)\n",
        "    return x, mask\n",
        "\n",
        "class SimpleResBlock1D(nn.Module):\n",
        "    def __init__(self, channels: int, kernel_size: int = 3):\n",
        "        super().__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.conv1 = nn.Conv1d(channels, channels, kernel_size, padding=padding, bias=False)\n",
        "        self.conv2 = nn.Conv1d(channels, channels, kernel_size, padding=padding, bias=False)\n",
        "    def forward(self, x: torch.Tensor, gamma: torch.Tensor | None = None, beta: torch.Tensor | None = None):\n",
        "        y = self.conv1(x)\n",
        "        y = self.conv2(y)\n",
        "        return y + x\n",
        "\n",
        "class GistUNet1D(nn.Module):\n",
        "    def __init__(self, feat_dim=FEAT_DIM, hidden=HIDDEN, levels=LEVELS):\n",
        "        super().__init__()\n",
        "        self.levels = levels\n",
        "        self.in_proj  = nn.Conv1d(in_channels=feat_dim, out_channels=hidden, kernel_size=1, bias=False)\n",
        "        self.out_proj = nn.Conv1d(in_channels=hidden,   out_channels=feat_dim, kernel_size=1, bias=False)\n",
        "        self.down_blocks = nn.ModuleList([SimpleResBlock1D(hidden) for _ in range(levels)])\n",
        "        self.downsample  = nn.ModuleList([\n",
        "            nn.Conv1d(hidden, hidden, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "            for _ in range(levels)\n",
        "        ])\n",
        "        self.mid1 = SimpleResBlock1D(hidden)\n",
        "        self.mid2 = SimpleResBlock1D(hidden)\n",
        "        self.upsample = nn.ModuleList([\n",
        "            nn.ConvTranspose1d(hidden, hidden, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "            for _ in range(levels)\n",
        "        ])\n",
        "        self.up_blocks = nn.ModuleList([SimpleResBlock1D(hidden) for _ in range(levels)])\n",
        "        self.out_conv = nn.Conv1d(hidden, hidden, kernel_size=3, padding=1, bias=False)\n",
        "    def forward(self, x):\n",
        "        x = x.squeeze(1)\n",
        "        x = self.in_proj(x)\n",
        "        print(f\"After bt1/in_proj    : {tuple(x.shape)}\")\n",
        "        skips = []\n",
        "        for i, (block, down) in enumerate(zip(self.down_blocks, self.downsample), 1):\n",
        "            x = block(x, None, None)\n",
        "            print(f\"Down {i} (resblock)  : {tuple(x.shape)}\")\n",
        "            skips.append(x)\n",
        "            x = down(x)\n",
        "            print(f\"Down {i} (downsample): {tuple(x.shape)}\")\n",
        "        x = self.mid1(x)\n",
        "        x = self.mid2(x)\n",
        "        print(f\"Mid (2x resblocks)   : {tuple(x.shape)}\")\n",
        "        for j, (up, block) in enumerate(zip(self.upsample, self.up_blocks), 1):\n",
        "            x = up(x)\n",
        "            print(f\"Up {j} (upsample)    : {tuple(x.shape)}\")\n",
        "            skip = skips[-j]\n",
        "            if x.size(-1) > skip.size(-1):\n",
        "                print(f\"Align (crop)        : {tuple(x.shape)} -> {skip.size(-1)}\")\n",
        "                x = x[..., :skip.size(-1)]\n",
        "            elif x.size(-1) < skip.size(-1):\n",
        "                pad = skip.size(-1) - x.size(-1)\n",
        "                print(f\"Align (pad)         : {tuple(x.shape)} -> +{pad} to {skip.size(-1)}\")\n",
        "                x = F.pad(x, (0, pad))\n",
        "            x = x + skip\n",
        "            x = block(x, None, None)\n",
        "            print(f\"Up {j} (+skip,resblk): {tuple(x.shape)}\")\n",
        "        x = self.out_conv(x)\n",
        "        print(f\"After out_conv       : {tuple(x.shape)}\")\n",
        "        x = self.out_proj(x)\n",
        "        print(f\"After bt2/out_proj   : {tuple(x.shape)}\")\n",
        "        x = x.unsqueeze(1)\n",
        "        print(f\"Final output         : {tuple(x.shape)}\")\n",
        "        return x\n",
        "\n",
        "torch.manual_seed(0)\n",
        "x = torch.rand(BATCH_SIZE, 1, FEAT_DIM, SEQ_LEN, device=device)\n",
        "x_pad, mask = pad_time(x, TARGET_LEN)\n",
        "print(f\"Input (after padding) : {tuple(x_pad.shape)}\")\n",
        "model = GistUNet1D(FEAT_DIM, HIDDEN, LEVELS).to(device)\n",
        "with torch.no_grad():\n",
        "    y = model(x_pad)\n"
      ]
    }
  ]
}
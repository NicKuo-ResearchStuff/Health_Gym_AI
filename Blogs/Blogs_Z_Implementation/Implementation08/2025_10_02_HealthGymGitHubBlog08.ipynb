{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tbUq1tjCkq-I"
      },
      "outputs": [],
      "source": [
        "# ========================\n",
        "# Standard Library\n",
        "# ========================\n",
        "import os\n",
        "import random\n",
        "import itertools\n",
        "from copy import deepcopy\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "# ========================\n",
        "# Core Scientific Stack\n",
        "# ========================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from scipy import stats\n",
        "from scipy.special import boxcox as sp_boxcox\n",
        "\n",
        "# ========================\n",
        "# Machine Learning Utilities\n",
        "# ========================\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ========================\n",
        "# Deep Learning (PyTorch)\n",
        "# ========================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import autograd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# ========================\n",
        "# Visualization\n",
        "# ========================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_all(seed: int = 42, deterministic: bool = True) -> None:\n",
        "    import os, random, numpy as np, torch\n",
        "\n",
        "    # Python built-ins\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "    # NumPy\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # PyTorch\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # cuDNN settings\n",
        "    torch.backends.cudnn.deterministic = deterministic\n",
        "    torch.backends.cudnn.benchmark = not deterministic\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cuda.matmul.allow_tf32 = False\n",
        "        torch.backends.cudnn.allow_tf32 = False\n"
      ],
      "metadata": {
        "id": "Yi38Obgxkuog"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_all()"
      ],
      "metadata": {
        "id": "HhTXw0u3kw_d"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, Hyper007_HD, data_types):\n",
        "        super().__init__()\n",
        "        HD, OD = Hyper007_HD, 1\n",
        "        self.max_real = max(data_types.loc[data_types[\"type\"] == \"real\", \"index_end\"])\n",
        "        self.embedding_layers = nn.ModuleList()\n",
        "        self.soft_embedding = []\n",
        "        for _, row in data_types.iterrows():\n",
        "            if row[\"type\"] != \"real\":\n",
        "                self.embedding_layers.append(nn.Embedding(row[\"num_classes\"], row[\"embedding_size\"]))\n",
        "                idxs, idxe = row[\"index_start\"], row[\"index_end\"]\n",
        "                self.soft_embedding.append(\n",
        "                    lambda x, W, idxs=idxs, idxe=idxe: x[..., idxs:idxe] @ W\n",
        "                )\n",
        "        self.linear1 = nn.Linear(sum(data_types[\"embedding_size\"]), HD)\n",
        "        self.linear2 = nn.Linear(HD, HD)\n",
        "        self.rnn_f = MyLSTM(HD, HD)\n",
        "        self.rnn_r = MyLSTM(HD, HD)\n",
        "        self.linear3 = nn.Linear(2 * HD, OD)\n",
        "        self.leakyReLU = nn.LeakyReLU(0.1)\n",
        "\n",
        "    def forward(self, x0):\n",
        "        x_list = [x0[..., :self.max_real]] + [\n",
        "            f(x0, emb.weight) for f, emb in zip(self.soft_embedding, self.embedding_layers)\n",
        "        ]\n",
        "        x1 = torch.cat(x_list, dim=-1)\n",
        "        x2 = self.leakyReLU(self.linear1(x1))\n",
        "        x3 = self.leakyReLU(self.linear2(x2))\n",
        "        _, (x4_f, _) = self.rnn_f(x3)\n",
        "        _, (x4_r, _) = self.rnn_r(x3.flip(dims=[1]))\n",
        "        x4 = torch.cat((x4_f, x4_r), dim=1)\n",
        "        return self.linear3(x4)"
      ],
      "metadata": {
        "id": "oUPclKInkxWI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Execute_D004:\n",
        "    def __init__(self,\n",
        "                 Hyper001_BatchSize, Hyper002_Epochs,\n",
        "                 Hyper003_G_iter, Hyper004_GP_Lambda, Hyper005_C_Lambda,\n",
        "                 Hyper006_ID, Hyper007_HD,\n",
        "                 Hyper008_LR, Hyper009_Betas,\n",
        "                 data_types,\n",
        "                 correlation_real,\n",
        "                 continue_info=[False, 'G_SD', 'D_SD', 0]):\n",
        "        super().__init__()\n",
        "        self.batch_size = Hyper001_BatchSize\n",
        "        self.epochs = Hyper002_Epochs\n",
        "        self.G_iter = Hyper003_G_iter\n",
        "        self.gp_weight = Hyper004_GP_Lambda\n",
        "        self.c_weight = Hyper005_C_Lambda\n",
        "        self.ID = Hyper006_ID\n",
        "        self.HD = Hyper007_HD\n",
        "        self.lr = Hyper008_LR\n",
        "        self.betas = Hyper009_Betas\n",
        "        self.correlation_real = correlation_real.cuda()\n",
        "        self.CUDA = torch.cuda.is_available()\n",
        "        self.G = Generator(Hyper006_ID, Hyper007_HD, data_types)\n",
        "        self.D = Discriminator(Hyper007_HD, data_types)\n",
        "        if self.CUDA:\n",
        "            self.G = self.G.cuda()\n",
        "            self.D = self.D.cuda()\n",
        "        G_SD, D_SD = LoadPreTrain(continue_info)\n",
        "        if G_SD != 0:\n",
        "            self.G.load_state_dict(G_SD)\n",
        "            self.D.load_state_dict(D_SD)\n",
        "            self.PreviousEpoch = continue_info[3]\n",
        "        else:\n",
        "            self.PreviousEpoch = 0\n",
        "        self.D_opt = optim.Adam(self.D.parameters(), lr=self.lr, betas=self.betas)\n",
        "        self.G_opt = optim.Adam(self.G.parameters(), lr=self.lr, betas=self.betas)\n",
        "\n",
        "    def generate_data(self, seq_len, num_samples=None):\n",
        "        if num_samples is None:\n",
        "            num_samples = self.batch_size\n",
        "        z = torch.rand((num_samples, seq_len, self.ID)).cuda()\n",
        "        return self.G(z)\n",
        "\n",
        "    def _critic_train_iteration(self, data_real):\n",
        "        data_fake = self.generate_data(data_real.shape[1], data_real.shape[0])\n",
        "        D_real = self.D(data_real)\n",
        "        D_fake = self.D(data_fake)\n",
        "        with torch.backends.cudnn.flags(enabled=False):\n",
        "            gradient_penalty = self._gradient_penalty(data_real, data_fake)\n",
        "        self.D_opt.zero_grad()\n",
        "        D_loss = D_fake.mean() - D_real.mean() + gradient_penalty\n",
        "        D_loss.backward()\n",
        "        self.D_opt.step()\n",
        "        return D_loss.item(), gradient_penalty.item(), D_real.mean().item(), D_fake.mean().item()\n",
        "\n",
        "    def _generator_train_iteration(self, seq_len):\n",
        "        data_fake = self.generate_data(seq_len)\n",
        "        D_fake = self.D(data_fake)\n",
        "        corr_loss = self._correlation_loss(data_fake)\n",
        "        self.G_opt.zero_grad()\n",
        "        G_loss = -D_fake.mean() + self.c_weight * corr_loss\n",
        "        G_loss.backward()\n",
        "        self.G_opt.step()\n",
        "        return G_loss.item(), corr_loss.item()\n",
        "\n",
        "    def _correlation_loss(self, data_fake):\n",
        "        correlation_fake = correlation(data_fake)\n",
        "        criterion = nn.L1Loss(reduction=\"mean\")\n",
        "        temp = torch.rand_like(correlation_fake)\n",
        "        temp_fake = torch.min(temp, torch.abs(correlation_fake))\n",
        "        temp_real = torch.min(temp, torch.abs(self.correlation_real))\n",
        "        correlation_fake = temp_fake * torch.sign(correlation_fake)\n",
        "        correlation_real = temp_real * torch.sign(self.correlation_real)\n",
        "        return criterion(correlation_fake, correlation_real)\n",
        "\n",
        "    def _gradient_penalty(self, data_real, data_fake):\n",
        "        alpha = torch.rand((self.batch_size, 1, 1)).cuda().expand_as(data_real)\n",
        "        interpolated = alpha * data_real + (1 - alpha) * data_fake\n",
        "        prob_interpolated = self.D(interpolated)\n",
        "        gradients = autograd.grad(\n",
        "            outputs=prob_interpolated,\n",
        "            inputs=interpolated,\n",
        "            grad_outputs=torch.ones_like(prob_interpolated).cuda(),\n",
        "            create_graph=True,\n",
        "            retain_graph=True,\n",
        "        )[0]\n",
        "        gradients = gradients.view(self.batch_size, -1)\n",
        "        gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
        "        return self.gp_weight * ((gradients_norm - 1) ** 2).mean()\n",
        "\n",
        "    def train(self, All_loader):\n",
        "        All_Length = sorted(All_loader.keys())\n",
        "        for epoch in range(self.epochs - self.PreviousEpoch):\n",
        "            for Cur_Len in All_Length:\n",
        "                Cur_loader = All_loader[Cur_Len]\n",
        "                for batch_idx, (data_real, _) in enumerate(Cur_loader):\n",
        "                    data_real = data_real.cuda()\n",
        "                    for _ in range(self.G_iter):\n",
        "                        D_Loss, GP, D_real_mean, D_fake_mean = self._critic_train_iteration(data_real)\n",
        "                    G_Loss, Corr_Loss = self._generator_train_iteration(seq_len=Cur_Len)\n",
        "                    if (np.mod(batch_idx + 1, round(len(Cur_loader) / 5)) == 0) or \\\n",
        "                       (batch_idx + 1 == len(Cur_loader)):\n",
        "                        print(f\"[Epoch {self.PreviousEpoch + epoch + 1}] \"\n",
        "                              f\"L={Cur_Len} step {batch_idx + 1}/{len(Cur_loader)} | \"\n",
        "                              f\"D_loss={D_Loss:.4f} GP={GP:.4f} \"\n",
        "                              f\"D_real={D_real_mean:.4f} D_fake={D_fake_mean:.4f} | \"\n",
        "                              f\"G_loss={G_Loss:.4f} Corr={Corr_Loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "c4mJbo_Bk1Yo"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}